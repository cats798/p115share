from p115client import P115Client, check_response
from p115client.fs import P115FileSystem
from p115client.util import share_extract_payload
from p115client.tool import share_iterdir_walk
from app.core.config import settings
from loguru import logger
import asyncio
import time
import random
from contextlib import asynccontextmanager
from typing import Literal, Optional, Tuple, Union
from app.core.database import async_session
from app.models.schema import PendingLink, LinkHistory
from sqlalchemy import select, delete

# é»˜è®¤ API è¯·æ±‚è¶…æ—¶ï¼ˆç§’ï¼‰
API_TIMEOUT = 60
# é»˜è®¤ API é‡è¯•æ¬¡æ•°
API_MAX_RETRIES = 3
# é‡è¯•é—´éš”ï¼ˆç§’ï¼‰
API_RETRY_DELAY = 5

# iOS ç”¨æˆ·ä»£ç†
IOS_UA = (
    "Mozilla/5.0 (iPhone; CPU iPhone OS 17_5_1 like Mac OS X) "
    "AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148 115wangpan_ios/36.2.20"
)


class P115Service:
    def __init__(self):
        self.client = None
        self.fs = None
        self.is_connected = False
        self._task_lock: Optional[asyncio.Lock] = None  # Lazy initialize
        self._current_task: str | None = None  # Track current task type
        self._save_dir_cid: int = 0  # Cached save directory CID
        # ä»»åŠ¡é˜Ÿåˆ—æœºåˆ¶
        self._task_queue = asyncio.Queue()
        self._worker_task = None
        self._worker_lock = asyncio.Lock()
        self._current_task_info = None # å­˜å‚¨å½“å‰æ­£åœ¨å¤„ç†çš„ä»»åŠ¡ä¿¡æ¯
        self._restriction_until: float = 0 # é™åˆ¶ç»“æŸçš„æ—¶é—´æˆ³
        
        if settings.P115_COOKIE:
            self.init_client(settings.P115_COOKIE)

    @property
    def queue_size(self) -> int:
        """è¿”å›å½“å‰åœ¨é˜Ÿåˆ—ä¸­ç­‰å¾…çš„ä»»åŠ¡æ•°é‡"""
        return self._task_queue.qsize()

    @property
    def is_busy(self) -> bool:
        """å¦‚æœ Worker æ­£åœ¨å¤„ç†ä»»åŠ¡æˆ–è€…å¤„äºé™åˆ¶çŠ¶æ€åˆ™è¿”å› True"""
        return self._current_task_info is not None or self.is_restricted

    @property
    def is_restricted(self) -> bool:
        """æ£€æŸ¥å½“å‰æ˜¯å¦å¤„äº 115 é™åˆ¶çŠ¶æ€"""
        return time.time() < self._restriction_until

    def set_restriction(self, hours: float = 1.0):
        """è®¾ç½®å…¨å±€é™åˆ¶çŠ¶æ€"""
        self._restriction_until = time.time() + (hours * 3600)
        logger.warning(f"ğŸš« 115 æœåŠ¡å·²è¿›å…¥å…¨å±€é™åˆ¶æ¨¡å¼ï¼Œé¢„è®¡æŒç»­ {hours} å°æ—¶ (ç›´åˆ° {time.strftime('%H:%M:%S', time.localtime(self._restriction_until))})")

    def clear_restriction(self):
        """æ¸…é™¤å…¨å±€é™åˆ¶çŠ¶æ€"""
        if self._restriction_until > 0:
            self._restriction_until = 0
            logger.info("ğŸ”“ 115 å…¨å±€é™åˆ¶æ¨¡å¼å·²è§£é™¤")

    def _get_ios_ua_kwargs(self):
        """è·å– iOS ç”¨æˆ·ä»£ç†ç›¸å…³çš„å‚æ•°"""
        return {
            "headers": {
                "user-agent": IOS_UA,
                "accept-encoding": "gzip, deflate"
            },
            "app": "ios"
        }


    async def _task_worker(self):
        """åå°ä»»åŠ¡å¤„ç† Worker"""
        logger.info("ğŸš€ P115 ä»»åŠ¡é˜Ÿåˆ— Worker å·²å¯åŠ¨")
        while True:
            # è·å–ä»»åŠ¡ï¼š(task_func, args, kwargs, future, task_type)
            task_func, args, kwargs, future, task_type = await self._task_queue.get()
            self._current_task_info = task_type
            try:
                logger.info(f"âš¡ é˜Ÿåˆ—æ­£åœ¨å¤„ç†ä»»åŠ¡: {task_type}")
                # æ‰§è¡Œå…·ä½“é€»è¾‘
                result = await task_func(*args, **kwargs)
                if not future.done():
                    future.set_result(result)
            except Exception as e:
                logger.error(f"âŒ é˜Ÿåˆ—æ‰§è¡Œä»»åŠ¡ {task_type} å‡ºé”™: {e}")
                if not future.done():
                    future.set_exception(e)
            finally:
                self._task_queue.task_done()
                self._current_task_info = None

    async def _api_call_with_timeout(
        self,
        coro_func,
        *args,
        timeout: int = API_TIMEOUT,
        max_retries: int = API_MAX_RETRIES,
        retry_delay: int = API_RETRY_DELAY,
        label: str = "API",
        **kwargs,
    ):
        """å¸¦è¶…æ—¶å’Œé‡è¯•çš„ API è°ƒç”¨åŒ…è£…å™¨ã€‚
        
        Args:
            coro_func: å¼‚æ­¥æ–¹æ³•ï¼ˆå¦‚ self.client.share_snapï¼‰
            *args: ä¼ ç»™ coro_func çš„ä½ç½®å‚æ•°
            timeout: å•æ¬¡è¯·æ±‚è¶…æ—¶ç§’æ•°
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            retry_delay: é‡è¯•é—´éš”ç§’æ•°
            label: æ—¥å¿—æ ‡è¯†
            **kwargs: ä¼ ç»™ coro_func çš„å…³é”®å­—å‚æ•°
        """
        last_error = None
        for attempt in range(1, max_retries + 1):
            try:
                result = await asyncio.wait_for(
                    coro_func(*args, **kwargs),
                    timeout=timeout,
                )
                return result
            except asyncio.TimeoutError:
                last_error = TimeoutError(f"{label} è¯·æ±‚è¶…æ—¶ ({timeout}s), å°è¯• {attempt}/{max_retries}")
                logger.warning(f"â±ï¸ {label} è¯·æ±‚è¶…æ—¶ (å°è¯• {attempt}/{max_retries})")
            except Exception as e:
                # éè¶…æ—¶å¼‚å¸¸ç›´æ¥æŠ›å‡ºï¼Œä¸é‡è¯•
                raise
            
            if attempt < max_retries:
                logger.info(f"ğŸ”„ {label} å°†åœ¨ {retry_delay}s åé‡è¯•...")
                await asyncio.sleep(retry_delay)
        
        raise last_error

    def init_client(self, cookie: str):
        try:
            # Apply proxy settings to environment if configured
            import os
            if settings.PROXY_ENABLED and settings.PROXY_HOST and settings.PROXY_PORT:
                proxy_type = settings.PROXY_TYPE.lower()
                auth = f"{settings.PROXY_USER}:{settings.PROXY_PASS}@" if settings.PROXY_USER and settings.PROXY_PASS else ""
                proxy_url = f"{proxy_type}://{auth}{settings.PROXY_HOST}:{settings.PROXY_PORT}"
                
                os.environ['HTTP_PROXY'] = proxy_url
                os.environ['http_proxy'] = proxy_url
                os.environ['HTTPS_PROXY'] = proxy_url
                os.environ['https_proxy'] = proxy_url
                
            self.client = P115Client(cookie, check_for_relogin=True)
            self.fs = P115FileSystem(self.client)
            
            proxy_info = ""
            if settings.PROXY_ENABLED:
                proxy_info = f" (Proxy: {settings.PROXY_TYPE}://{settings.PROXY_HOST}:{settings.PROXY_PORT})"
            logger.info(f"P115Client and FileSystem initialized successfully{proxy_info}")
            # Verify connection asynchronously
            asyncio.create_task(self.verify_connection())
        except Exception as e:
            logger.error(f"Failed to initialize P115Client: {e}")
            self.client = None
            self.fs = None
            self.is_connected = False

    @asynccontextmanager
    async def _acquire_task_lock(self, task_type: Literal["save_share", "cleanup"], wait: bool = True):
        """å·²åºŸå¼ƒï¼šæ”¹ä¸ºä½¿ç”¨ä»»åŠ¡é˜Ÿåˆ—æ’é˜Ÿå¤„ç†ã€‚
        ä¸ºäº†å…¼å®¹æ€§ä¿ç•™æ¥å£ï¼Œå®é™…é€»è¾‘æ”¹ä¸ºåœ¨é˜Ÿåˆ—ä¸­æ’é˜Ÿã€‚
        """
        # æ³¨æ„ï¼šæ¸…ç†ä»»åŠ¡ç›®å‰ä»å¯ä¿æŒåŒæ­¥ç­‰å¾…ï¼Œä½†å»ºè®®æ‰€æœ‰ 115 å†™æ“ä½œéƒ½è¿‡é˜Ÿåˆ—
        # è¿™é‡Œä¸ºäº†æœ€å°åŒ–å˜åŠ¨ï¼Œæš‚æ—¶ä»…é’ˆå¯¹ share é“¾æ¥è¿›è¡Œé˜Ÿåˆ—åŒ–
        yield

    async def _enqueue_op(self, task_type: str, func, *args, **kwargs):
        """å°†æ“ä½œæ”¾å…¥é˜Ÿåˆ—å¹¶ç­‰å¾…ç»“æœ"""
        # ç¡®ä¿ Worker æ­£åœ¨è¿è¡Œ
        if self._worker_task is None or self._worker_task.done():
            async with self._worker_lock:
                if self._worker_task is None or self._worker_task.done():
                    self._worker_task = asyncio.create_task(self._task_worker())
                    logger.info("âš¡ å»¶è¿Ÿå¯åŠ¨ P115 ä»»åŠ¡é˜Ÿåˆ— Worker")

        future = asyncio.get_running_loop().create_future()
        await self._task_queue.put((func, args, kwargs, future, task_type))
        return await future

    async def verify_connection(self) -> bool:
        """Verify the 115 cookie connection"""
        if not self.client:
            self.is_connected = False
            return False
            
        try:
            # Simple API call to verify cookie
            resp = await self._api_call_with_timeout(
                self.client.user_info, async_=True,
                timeout=30, max_retries=2, label="user_info",
                **self._get_ios_ua_kwargs()
            )
            if resp.get("state"):
                self.is_connected = True
                logger.info("âœ… 115 ç½‘ç›˜ç™»å½•éªŒè¯æˆåŠŸ")
                return True
        except Exception as e:
            logger.error(f"âŒ 115 ç½‘ç›˜ç™»å½•éªŒè¯å¤±è´¥: {e}")
            self.is_connected = False
            return False
            
        self.is_connected = False
        return False

    def clear_save_dir_cache(self):
        """Clear the cached save directory CID (e.g. after cleanup)"""
        self._save_dir_cid = 0
        logger.debug("ğŸ—‘ï¸ å·²æ¸…é™¤ä¿å­˜ç›®å½• CID ç¼“å­˜")

    async def _ensure_save_dir(self, path: Optional[str] = None):
        """Ensure the save directory exists and return its CID.
        
        Uses a cached CID to avoid repeated API calls for the default path.
        If a custom path is provided, it will always verify/create it.
        """
        is_default = path is None
        path = path or settings.P115_SAVE_DIR or "/åˆ†äº«ä¿å­˜"
        
        # Return cached CID if available and using default path
        if is_default and self._save_dir_cid > 0:
            logger.debug(f"ğŸ“‚ ä½¿ç”¨ç¼“å­˜çš„ä¿å­˜ç›®å½• CID: {self._save_dir_cid}")
            return self._save_dir_cid
        
        logger.info(f"ğŸ” å¼€å§‹æ£€æŸ¥/åˆ›å»ºä¿å­˜ç›®å½•: {path}")
        
        if not self.client:
            raise RuntimeError("P115Client æœªåˆå§‹åŒ–ï¼Œæ— æ³•åˆ›å»ºä¿å­˜ç›®å½•")
        
        # Retry up to 3 times with timeout
        last_error = None
        for attempt in range(1, 4):
            try:
                logger.info(f"ğŸ“ è°ƒç”¨ fs_makedirs_app åˆ›å»ºç›®å½•... (å°è¯• {attempt}/3)")
                # Add 30s timeout to prevent indefinite hanging
                resp = await asyncio.wait_for(
                    self.client.fs_makedirs_app(path, pid=0, async_=True, **self._get_ios_ua_kwargs()),
                    timeout=30
                )
                logger.info(f"ğŸ“‹ fs_makedirs_app å“åº”: {resp}")
                check_response(resp)
                
                # The response structure has 'cid' at the top level (not in 'data')
                # Response format: {'state': True, 'error': '', 'errCode': 0, 'cid': '3358575817564146054'}
                cid = 0
                if "cid" in resp:
                    cid = int(resp["cid"])
                    logger.info(f"ğŸ”¢ ä»å“åº”ä¸­æå–åˆ° CID: {cid}")
                elif "data" in resp:
                    data = resp["data"]
                    cid = int(data.get("category_id") or data.get("cid") or data.get("id") or 0)
                    logger.info(f"ğŸ”¢ ä» data å­—æ®µä¸­æå–åˆ° CID: {cid}")
                else:
                    logger.error(f"âŒ å“åº”ä¸­æ²¡æœ‰ 'cid' æˆ– 'data' å­—æ®µ: {resp}")
                    
                if cid == 0:
                    raise RuntimeError(f"æ— æ³•ä»å“åº”è·å–æœ‰æ•ˆçš„ CID: {resp}")
                    
                # Cache the CID only if it's the default path
                if is_default:
                    self._save_dir_cid = cid
                logger.info(f"âœ… ä¿å­˜ç›®å½•å·²ç¡®è®¤: {path} (CID: {cid})")
                return cid
                
            except asyncio.TimeoutError:
                last_error = TimeoutError(f"fs_makedirs_app è¯·æ±‚è¶…æ—¶ (30s), å°è¯• {attempt}/3")
                logger.warning(f"â±ï¸ fs_makedirs_app è¯·æ±‚è¶…æ—¶ (å°è¯• {attempt}/3)")
            except Exception as e:
                last_error = e
                logger.warning(f"âš ï¸ åˆ›å»ºç›®å½•å¤±è´¥ (å°è¯• {attempt}/3): {e}")
            
            if attempt < 3:
                await asyncio.sleep(3)
        
        # All retries exhausted â€” raise to prevent saving to root
        raise RuntimeError(f"æ— æ³•ç¡®ä¿ä¿å­˜ç›®å½• {path} å­˜åœ¨ (å·²é‡è¯•3æ¬¡): {last_error}")

    async def save_share_link(self, share_url: str, metadata: dict = None, target_dir: Optional[str] = None):
        """é€šè¿‡é˜Ÿåˆ—ä¿å­˜é“¾æ¥"""
        return await self._enqueue_op("save_share", self._save_share_link_internal, share_url, metadata, target_dir)

    async def save_and_share(self, share_url: str, metadata: dict = None, target_dir: Optional[str] = None):
        """é€šè¿‡é˜Ÿåˆ—è¿›è¡Œè½¬å­˜å¹¶åˆ†äº«"""
        async def _internal_flow():
            save_res = await self._save_share_link_internal(share_url, metadata, target_dir)
            if save_res and save_res.get("status") == "success":
                share_res = await self.create_share_link(save_res)
                if isinstance(share_res, str):
                    return {"status": "success", "share_link": share_res}
                elif isinstance(share_res, dict) and share_res.get("status") == "error":
                    # å°†åˆ›å»ºåˆ†äº«æ—¶çš„ç‰¹å®šé”™è¯¯æ˜ å°„å›è½¬å­˜ç»“æœ
                    return {
                        "status": "error",
                        "error_type": share_res.get("error_type", "share_failed"),
                        "message": share_res.get("message", "ç”Ÿæˆåˆ†äº«é“¾æ¥å¤±è´¥")
                    }
                return {
                    "status": "error",
                    "error_type": "share_failed",
                    "message": "è½¬å­˜æˆåŠŸä½†ç”Ÿæˆåˆ†äº«é“¾æ¥å¤±è´¥"
                }
            return save_res

        return await self._enqueue_op(f"save_and_share({share_url})", _internal_flow)

    async def _save_share_link_internal(self, share_url: str, metadata: dict = None, target_dir: Optional[str] = None):
        """Internal logic for saving a 115 share link (no locking)"""
        if not self.client:
            logger.warning("P115Client not initialized, cannot save link")
            return None
        
        logger.info(f"ğŸ“¥ å¼€å§‹å¤„ç†åˆ†äº«é“¾æ¥: {share_url}")
        try:
            # 1. Extract share/receive codes
            payload = share_extract_payload(share_url)
            
            # 2. Get share snapshot to get file IDs and names (å¸¦è¶…æ—¶é‡è¯•)
            snap_resp = await self._api_call_with_timeout(
                self.client.share_snap_app, payload, async_=True,
                timeout=API_TIMEOUT, label="share_snap",
                **self._get_ios_ua_kwargs()
            )
            check_response(snap_resp)
            logger.debug(f"ğŸ“‹ share_snap å“åº”æ•°æ®: {snap_resp.get('data')}")

            # Check for audit and violation status
            data = snap_resp.get("data", {})
            if not data:
                logger.error("âŒ share_snap å“åº”ä¸­ç¼ºå°‘ data å­—æ®µ")
                return {
                    "status": "error",
                    "error_type": "api_error",
                    "message": "è·å–åˆ†äº«ä¿¡æ¯å¤±è´¥ï¼šAPI å“åº”æ•°æ®ä¸ºç©º"
                }

            share_info = data.get("shareinfo" if "shareinfo" in data else "share_info", {})
            share_state = data.get("share_state", share_info.get("share_state", share_info.get("status"))) # Multiple fallbacks
            share_title = share_info.get("share_title", "")
            have_vio_file = share_info.get("have_vio_file", 0)
            
            logger.info(f"ğŸ“Š åˆ†äº«çŠ¶æ€: {share_state}, æ ‡é¢˜: {share_title}, è¿è§„æ ‡å¿—: {have_vio_file}")

            # å³ä½¿åŒ…å«è¿è§„å†…å®¹æ ‡å¿—ï¼Œä¹Ÿå°è¯•ç»§ç»­å¤„ç†ï¼Œå› ä¸ºå¾ˆå¤šæ—¶å€™æ–‡ä»¶åˆ—è¡¨ä¾ç„¶å¯ç”¨
            if have_vio_file == 1:
                logger.warning(f"âš ï¸ åˆ†äº«é“¾æ¥åŒ…å«è¿è§„å†…å®¹æ ‡å¿— (have_vio_file=1): {share_url}")
                # ä¸å†ç›´æ¥è¿”å›é”™è¯¯ï¼Œå…è®¸é€»è¾‘ç»§ç»­æ‰§è¡Œä»¥æ£€æŸ¥ items åˆ—è¡¨


            is_snapshotting = "æ­£åœ¨ç”Ÿæˆæ–‡ä»¶å¿«ç…§" in str(snap_resp)
            if share_state == 0 or is_snapshotting:
                reason = "snapshotting" if is_snapshotting else "auditing"
                logger.info(f"ğŸ” åˆ†äº«é“¾æ¥å¤„äº{ 'å®¡æ ¸ä¸­' if reason == 'auditing' else 'å¿«ç…§ç”Ÿæˆä¸­' }ï¼Œè¿›å…¥è½®è¯¢ç­‰å¾…é˜Ÿåˆ—: {share_url}")
                # Save to DB for persistence
                async with async_session() as session:
                    new_task = PendingLink(
                        share_url=share_url,
                        metadata_json=metadata or {},
                        status=reason
                    )
                    session.add(new_task)
                    await session.commit()
                    db_id = new_task.id
                
                return {
                    "status": "pending",
                    "reason": reason,
                    "share_url": share_url,
                    "metadata": metadata or {},
                    "db_id": db_id
                }
            
            if share_state == 7:
                logger.warning(f"âš ï¸ åˆ†äº«é“¾æ¥å·²è¿‡æœŸ: {share_url}")
                return {
                    "status": "error",
                    "error_type": "expired",
                    "message": "é“¾æ¥å·²è¿‡æœŸ"
                }
            
            if share_state != 1:
                logger.warning(f"âš ï¸ åˆ†äº«é“¾æ¥çŠ¶æ€å¼‚å¸¸ (state={share_state}): {share_url}")
                # Allow attempt if state is unknown but not explicitly pending/expired/prohibited
            
            items = data.get("list", [])
            if not items:
                logger.warning(f"âš ï¸ åˆ†äº«é“¾æ¥å†…æ²¡æœ‰æ–‡ä»¶ã€‚have_vio_file={have_vio_file}, çŠ¶æ€: {snap_resp.get('state')}")
                if have_vio_file == 1:
                    return {
                        "status": "error",
                        "error_type": "violated",
                        "message": "é“¾æ¥åŒ…å«è¿è§„å†…å®¹ï¼Œæ— æ³•è½¬å­˜åˆ†äº«"
                    }
                return {
                    "status": "error",
                    "error_type": "empty_share",
                    "message": "åˆ†äº«é“¾æ¥å†…æ²¡æœ‰å¯ä¾›è½¬å­˜çš„æ–‡ä»¶"
                }
            
            # Extract file/folder IDs and names
            # Files use 'fid', folders use 'cid'
            fids = []
            names = []
            for item in items:
                # Try to get fid (file) or cid (folder)
                fid = item.get("fid") or item.get("cid")
                if fid:
                    fids.append(str(fid))
                    # 115 share_snap returns names with unnecessary escapes sometimes (e.g. \' for ')
                    raw_name = item.get("n") or item.get("fn") or item.get("name") or item.get("file_name") or item.get("title")
                    if not raw_name:
                        logger.warning(f"âš ï¸ æ— æ³•ä»åˆ†äº«é¡¹æå–æ–‡ä»¶åï¼Œå¯ç”¨çš„é”®æœ‰: {list(item.keys())}")
                        raw_name = "æœªçŸ¥"
                    cleaned_name = raw_name.replace("\\'", "'").replace('\\"', '"')
                    names.append(cleaned_name)
                else:
                    logger.warning(f"Item missing both fid and cid: {item}")
            
            if not fids:
                logger.error(f"âŒ æœªèƒ½ä»åˆ—è¡¨é¡¹æå–åˆ°ä»»ä½•æœ‰æ•ˆçš„æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹ IDã€‚é¡¹ç›®æ•°: {len(items)}")
                return {
                    "status": "error",
                    "error_type": "parse_error",
                    "message": "è§£æåˆ†äº«æ–‡ä»¶åˆ—è¡¨å¤±è´¥ï¼Œæ— æ³•æå–æ–‡ä»¶ ID"
                }
            
            logger.info(f"ğŸ“¦ æ£€æµ‹åˆ° {len(fids)} ä¸ªé¡¹ç›®: {', '.join(names[:3])}{'...' if len(names) > 3 else ''}")
            
            # 3. Ensure save directory (with network recovery retry)
            #    If _ensure_save_dir fails (e.g. network issue), pause and retry
            #    for up to 30 minutes instead of discarding the task.
            to_cid = None
            max_network_wait = 1800  # 30 minutes
            network_start = time.time()
            network_attempt = 0
            
            while True:
                try:
                    to_cid = await self._ensure_save_dir(target_dir)
                    if network_attempt > 0:
                        logger.info(f"ğŸ‰ ç½‘ç»œå·²æ¢å¤ï¼Œç»§ç»­å¤„ç†ä»»åŠ¡ (ç­‰å¾…äº† {time.time() - network_start:.0f}s)")
                    break
                except Exception as dir_err:
                    network_attempt += 1
                    elapsed = time.time() - network_start
                    remaining = max_network_wait - elapsed
                    
                    if remaining <= 0:
                        logger.error(f"âŒ ç½‘ç›˜ç½‘ç»œæ¢å¤ç­‰å¾…è¶…æ—¶ (30åˆ†é’Ÿ)ï¼Œä¸­æ­¢ä»»åŠ¡: {dir_err}")
                        return {
                            "status": "error",
                            "error_type": "dir_failed",
                            "message": f"ç½‘ç›˜ç½‘ç»œæŒç»­ä¸å¯ç”¨ (å·²ç­‰å¾…30åˆ†é’Ÿ): {dir_err}"
                        }
                    
                    wait_time = min(30, remaining)
                    logger.warning(
                        f"â¸ï¸ ç½‘ç›˜ç½‘ç»œå¼‚å¸¸ï¼Œä»»åŠ¡æš‚åœç­‰å¾…æ¢å¤ "
                        f"(ç¬¬{network_attempt}æ¬¡é‡è¯•, å·²ç­‰å¾… {elapsed:.0f}s, å‰©ä½™ {remaining:.0f}s): {dir_err}"
                    )
                    await asyncio.sleep(wait_time)
            
            # 4. Receive files
            # ğŸ’¡ å¢åŠ é¢„æ£€ï¼šåœ¨å¤§æ–‡ä»¶ä¿å­˜å‰å°è¯•æ¸…ç†
            # æå–åˆ†äº«çš„æ€»å¤§å°ç”¨äºç²¾å‡†å®¹é‡åˆ¤æ–­
            try:
                total_size = int(share_info.get("file_size") or 0)
            except (ValueError, TypeError):
                total_size = 0
            await self.check_and_prepare_capacity(file_count=len(fids), total_size=total_size)
            # é‡æ–°è·å–æœ€æ–°çš„ CIDï¼Œä»¥é˜²æ¸…ç†é€»è¾‘åˆ é™¤äº†ç›®å½•å¹¶é‡å»ºäº†å®ƒ
            to_cid = await self._ensure_save_dir(target_dir)

            receive_payload = {
                "share_code": payload["share_code"],
                "receive_code": payload["receive_code"] or "",
                "file_id": ",".join(fids),
                "cid": to_cid
            }
            
            try:
                recv_resp = await self._api_call_with_timeout(
                    self.client.share_receive_app, receive_payload, async_=True,
                    timeout=API_TIMEOUT, label="share_receive",
                    **self._get_ios_ua_kwargs()
                )
                check_response(recv_resp)
                logger.info(f"âœ… é“¾æ¥è½¬å­˜æŒ‡ä»¤å·²å‘é€: {share_url} -> CID {to_cid}")
                recursive_links = []
            except Exception as recv_error:
                # Check for 500-file limit error (errno 4200044)
                error_info = getattr(recv_error, "args", [None, {}])[1] if hasattr(recv_error, "args") and len(recv_error.args) >= 2 else {}
                errno_val = error_info.get("errno") if isinstance(error_info, dict) else None
                
                if errno_val == 4200044 or "è¶…è¿‡å½“å‰ç­‰çº§é™åˆ¶" in str(recv_error):
                    logger.warning(f"âš ï¸ è§¦å‘ 115 éä¼šå‘˜ 500 æ–‡ä»¶ä¿å­˜é™åˆ¶ï¼Œå°è¯•é€’å½’åˆ†æ‰¹ä¿å­˜: {share_url}")
                    recursive_links = await self._save_share_recursive(share_url, to_cid)
                    logger.info(f"âœ… é€’å½’åˆ†æ‰¹ä¿å­˜æŒ‡ä»¤å·²å¤„ç†å®Œæ¯•: {share_url}")
                # Check if it's a "file already received" error (errno 4200045)
                elif "4200045" in str(recv_error) or "å·²ç»æ¥æ”¶" in str(recv_error):
                    logger.warning(f"âš ï¸ 115 æç¤ºæ–‡ä»¶è¯¥åˆ†äº«å·²æ¥æ”¶è¿‡: {share_url}")
                    # Verify if files really exist in to_cid
                    found_all = False
                    try:
                        # ç”¨ _find_files_in_dir æŸ¥æ‰¾ï¼ˆæ”¯æŒ search + list åŒé‡æŸ¥æ‰¾ï¼‰
                        found_files = await self._find_files_in_dir(to_cid, names)
                        found_count = len(found_files)
                        if found_count > 0:
                            logger.info(f"âœ… åœ¨ä¿å­˜ç›®å½•ä¸­æ‰¾åˆ° {found_count} ä¸ªåŒåæ–‡ä»¶ï¼Œç»§ç»­å¤„ç†")
                            # Continue to share creation with existing files
                        else:
                            logger.error("âŒ 115 æç¤ºå·²æ¥æ”¶ï¼Œä½†åœ¨ä¿å­˜ç›®å½•æœªæ‰¾åˆ°æ–‡ä»¶ï¼ˆå¯èƒ½å·²è¢«åˆ é™¤ï¼‰ã€‚æ— æ³•é‡æ–°è½¬å­˜åŒä¸€åˆ†äº«é“¾æ¥ã€‚")
                            return {
                                "status": "error",
                                "error_type": "already_exists_missing",
                                "message": "è¯¥åˆ†äº«é“¾æ¥æ‚¨å·²è½¬å­˜è¿‡ã€‚115 é™åˆ¶åŒä¸€é“¾æ¥æ— æ³•ç”±äºæ–‡ä»¶ä¸¢å¤±è€Œé‡å¤è½¬å­˜ï¼Œè¯·å°è¯•å¯»æ‰¾åŸæ–‡ä»¶æˆ–ä»å›æ”¶ç«™è¿˜åŸã€‚"
                            }
                    except Exception as check_e:
                        logger.warning(f"âš ï¸ æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨æ—¶å‡ºé”™: {check_e}")
                        # Assume failure to be safe
                        return {
                            "status": "error", 
                            "error_type": "unknown",
                            "message": "ä¿å­˜å¤±è´¥ï¼Œä¸”æ— æ³•éªŒè¯æ–‡ä»¶æ˜¯å¦å­˜åœ¨"
                        }
                else:
                    # Other errors, re-raise
                    raise
            
            return {
                "status": "success", 
                "to_cid": to_cid, 
                "names": names,
                "share_url": share_url,
                "recursive_links": recursive_links if 'recursive_links' in locals() else [],
                "metadata": metadata or {},
                "have_vio": have_vio_file == 1
            }
        except Exception as e:
            # å½»åº•é¿å… loguru æ ¼å¼åŒ–å¼‚å¸¸æ—¶å¯èƒ½è§¦å‘çš„ KeyError
            try:
                if hasattr(e, 'args') and len(e.args) >= 2 and isinstance(e.args[1], dict):
                    error_msg = str(e.args[1].get('error', e))
                else:
                    error_msg = str(e)
            except:
                error_msg = "æœªçŸ¥å¼‚å¸¸"
            
            if "æ­£åœ¨ç”Ÿæˆæ–‡ä»¶å¿«ç…§" in error_msg:
                logger.info(f"ğŸ” åˆ†äº«é“¾æ¥æ­£åœ¨ç”Ÿæˆå¿«ç…§ï¼Œè¿›å…¥è½®è¯¢ç­‰å¾…é˜Ÿåˆ—: {share_url}")
                async with async_session() as session:
                    new_task = PendingLink(
                        share_url=share_url,
                        metadata_json=metadata or {},
                        status="snapshotting"
                    )
                    session.add(new_task)
                    await session.commit()
                    db_id = new_task.id
                
                return {
                    "status": "pending",
                    "reason": "snapshotting",
                    "share_url": share_url,
                    "metadata": metadata or {},
                    "db_id": db_id
                }
            
            # æ£€æŸ¥æ˜¯å¦ç”±äºè´¦å·é™åˆ¶å¯¼è‡´å¤±è´¥
            if "é™åˆ¶æ¥æ”¶" in error_msg:
                logger.warning(f"ğŸš« è§¦å‘ 115 æ¥æ”¶é™åˆ¶: {share_url}")
                self.set_restriction(hours=1.0) # è®¾ç½® 1 å°æ—¶å…¨å±€é™åˆ¶
                
                async with async_session() as session:
                    new_task = PendingLink(
                        share_url=share_url,
                        metadata_json=metadata or {},
                        status="restricted"
                    )
                    session.add(new_task)
                    await session.commit()
                    db_id = new_task.id
                
                return {
                    "status": "pending",
                    "reason": "restricted",
                    "share_url": share_url,
                    "metadata": metadata or {},
                    "db_id": db_id
                }

            logger.error("âŒ ä¿å­˜åˆ†äº«é“¾æ¥å‘ç”Ÿç¨‹åºå¼‚å¸¸: {}", error_msg)
            return {
                "status": "error",
                "error_type": "exception",
                "message": f"ç¨‹åºå¼‚å¸¸: {error_msg}"
            }

    async def _save_share_recursive(self, share_url: str, target_pid: int) -> list[str]:
        """é€’å½’åˆ†æ‰¹ä¿å­˜åˆ†äº«å†…å®¹ (è§„é¿ 500 æ–‡ä»¶é™åˆ¶ï¼Œé›†æˆä¸­è½¬æ¸…ç†é€»è¾‘)"""
        payload = share_extract_payload(share_url)
        share_code = payload["share_code"]
        receive_code = payload["receive_code"] or ""
        
        # çŠ¶æ€è¿½è¸ª
        cid_map = {0: target_pid}
        share_links = []
        files_saved_total = 0
        
        # è·¯å¾„é‡å»ºè¿½è¸ªï¼šshare_cid -> (parent_share_cid, name)
        share_structure = {0: (None, "")}
        
        async def reconstruct_path(current_share_cid, current_cid_map):
            """åœ¨æ¸…ç†åé‡å»ºå½“å‰æ‰€åœ¨çš„æ–‡ä»¶å¤¹è·¯å¾„"""
            # 1. ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
            new_root_cid = await self._ensure_save_dir()
            current_cid_map.clear()
            current_cid_map[0] = new_root_cid
            
            # 2. è·å–ä»æ ¹åˆ°å½“å‰çš„è·¯å¾„ååˆ—è¡¨
            path_names = []
            temp_cid = current_share_cid
            while temp_cid != 0:
                parent, name = share_structure[temp_cid]
                path_names.append(name)
                temp_cid = parent
            path_names.reverse()
            
            # 3. é€å±‚åˆ›å»º
            current_share = 0
            current_real = new_root_cid
            for name in path_names:
                # å¯»æ‰¾å¯¹åº”çš„å­ share_cid
                child_share = next(s_cid for s_cid, info in share_structure.items() if info[0] == current_share and info[1] == name)
                resp = await self._api_call_with_timeout(
                    self.client.fs_makedirs_app, name, pid=current_real, async_=True,
                    **self._get_ios_ua_kwargs()
                )
                check_response(resp)
                current_real = int(resp.get("cid") or resp.get("id") or (resp.get("data") or {}).get("cid") or 0)
                current_cid_map[child_share] = current_real
                current_share = child_share
            
            return current_real

        async for pid, dirs, files in share_iterdir_walk(
            self.client, share_code, receive_code, async_=True
        ):
            if pid not in cid_map:
                # å¦‚æœå› ä¸ºä¸­è½¬æ¸…ç†ä¸¢å¤±äº†æ˜ å°„ï¼Œé‡å»ºå®ƒ
                logger.info(f"ğŸ”„ æ­£åœ¨é€’å½’æ·±åº¦ä¸­é‡å»ºç›®å½•ç»“æ„ (Share CID: {pid})...")
                cid_map[pid] = await reconstruct_path(pid, cid_map)
                
            current_target_pid = cid_map[pid]
            
            # 1. è®°å½•ç»“æ„å¹¶åˆ›å»ºå­ç›®å½•
            for d in dirs:
                share_cid = d["id"]
                name = d["name"]
                share_structure[share_cid] = (pid, name)
                try:
                    resp = await self._api_call_with_timeout(
                        self.client.fs_makedirs_app, name, pid=current_target_pid, async_=True,
                        label=f"fs_makedirs({name})",
                        **self._get_ios_ua_kwargs()
                    )
                    check_response(resp)
                    new_cid = int(resp.get("cid") or resp.get("id") or (resp.get("data") or {}).get("cid") or 0)
                    if new_cid:
                        cid_map[share_cid] = new_cid
                except Exception as e:
                    if "å·²ç»å­˜åœ¨" in str(e) or "40004" in str(e):
                        found = await self._find_files_in_dir(current_target_pid, [name])
                        if found:
                            cid_map[share_cid] = int(found[0]["fid"])
                    else:
                        logger.error(f"âŒ é€’å½’ä¿å­˜è¿‡ç¨‹ä¸­åˆ›å»ºå­ç›®å½• {name} å¤±è´¥: {e}")
            
            # 2. åˆ†æ‰¹è½¬å­˜è¯¥ç›®å½•ä¸‹çš„æ–‡ä»¶
            fids = [str(f["id"]) for f in files]
            if not fids:
                continue
                
            for i in range(0, len(fids), 500):
                # ğŸš¦ æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸­è½¬æ¸…ç†
                # æ¡ä»¶ï¼šå·²å¤„ç†è¶…è¿‡ 10,000 æ–‡ä»¶ï¼Œæˆ–è€…å®¹é‡æ¥è¿‘ä¸Šé™ (90%)
                need_cleanup = files_saved_total >= 10000
                if not need_cleanup and settings.P115_CLEANUP_CAPACITY_ENABLED:
                    used, total = await self.get_storage_stats()
                    if total > 0 and (used / total) > 0.9:
                        need_cleanup = True
                        logger.warning(f"âš ï¸ å®¹é‡é€¼è¿‘ä¸Šé™ ({used/total:.1%})ï¼Œè§¦å‘ä¸­è½¬æ¸…ç†")

                if need_cleanup:
                    logger.info("ğŸ“¦ è§¦å‘ä¸­è½¬æµç¨‹ï¼šæ­£åœ¨ç”Ÿæˆå½“å‰å·²ä¿å­˜å†…å®¹çš„åˆ†äº«é“¾æ¥...")
                    # è¿™é‡Œçš„ CID è·å–å¯èƒ½ä¸å‡†ï¼Œå› ä¸ºæˆ‘ä»¬æ˜¯å…¨é‡æ¸…ç†ï¼Œæ‰€ä»¥ç›´æ¥åˆ†äº«ä¿å­˜ç›®å½•æ ¹èŠ‚ç‚¹
                    save_dir_cid = await self._ensure_save_dir()
                    save_name = settings.P115_SAVE_DIR
                    # è·å–ä¿å­˜ç›®å½•çš„çˆ¶ CID å’Œ è‡ªå·±çš„åå­—ï¼Œä»¥ä¾¿ create_share_link èƒ½æ‰¾åˆ°å®ƒ
                    # ç”±äº _ensure_save_dir åªç»™å‡ºäº† CIDï¼Œæˆ‘ä»¬å‡è®¾å®ƒå°±åœ¨æ ¹ç›®å½•ä¸‹æˆ–è€…æˆ‘ä»¬å¯ä»¥é€šè¿‡å…¶å®ƒæ–¹å¼åˆ†äº«
                    # ç®€åŒ–é€»è¾‘ï¼šç›´æ¥åˆ†äº«ä¿å­˜ç›®å½•ä¸‹çš„æ‰€æœ‰ä¸œè¥¿
                    # é‡æ–°æ„é€ ä¸€ä¸ª save_result æ¥è°ƒç”¨ create_share_link
                    # æ³¨æ„ï¼šæˆ‘ä»¬è¦æ‰¾çš„æ˜¯ä¿å­˜ç›®å½•é‡Œçš„ä¸œè¥¿
                    try:
                        # åˆ—å‡ºä¿å­˜ç›®å½•ä¸‹çš„é¡¶çº§æ–‡ä»¶/æ–‡ä»¶å¤¹å
                        ls_resp = await self._api_call_with_timeout(
                            self.client.fs_files_app2, save_dir_cid, async_=True,
                            **self._get_ios_ua_kwargs()
                        )
                        ls_items = ls_resp.get("data", [])
                        ls_names = [it["n"] for it in ls_items]
                        
                        if ls_names:
                            intermediate_link = await self.create_share_link({"to_cid": save_dir_cid, "names": ls_names})
                            if intermediate_link:
                                logger.info(f"ğŸ“¤ ä¸­è½¬é“¾æ¥å·²ç”Ÿæˆ: {intermediate_link}")
                                share_links.append(intermediate_link)
                                # TODO: è¿™é‡Œå¦‚æœèƒ½é€šè¿‡æœºå™¨äººå‘é€å³æ—¶æ¶ˆæ¯æ›´å¥½
                    except Exception as share_e:
                        logger.error(f"âŒ ä¸­è½¬åˆ†äº«ç”Ÿæˆå¤±è´¥: {share_e}")

                    # æ‰§è¡Œæ¸…ç†
                    await self._do_cleanup_logic()
                    logger.info("ğŸ§¹ ä¸­è½¬æ¸…ç†å®Œæˆï¼Œç­‰å¾… 5 ç§’æ¢å¤...")
                    await asyncio.sleep(5)
                    
                    # é‡ç½®è®¡æ•°å™¨å¹¶é‡å»ºå½“å‰è·¯å¾„æ˜ å°„
                    files_saved_total = 0
                    current_target_pid = await reconstruct_path(pid, cid_map)
                
                batch = fids[i:i+500]
                try:
                    receive_payload = {
                        "share_code": share_code,
                        "receive_code": receive_code,
                        "file_id": ",".join(batch),
                        "cid": current_target_pid
                    }
                    recv_resp = await self._api_call_with_timeout(
                        self.client.share_receive_app, receive_payload, async_=True,
                        timeout=API_TIMEOUT, label=f"share_receive_batch({i//500})",
                        **self._get_ios_ua_kwargs()
                    )
                    check_response(recv_resp)
                    files_saved_total += len(batch)
                    logger.info(f"âœ… é€’å½’åˆ†æ‰¹è½¬å­˜æˆåŠŸ: {len(batch)} ä¸ªæ–‡ä»¶ -> CID {current_target_pid} (æœ¬è½®ç´¯è®¡: {files_saved_total})")
                    
                    await asyncio.sleep(random.randint(2, 3))
                except Exception as e:
                    if "4200045" in str(e) or "å·²ç»æ¥æ”¶" in str(e):
                        continue
                    logger.error(f"âŒ é€’å½’è½¬å­˜æ–‡ä»¶åŒ…å¤±è´¥: {e}")
        
        return share_links

    async def get_share_status(self, share_url: str):
        """Check the current status of a share link
        
        Returns:
            dict: {
                "share_state": int,
                "is_auditing": bool,
                "is_expired": bool,
                "is_prohibited": bool,
                "title": str
            }
        """
        try:
            payload = share_extract_payload(share_url)
            snap_resp = await self._api_call_with_timeout(
                self.client.share_snap_app, payload, async_=True,
                timeout=API_TIMEOUT, label="share_snap(status)",
                **self._get_ios_ua_kwargs()
            )
            check_response(snap_resp)
            
            data = snap_resp.get("data", {})
            share_info = data.get("shareinfo" if "shareinfo" in data else "share_info", {})
            share_state = data.get("share_state", share_info.get("share_state", share_info.get("status")))
            share_title = share_info.get("share_title", "")
            have_vio_file = share_info.get("have_vio_file", 0)
            
            is_snapshotting = "æ­£åœ¨ç”Ÿæˆæ–‡ä»¶å¿«ç…§" in str(snap_resp)
            res = {
                "share_state": share_state,
                "is_auditing": share_state == 0,
                "is_snapshotting": is_snapshotting,
                "is_pending": share_state == 0 or is_snapshotting,
                "is_expired": share_state == 7,
                "is_prohibited": have_vio_file == 1,
                "title": share_title
            }
            if is_snapshotting:
                logger.info(f"ğŸ“Š æ£€æŸ¥é“¾æ¥å‘ç°æ­£åœ¨ç”Ÿæˆå¿«ç…§: {share_url}")
            logger.debug(f"ğŸ“Š æ£€æŸ¥é“¾æ¥çŠ¶æ€: {share_url} -> {res}")
            return res
        except Exception as e:
            error_msg = str(e)
            # æ£€æŸ¥æ˜¯å¦ä¸ºé“¾æ¥å¤±æ•ˆæˆ–å–æ¶ˆé”™è¯¯ (errno 4100009 æˆ– 4100010)
            if any(code in error_msg for code in ["4100009", "4100010"]) or \
               any(msg in error_msg for msg in ["é“¾æ¥å·²å¤±æ•ˆ", "åˆ†äº«å·²å–æ¶ˆ"]):
                logger.warning(f"â° æ£€æŸ¥é“¾æ¥çŠ¶æ€å‘ç°é“¾æ¥å·²å¤±æ•ˆæˆ–è¢«å–æ¶ˆ: {share_url}")
                return {
                    "share_state": 7,
                    "is_auditing": False,
                    "is_expired": True,
                    "is_prohibited": False,
                    "title": ""
                }
            if "æ­£åœ¨ç”Ÿæˆæ–‡ä»¶å¿«ç…§" in error_msg:
                logger.info(f"ğŸ“Š æ£€æŸ¥é“¾æ¥çŠ¶æ€å‘ç°æ­£åœ¨ç”Ÿæˆå¿«ç…§: {share_url}")
                return {
                    "share_state": 0,
                    "is_auditing": False,
                    "is_snapshotting": True,
                    "is_pending": True,
                    "is_expired": False,
                    "is_prohibited": False,
                    "title": ""
                }
            logger.error(f"âŒ æ£€æŸ¥é“¾æ¥çŠ¶æ€å¤±è´¥: {share_url}, é”™è¯¯: {e}")
            return None

    async def _find_files_in_dir(self, cid: int, target_names: list) -> list:
        """åœ¨æŒ‡å®šç›®å½•ä¸­æŸ¥æ‰¾æ–‡ä»¶ï¼Œä½¿ç”¨å¤šç§æ–¹å¼ç¡®ä¿æ‰¾åˆ°
        
        ä¼˜å…ˆä½¿ç”¨ fs_searchï¼ˆæŒ‰æ–‡ä»¶åæœç´¢ï¼‰ï¼Œå¤±è´¥åå›é€€åˆ° fs_filesï¼ˆåˆ—ç›®å½•ï¼‰ã€‚
        
        Args:
            cid: ç›®å½• ID
            target_names: è¦æŸ¥æ‰¾çš„æ–‡ä»¶ååˆ—è¡¨
            
        Returns:
            åŒ¹é…çš„æ–‡ä»¶åˆ—è¡¨ [{fid, name, size, time}, ...]
        """
        matched = []
        
        # æ–¹å¼ 1: ä½¿ç”¨ fs_search æŒ‰æ–‡ä»¶åæœç´¢ï¼ˆæ›´å¯é ï¼Œä¸ä¾èµ–ç›®å½•ç¼“å­˜ï¼‰
        for name in target_names:
            try:
                search_resp = await self._api_call_with_timeout(
                    self.client.fs_search_app2,
                    {"search_value": name, "cid": cid, "limit": 20},
                    async_=True,
                    timeout=30, max_retries=2, label=f"fs_search({name})",
                    **self._get_ios_ua_kwargs()
                )
                check_response(search_resp)
                search_data = search_resp.get("data", [])
                
                # fs_search çš„ç»“æœå¯èƒ½åœ¨ data æ•°ç»„æˆ– data.list ä¸­
                if isinstance(search_data, dict):
                    search_items = search_data.get("list", [])
                else:
                    search_items = search_data
                
                logger.debug(f"ğŸ” fs_search '{name}' åœ¨ CID:{cid} è¿”å› {len(search_items)} æ¡ç»“æœ")
                
                for item in search_items:
                    item_name = item.get("n") or item.get("fn") or item.get("name") or item.get("file_name") or item.get("title") or item.get("category_name")
                    if item_name == name:
                        item_id = item.get("fid") or item.get("cid") or item.get("file_id") or item.get("category_id")
                        if item_id:
                            matched.append({
                                "fid": str(item_id),
                                "name": item_name,
                                "size": item.get("s", item.get("file_size", 0)),
                                "time": item.get("te", 0),
                            })
                            logger.info(f"ğŸ“„ fs_search æ‰¾åˆ°: {item_name} (ID: {item_id})")
                            break
            except Exception as e:
                logger.warning(f"âš ï¸ fs_search æœç´¢ '{name}' å¤±è´¥: {e}")
        
        if len(matched) == len(target_names):
            return matched
        
        # æ–¹å¼ 2: å›é€€åˆ° fs_files åˆ—ç›®å½•
        found_names = {m["name"] for m in matched}
        remaining_names = [n for n in target_names if n not in found_names]
        logger.info(f"ğŸ” fs_search æ‰¾åˆ° {len(matched)}/{len(target_names)} ä¸ªæ–‡ä»¶ï¼Œå°è¯• fs_files æŸ¥æ‰¾å‰©ä½™: {remaining_names}")
        
        try:
            resp = await self._api_call_with_timeout(
                self.client.fs_files_app2,
                {"cid": cid, "limit": 500, "show_dir": 1},
                async_=True,
                timeout=30, max_retries=2, label="fs_files",
                **self._get_ios_ua_kwargs()
            )
            check_response(resp)
            file_list = resp.get("data", [])
            
            # æ£€æŸ¥ data çš„ç±»å‹ï¼Œå…¼å®¹ä¸åŒå“åº”æ ¼å¼
            if isinstance(file_list, dict):
                file_list = file_list.get("list", [])
            
            # è·å–å“åº”ä¸­çš„å®é™… CIDï¼ŒéªŒè¯æ˜¯å¦æ­£ç¡®åˆ—å‡ºäº†ç›®æ ‡ç›®å½•
            resp_path = resp.get("path", [])
            resp_cid = None
            if resp_path:
                last_path = resp_path[-1] if isinstance(resp_path, list) else resp_path
                resp_cid = last_path.get("cid") if isinstance(last_path, dict) else None
            
            actual_count = resp.get("count", "?")
            logger.debug(f"ğŸ“‚ fs_files CID:{cid} è¿”å› {len(file_list)} é¡¹ (æ€»æ•°: {actual_count}, è·¯å¾„CID: {resp_cid})")
            
            # éªŒè¯è¿”å›çš„æ˜¯å¦æ˜¯æ­£ç¡®çš„ç›®å½•ï¼ˆé˜²æ­¢ CID ä¸å­˜åœ¨æ—¶å›é€€åˆ°æ ¹ç›®å½•ï¼‰
            if resp_cid is not None and str(resp_cid) != str(cid):
                logger.warning(f"âš ï¸ fs_files è¿”å›çš„ç›®å½• CID({resp_cid}) ä¸è¯·æ±‚çš„ CID({cid}) ä¸åŒ¹é…ï¼å¯èƒ½ç›®å½•ä¸å­˜åœ¨")
            
            # æ—¥å¿—æ‰“å°ç›®å½•ä¸­çš„å‰10ä¸ªæ–‡ä»¶åï¼Œä¾¿äºæ’æŸ¥
            if file_list:
                dir_file_names = [(item.get("n") or item.get("fn") or item.get("name") or item.get("file_name") or item.get("title") or item.get("category_name") or f"? (keys: {list(item.keys())})") for item in file_list[:10]]
                logger.debug(f"ğŸ“‹ ç›®å½•å†…æ–‡ä»¶(å‰10): {dir_file_names}")
            
            for item in file_list:
                item_name = item.get("n") or item.get("fn") or item.get("name") or item.get("file_name") or item.get("title") or item.get("category_name")
                if item_name in remaining_names:
                    item_id = item.get("fid") or item.get("cid") or item.get("file_id") or item.get("category_id") or item.get("id")
                    if item_id:
                        matched.append({
                            "fid": str(item_id),
                            "name": item_name,
                            "size": item.get("s", 0),
                            "time": item.get("te", 0),
                        })
                        logger.info(f"ğŸ“„ fs_files æ‰¾åˆ°: {item_name} (ID: {item_id})")
                        
        except Exception as e:
            logger.warning(f"âš ï¸ fs_files åˆ—ç›®å½•å¤±è´¥: {e}")
        
        return matched

    async def create_share_link(self, save_result: dict):
        if not self.client or not save_result:
            return None
            
        to_cid = save_result.get("to_cid")
        names = save_result.get("names", [])
        
        try:
            # 5. Wait for a short time to allow 115 to start processing
            logger.info(f"â³ ç­‰å¾… 2 ç§’ä»¥ç¡®ä¿æ–‡ä»¶ä¿å­˜å¼€å§‹...")
            await asyncio.sleep(2)
            
            # 6. Find files with polling (using search + list as fallback)
            new_fids = []
            matched_files = []
            
            max_poll_attempts = 10  # å¢åŠ å°è¯•æ¬¡æ•°ï¼Œä½†ç”±äºé—´éš”ç¼©çŸ­ï¼Œæ€»æ—¶é—´å…¶å®å‡å°‘äº†
            for poll_attempt in range(1, max_poll_attempts + 1):
                try:
                    logger.info(f"ğŸ” æ­£åœ¨æŸ¥æ‰¾æ–‡ä»¶ (ç¬¬ {poll_attempt}/{max_poll_attempts} æ¬¡), ç›®æ ‡ç›®å½• CID: {to_cid}")
                    current_matched = await self._find_files_in_dir(to_cid, names)
                    
                    if current_matched:
                        # ä¼˜åŒ–ï¼šå¦‚æœæ‰¾åˆ°çš„æ‰€æœ‰æ–‡ä»¶åå’Œé¢„æœŸä¸€è‡´ä¸”æ•°é‡ç›¸ç­‰ï¼Œç«‹å³è®¤ä¸ºå®Œæˆ
                        if len(current_matched) == len(names):
                            logger.info(f"âœ… æ–‡ä»¶å·²å…¨éƒ¨åˆ°è¾¾ï¼Œå…± {len(current_matched)} ä¸ªï¼Œç«‹å³ç»§ç»­")
                            new_fids = [f["fid"] for f in current_matched]
                            break
                        
                        # å¦‚æœè¿˜æ²¡å‡‘é½ï¼Œå†å¯¹æ¯”ä¸‹çŠ¶æ€æ˜¯å¦ç¨³å®šï¼ˆæ—§é€»è¾‘ä½œä¸ºä¿åº•ï¼‰
                        if matched_files:
                            stable = len(current_matched) == len(matched_files)
                            if stable:
                                for curr, prev in zip(sorted(current_matched, key=lambda x: x["fid"]), 
                                                     sorted(matched_files, key=lambda x: x["fid"])):
                                    if curr["fid"] != prev["fid"] or curr["size"] != prev["size"]:
                                        stable = False
                                        break
                            
                            if stable:
                                logger.info(f"âœ… æ–‡ä»¶çŠ¶æ€å·²ç¨³å®šï¼Œæ£€æµ‹åˆ° {len(current_matched)} ä¸ªæ–‡ä»¶")
                                new_fids = [f["fid"] for f in current_matched]
                                break
                            else:
                                logger.debug(f"ğŸ”„ æ–‡ä»¶çŠ¶æ€å˜åŒ–ä¸­ (ç¬¬ {poll_attempt}/{max_poll_attempts} æ¬¡è½®è¯¢)")
                        
                        matched_files = current_matched
                        
                        if poll_attempt < max_poll_attempts:
                            await asyncio.sleep(2)
                    else:
                        logger.warning(f"âš ï¸ è½®è¯¢æœªæ‰¾åˆ°æ–‡ä»¶ (ç¬¬ {poll_attempt}/{max_poll_attempts} æ¬¡)")
                        if poll_attempt < max_poll_attempts:
                            await asyncio.sleep(2)
                            
                except Exception as e:
                    logger.warning(f"âš ï¸ æŸ¥æ‰¾æ–‡ä»¶å¤±è´¥ (è½®è¯¢ {poll_attempt}/{max_poll_attempts}): {e}")
                    if poll_attempt < max_poll_attempts:
                        await asyncio.sleep(5)
            
            # If polling didn't find stable files, use the last matched files
            if not new_fids and matched_files:
                logger.info(f"âš ï¸ æ–‡ä»¶æœªå®Œå…¨ç¨³å®šï¼Œä½†ä½¿ç”¨ {len(matched_files)} ä¸ªå·²åŒ¹é…çš„æ–‡ä»¶å°è¯•åˆ›å»ºåˆ†äº«")
                new_fids = [f["fid"] for f in matched_files]
            
            if not new_fids:
                logger.warning(f"âš ï¸ åœ¨ä¿å­˜ç›®å½• {to_cid} ä¸­æœªæ‰¾åˆ°å¯¹åº”çš„æ–‡ä»¶ {names}ï¼Œå¯èƒ½ 115 å¤„ç†å»¶è¿Ÿæˆ–ä¿å­˜å¤±è´¥")
                return None
            
            # 7. Create new share with retry mechanism and split if > 10,000 files
            share_links = []
            fids_str_list = [str(fid) for fid in new_fids]
            max_share_retries = 3
            
            # Split fids into batches of 10,000 to respect 115 limits
            for batch_idx, i in enumerate(range(0, len(fids_str_list), 10000), 1):
                batch_fids = fids_str_list[i:i+10000]
                batch_share_code = None
                batch_receive_code = None
                
                for retry_attempt in range(1, max_share_retries + 1):
                    try:
                        logger.info(f"ğŸ“¤ æ­£åœ¨åˆ›å»ºåˆ†äº«é“¾æ¥ (åˆ†å· {batch_idx}, å°è¯• {retry_attempt}/{max_share_retries})...")
                        send_resp = await self._api_call_with_timeout(
                            self.client.share_send_app, ",".join(batch_fids), async_=True,
                            timeout=API_TIMEOUT, max_retries=1, label=f"share_send_batch_{batch_idx}",
                            **self._get_ios_ua_kwargs()
                        )
                        check_response(send_resp)
                        
                        data = send_resp["data"]
                        batch_share_code = data.get("share_code")
                        batch_receive_code = data.get("receive_code") or data.get("recv_code")
                        
                        logger.info(f"âœ… åˆ†äº«åˆ†å· {batch_idx} åˆ›å»ºæˆåŠŸ: {batch_share_code}")
                        break
                        
                    except Exception as share_error:
                        error_str = str(share_error)
                        if ("4100005" in error_str or "å·²è¢«ç§»åŠ¨æˆ–åˆ é™¤" in error_str) and retry_attempt < max_share_retries:
                            logger.warning(f"âš ï¸ æ–‡ä»¶å°šæœªå°±ç»ªï¼Œç­‰å¾… 5 ç§’åé‡è¯•...")
                            await asyncio.sleep(5)
                        else:
                            logger.error(f"âŒ åˆ›å»ºåˆ†äº«åˆ†å· {batch_idx} å¤±è´¥: {share_error}")
                            if batch_idx == 1: raise # If even the first batch fails, raise
                            break # Otherwise skip this batch
                
                if batch_share_code:
                    # Update share to permanent
                    try:
                        logger.info(f"ğŸ”„ æ­£åœ¨å°†åˆ†äº«é“¾æ¥ {batch_share_code} è½¬æ¢ä¸ºé•¿æœŸæœ‰æ•ˆ...")
                        await self._api_call_with_timeout(
                            self.client.share_update_app, {"share_code": batch_share_code, "share_duration": -1},
                            async_=True, timeout=API_TIMEOUT, max_retries=2, label=f"share_update_{batch_idx}",
                            **self._get_ios_ua_kwargs()
                        )
                    except Exception as e:
                        logger.warning(f"âš ï¸ è½¬æ¢é•¿æœŸåˆ†äº«å¤±è´¥ (åˆ†å· {batch_idx}): {e}")
                    
                    full_link = f"https://115.com/s/{batch_share_code}"
                    if batch_receive_code:
                        full_link += f"?password={batch_receive_code}"
                    share_links.append(full_link)
            
            if not share_links:
                logger.error("âŒ æœªèƒ½ç”Ÿæˆä»»ä½•åˆ†äº«é“¾æ¥")
                return None
            
            # Format multi-link response if split occurred
            if len(share_links) > 1:
                formatted_links = []
                for idx, link in enumerate(share_links, 1):
                    formatted_links.append(f"é“¾æ¥ {idx}: {link}")
                result_share = "\n".join(formatted_links)
                logger.info(f"ğŸ”— å·²ç”Ÿæˆ {len(share_links)} ä¸ªåˆ†å·åˆ†äº«é“¾æ¥")
            else:
                result_share = share_links[0]
                logger.info(f"ğŸ”— é•¿æœŸåˆ†äº«é“¾æ¥å·²ç”Ÿæˆ: {result_share}")
                
            return result_share
            
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºæ–°åˆ†äº«é“¾æ¥å¤±è´¥: {e}")
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç”±äºè¿è§„å¯¼è‡´çš„ç©ºæ–‡ä»¶å¤¹åˆ†äº«å¤±è´¥ (errno 4100016)
            error_info = getattr(e, "args", [None, {}])[1] if hasattr(e, "args") and len(e.args) >= 2 else {}
            errno_val = error_info.get("errno") if isinstance(error_info, dict) else None
            
            if errno_val == 4100016 and save_result.get("have_vio"):
                return {
                    "status": "error",
                    "error_type": "violated",
                    "message": "é“¾æ¥åŒ…å«è¿è§„å†…å®¹ï¼Œæ— æ³•è½¬å­˜åˆ†äº«"
                }
            
            # æ£€æŸ¥åˆ†äº«é™åˆ¶
            error_msg = str(e)
            if "é™åˆ¶åˆ†äº«" in error_msg:
                logger.warning(f"ğŸš« è§¦å‘ 115 åˆ†äº«é™åˆ¶")
                self.set_restriction(hours=1.0)
                return {
                    "status": "pending",
                    "reason": "restricted",
                    "share_url": save_result.get("share_url"),
                    "metadata": save_result.get("metadata", {})
                }

            return None

    async def cleanup_save_directory(self, wait: bool = True):
        """Clean up the save directory by deleting the entire folder (with locking)."""
        try:
            async with self._acquire_task_lock("cleanup", wait=wait):
                return await self._cleanup_save_directory_internal()
        except BlockingIOError:
            return False

    async def _cleanup_save_directory_internal(self) -> bool:
        """Internal logic to clean up the save directory (no locking)."""
        try:
            logger.info(f"ğŸ§¹ å¼€å§‹æ¸…ç†ä¿å­˜ç›®å½•: {settings.P115_SAVE_DIR}")
            cid = await self._ensure_save_dir()
            if not cid:
                return False
            
            resp = await self._api_call_with_timeout(
                self.client.fs_delete, cid, async_=True,
                timeout=API_TIMEOUT, label="fs_delete",
                **self._get_ios_ua_kwargs()
            )
            check_response(resp)
            
            self.clear_save_dir_cache()
            logger.info("âœ… ä¿å­˜ç›®å½•æ¸…ç†å®Œæˆ")
            return True
        except Exception as e:
            logger.error(f"âŒ å†…éƒ¨æ¸…ç†ä¿å­˜ç›®å½•å¤±è´¥: {e}")
            return False

    async def get_storage_stats(self) -> Tuple[int, int]:
        """Get storage stats (used, total) of 115 Drive in bytes"""
        if not self.client:
            return 0, 0
        try:
            resp = await self._api_call_with_timeout(
                self.client.user_space_info, async_=True,
                timeout=API_TIMEOUT, label="user_space_info",
                **self._get_ios_ua_kwargs()
            )
            check_response(resp)
            data = resp.get("data", {})
            
            def extract_size(val) -> int:
                if isinstance(val, dict):
                    # Handle cases like {'size': '...', 'size_format': '...'} or {'size_total': ...}
                    return int(val.get("size") or val.get("size_total") or val.get("size_use") or 0)
                try:
                    return int(val) if val is not None else 0
                except (ValueError, TypeError):
                    return 0

            # Try common keys for used and total space
            used = extract_size(data.get("all_used") or data.get("all_use") or data.get("used") or 0)
            total = extract_size(data.get("all_total") or data.get("total") or 0)
            
            return used, total
        except Exception as e:
            logger.error("âŒ è·å–ç½‘ç›˜å®¹é‡å¤±è´¥: {}", str(e))
            return 0, 0

    async def check_and_prepare_capacity(self, file_count: int = 0, total_size: int = 0):
        """Check capacity and optionally clean up before starting a task (internal/no-lock).
        
        Trigger cleanup if:
        1. file_count > 500 AND total_size > remainder (Avoid predictive cleanup if space is enough)
        2. Space is tighter than configured threshold (Target maintenance)
        """
        if not settings.P115_CLEANUP_CAPACITY_ENABLED:
            return

        used_bytes, total_bytes = await self.get_storage_stats()
        if total_bytes == 0:
            return
            
        remaining_bytes = total_bytes - used_bytes

        # 1. Predictive cleanup for batch tasks
        # Only cleanup if we have many files AND they might not fit
        if file_count > 500 and total_size > remaining_bytes:
            logger.info(f"ğŸš€ é¢„æµ‹æ€§æ¸…ç†ï¼šæ£€æµ‹åˆ°å¤§æ‰¹é‡æ–‡ä»¶ ({file_count} ä¸ª, {total_size/(1024**3):.2f}GB)ï¼Œå‰©ä½™ç©ºé—´ä¸è¶³ï¼Œæ‰§è¡Œæ¸…ç†...")
            await self._do_cleanup_logic()
            await asyncio.sleep(3) # Wait for 115 to sync
            return

        # 2. Threshold-based maintenance cleanup
        # Modified: Only cleanup if the new file(s) won't fit, regardless of threshold
        # If total_size is 0 (unknown), we skip cleanup unless we are critically low (e.g. < 1GB)
        # But per user request: "remove the logic that cleans up just because it's over threshold"
        
        if total_size > 0 and total_size > remaining_bytes:
             logger.warning(f"âš ï¸ å‰©ä½™ç©ºé—´ä¸è¶³ (éœ€ {total_size/(1024**3):.2f}GB, å‰© {remaining_bytes/(1024**3):.2f}GB)ï¼Œæ‰§è¡Œæ¸…ç†...")
             await self._do_cleanup_logic()
             await asyncio.sleep(3)

    async def check_capacity_and_cleanup(self, mode: str = "manual"):
        """Check current capacity and trigger cleanup if it exceeds limit.
        
        Args:
            mode: "manual", "scheduled", or "batch"
        """
        # Determine if we should wait for the lock
        wait_for_lock = True
        if mode == "scheduled":
            wait_for_lock = False # Skip if busy
            # æå‰æ£€æŸ¥é”ï¼Œä»¥ä¾¿åœ¨è½¬å­˜è¿è¡Œæ—¶ç»™å‡ºæ˜ç¡®çš„â€œè·³è¿‡â€æ—¥å¿—ï¼Œå³ä¾¿ç©ºé—´å……è¶³ä¹Ÿå‘ŠçŸ¥ç”¨æˆ·
            try:
                async with self._acquire_task_lock("capacity_check_probe", wait=False):
                    pass
            except BlockingIOError:
                logger.info("â­ï¸ å®šæ—¶å®¹é‡æ£€æŸ¥ï¼šæ£€æµ‹åˆ°è½¬å­˜ä»»åŠ¡è¿è¡Œä¸­ï¼ŒæŒ‰è®¡åˆ’è·³è¿‡é”å®šç›‘æµ‹")
                return False
        
        logger.debug(f"ğŸ” [å®¹é‡æ£€æŸ¥] æ¨¡å¼: {mode}, æ­£åœ¨è·å–å­˜å‚¨çŠ¶æ€...")
            
        # 1. Determine the threshold
        # If batch mode and auto-cleanup is disabled, use 10% fallback
        use_fallback = (mode == "batch" and not settings.P115_CLEANUP_CAPACITY_ENABLED)
        
        limit = settings.P115_CLEANUP_CAPACITY_LIMIT
        unit = settings.P115_CLEANUP_CAPACITY_UNIT
        
        used_bytes, total_bytes = await self.get_storage_stats()
        if total_bytes <= 0:
            return False

        should_cleanup = False
        
        if use_fallback:
            # check for 10% remaining
            if (total_bytes - used_bytes) < (total_bytes * 0.1):
                logger.warning(f"ğŸš¨ [æ‰¹é‡ä»»åŠ¡] å‰©ä½™ç©ºé—´ä¸è¶³ 10% ({(total_bytes-used_bytes)/(1024**4):.2f}TB)ï¼Œè§¦å‘ç¡¬æ€§æ¸…ç†")
                should_cleanup = True
        elif settings.P115_CLEANUP_CAPACITY_ENABLED and limit > 0:
            limit_bytes = limit * (1024**4) if unit == "TB" else limit * (1024**3)
            if used_bytes > limit_bytes:
                logger.info(f"ğŸ“Š [{mode}] ç½‘ç›˜å·²ç”¨ç©ºé—´ ({used_bytes/(1024**4):.2f}TB) è¶…è¿‡é˜ˆå€¼ ({limit} {unit})")
                should_cleanup = True
        
        if should_cleanup or mode == "manual":
            # Execute cleanup with non-blocking support for scheduled tasks
            try:
                # We don't acquire the lock here directly, but pass wait down to atomic cleanup methods
                # which DO acquire the lock. 
                # Actually, check_capacity_and_cleanup held lock in original version.
                # Let's wrap the actual cleanup calls in the lock.
                async with self._acquire_task_lock("cleanup", wait=wait_for_lock):
                    logger.info(f"ğŸ§¹ æ‰§è¡Œå®¹é‡ç®¡ç†æ¸…ç† (æ¨¡å¼: {mode})...")
                    # Note: we call internal versions or handle logic here to avoid re-acquiring lock
                    # But cleanup_save_directory has its own lock. So we need a way to bypass it oré€ä¼ .
                    # Best is to have an internal _cleanup method.
                    await self._do_cleanup_logic()
                    return True
            except BlockingIOError:
                if mode == "scheduled":
                    # ç†è®ºä¸Šè¿™é‡Œç”±äºä¹‹å‰çš„ probe ä¸ä¼šè½»æ˜“è§¦å‘ï¼Œä½†ä½œä¸ºå®‰å…¨å…œåº•ä¿ç•™
                    logger.info("â­ï¸ å®šæ—¶å®¹é‡æ£€æŸ¥ï¼šè½¬å­˜é”è·å–å†²çªï¼ŒæŒ‰è®¡åˆ’è·³è¿‡ä»»åŠ¡")
                return False
        else:
            # Always log available space for debugging
            logger.debug(f"âœ… [å®¹é‡æ£€æŸ¥] æ¨¡å¼: {mode}, å½“å‰ç©ºé—´å……è¶³ ({used_bytes/(1024**4):.2f}TB)ï¼Œæ— éœ€æ¸…ç†")
        return False

    async def _do_cleanup_logic(self):
        """Helper to execute both cleanup tasks without lock acquisition."""
        await self._cleanup_save_directory_internal()
        await self._cleanup_recycle_bin_internal()

    async def get_history_link(self, original_url: str) -> Optional[Union[str, list[str]]]:
        """Check if a link has been processed before. Returns string or list of strings."""
        try:
            import json
            from app.models.schema import LinkHistory
            async with async_session() as session:
                result = await session.execute(
                    select(LinkHistory).where(LinkHistory.original_url == original_url)
                )
                record = result.scalar_one_or_none()
                if record:
                    link_val = record.share_link
                    if link_val.startswith("[") and link_val.endswith("]"):
                        try:
                            return json.loads(link_val)
                        except:
                            return link_val
                    return link_val
            return None
        except Exception as e:
            logger.error(f"æŸ¥è¯¢å†å²è®°å½•å¤±è´¥: {e}")
            return None

    async def save_history_link(self, original_url: str, share_link: Union[str, list[str]]):
        """Save processed link(s) to history. share_link can be a list."""
        try:
            import json
            from app.models.schema import LinkHistory
            
            # Convert list to JSON string
            if isinstance(share_link, list):
                if not share_link:
                    return
                # If only one link, store as string, otherwise JSON
                link_to_store = json.dumps(share_link) if len(share_link) > 1 else share_link[0]
            else:
                link_to_store = share_link

            async with async_session() as session:
                existing = await session.execute(
                    select(LinkHistory).where(LinkHistory.original_url == original_url)
                )
                record = existing.scalar_one_or_none()
                if record:
                    record.share_link = link_to_store
                else:
                    new_record = LinkHistory(original_url=original_url, share_link=link_to_store)
                    session.add(new_record)
                await session.commit()
                logger.info(f"å·²ä¿å­˜å†å²è®°å½•: {original_url} -> {link_to_store[:50]}...")
        except Exception as e:
            logger.error(f"ä¿å­˜å†å²è®°å½•å¤±è´¥: {e}")

    async def delete_all_history_links(self):
        """Clear all history links"""
        try:
            from app.models.schema import LinkHistory
            from sqlalchemy import delete
            async with async_session() as session:
                await session.execute(delete(LinkHistory))
                await session.commit()
                logger.info("å·²æ¸…ç©ºæ‰€æœ‰å†å²è®°å½•")
                return True
        except Exception as e:
            logger.error(f"æ¸…ç©ºå†å²è®°å½•å¤±è´¥: {e}")
            return False

    async def cleanup_recycle_bin(self, wait: bool = True):
        """Empty the recycle bin (with locking)."""
        try:
            async with self._acquire_task_lock("cleanup", wait=wait):
                return await self._cleanup_recycle_bin_internal()
        except BlockingIOError:
            return False

    async def _cleanup_recycle_bin_internal(self) -> bool:
        """Internal logic to empty the recycle bin (no locking)."""
        try:
            logger.info("ğŸ—‘ï¸ å¼€å§‹æ¸…ç©ºå›æ”¶ç«™...")
            payload = {}
            if settings.P115_RECYCLE_PASSWORD:
                payload["password"] = settings.P115_RECYCLE_PASSWORD
                logger.debug("ä½¿ç”¨å›æ”¶ç«™å¯†ç ")
            
            resp = await self._api_call_with_timeout(
                self.client.recyclebin_clean_app, payload, async_=True,
                timeout=API_TIMEOUT, label="recyclebin_clean",
                **self._get_ios_ua_kwargs()
            )
            check_response(resp)
            logger.info("âœ… å›æ”¶ç«™å·²æ¸…ç©º")
            return True
        except Exception as e:
            logger.error("âŒ å†…éƒ¨æ¸…ç©ºå›æ”¶ç«™å¤±è´¥: {}", e)
            return False

    
p115_service = P115Service()
