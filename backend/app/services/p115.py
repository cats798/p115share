from p115client import P115Client, check_response
from p115client.fs import P115FileSystem
from p115client.util import share_extract_payload
from app.core.config import settings
from loguru import logger
import asyncio
import time
from contextlib import asynccontextmanager
from typing import Literal, Optional
from app.core.database import async_session
from app.models.schema import PendingLink, LinkHistory
from sqlalchemy import select, delete

class P115Service:
    def __init__(self):
        self.client = None
        self.fs = None
        self.is_connected = False
        self._task_lock: Optional[asyncio.Lock] = None  # Lazy initialize
        self._current_task: str | None = None  # Track current task type
        if settings.P115_COOKIE:
            self.init_client(settings.P115_COOKIE)

    def init_client(self, cookie: str):
        try:
            # Apply proxy settings to environment if configured
            import os
            if settings.PROXY_ENABLED and settings.PROXY_HOST and settings.PROXY_PORT:
                proxy_type = settings.PROXY_TYPE.lower()
                auth = f"{settings.PROXY_USER}:{settings.PROXY_PASS}@" if settings.PROXY_USER and settings.PROXY_PASS else ""
                proxy_url = f"{proxy_type}://{auth}{settings.PROXY_HOST}:{settings.PROXY_PORT}"
                
                os.environ['HTTP_PROXY'] = proxy_url
                os.environ['http_proxy'] = proxy_url
                os.environ['HTTPS_PROXY'] = proxy_url
                os.environ['https_proxy'] = proxy_url
                
            self.client = P115Client(cookie, check_for_relogin=True)
            self.fs = P115FileSystem(self.client)
            
            proxy_info = ""
            if settings.PROXY_ENABLED:
                proxy_info = f" (Proxy: {settings.PROXY_TYPE}://{settings.PROXY_HOST}:{settings.PROXY_PORT})"
            logger.info(f"P115Client and FileSystem initialized successfully{proxy_info}")
            # Verify connection asynchronously
            asyncio.create_task(self.verify_connection())
        except Exception as e:
            logger.error(f"Failed to initialize P115Client: {e}")
            self.client = None
            self.fs = None
            self.is_connected = False

    @asynccontextmanager
    async def _acquire_task_lock(self, task_type: Literal["save_share", "cleanup"]):
        """Acquire task lock with waiting logic"""
        if self._task_lock is None:
            self._task_lock = asyncio.Lock()
            
        max_wait = 300  # 5 minutes max wait
        start_time = time.time()
        
        while self._task_lock.locked():
            if time.time() - start_time > max_wait:
                raise TimeoutError(f"ç­‰å¾…ä»»åŠ¡é”è¶…æ—¶: {task_type}")
            logger.info(f"â³ {task_type} ä»»åŠ¡ç­‰å¾…ä¸­ï¼Œå½“å‰ä»»åŠ¡: {self._current_task}")
            await asyncio.sleep(5)
        
        async with self._task_lock:
            self._current_task = task_type
            logger.info(f"ğŸ”’ {task_type} ä»»åŠ¡å·²è·å–é”")
            try:
                yield
            finally:
                self._current_task = None
                logger.info(f"ğŸ”“ {task_type} ä»»åŠ¡å·²é‡Šæ”¾é”")

    async def verify_connection(self) -> bool:
        """Verify the 115 cookie connection"""
        if not self.client:
            self.is_connected = False
            return False
            
        try:
            # Simple API call to verify cookie
            resp = await self.client.user_info(async_=True)
            if resp.get("state"):
                self.is_connected = True
                logger.info("âœ… 115 ç½‘ç›˜ç™»å½•éªŒè¯æˆåŠŸ")
                return True
        except Exception as e:
            logger.error(f"âŒ 115 ç½‘ç›˜ç™»å½•éªŒè¯å¤±è´¥: {e}")
            self.is_connected = False
            return False
            
        self.is_connected = False
        return False

    async def _ensure_save_dir(self):
        """Ensure the save directory exists and return its CID"""
        path = settings.P115_SAVE_DIR or "/åˆ†äº«ä¿å­˜"
        logger.info(f"ğŸ” å¼€å§‹æ£€æŸ¥/åˆ›å»ºä¿å­˜ç›®å½•: {path}")
        
        if not self.client:
            logger.warning("âš ï¸ Client not initialized")
            return 0
        
        try:
            # fs_makedirs_app creates the directory if it doesn't exist
            # and returns the final directory's info
            logger.info(f"ğŸ“ è°ƒç”¨ fs_makedirs_app åˆ›å»ºç›®å½•...")
            resp = await self.client.fs_makedirs_app(path, pid=0, async_=True)
            logger.info(f"ğŸ“‹ fs_makedirs_app å“åº”: {resp}")
            check_response(resp)
            
            # The response structure has 'cid' at the top level (not in 'data')
            # Response format: {'state': True, 'error': '', 'errCode': 0, 'cid': '3358575817564146054'}
            cid = 0
            if "cid" in resp:
                # CID is returned as a string, convert to int
                cid = int(resp["cid"])
                logger.info(f"ğŸ”¢ ä»å“åº”ä¸­æå–åˆ° CID: {cid}")
            elif "data" in resp:
                # Fallback: check if it's in a 'data' field (for compatibility)
                data = resp["data"]
                cid = int(data.get("category_id") or data.get("cid") or data.get("id") or 0)
                logger.info(f"ğŸ”¢ ä» data å­—æ®µä¸­æå–åˆ° CID: {cid}")
            else:
                logger.error(f"âŒ å“åº”ä¸­æ²¡æœ‰ 'cid' æˆ– 'data' å­—æ®µ: {resp}")
                
            if cid == 0:
                logger.error(f"âŒ æ— æ³•ä»å“åº”è·å–æœ‰æ•ˆçš„ CID: {resp}")
                return 0
                
            logger.info(f"âœ… ä¿å­˜ç›®å½•å·²ç¡®è®¤: {path} (CID: {cid})")
            return cid
        except Exception as e:
            logger.error(f"âŒ æ— æ³•ç¡®ä¿ä¿å­˜ç›®å½• {path} å­˜åœ¨: {e}", exc_info=True)
            return 0

    async def save_share_link(self, share_url: str, metadata: dict = None):
        """Save a 115 share link to the configured directory
        
        Args:
            share_url: The 115 share URL to save
            metadata: Optional metadata dict containing description, full_text, photo_id, etc.
        """
        async with self._acquire_task_lock("save_share"):
            if not self.client:
                logger.warning("P115Client not initialized, cannot save link")
                return None
            
            logger.info(f"ğŸ“¥ å¼€å§‹å¤„ç†åˆ†äº«é“¾æ¥: {share_url}")
            try:
                # 1. Extract share/receive codes
                payload = share_extract_payload(share_url)
                
                # 2. Get share snapshot to get file IDs and names
                snap_resp = await self.client.share_snap(payload, async_=True)
                check_response(snap_resp)

                # Check for audit and violation status
                data = snap_resp.get("data", {})
                share_info = data.get("shareinfo" if "shareinfo" in data else "share_info", {})
                share_state = data.get("share_state", share_info.get("share_state", share_info.get("status"))) # Multiple fallbacks
                share_title = share_info.get("share_title", "")
                have_vio_file = share_info.get("have_vio_file", 0)

                if share_state == 0:
                    logger.info(f"ğŸ” åˆ†äº«é“¾æ¥å¤„äºå®¡æ ¸ä¸­ï¼Œè¿›å…¥è½®è¯¢ç­‰å¾…é˜Ÿåˆ—: {share_url}")
                    # Save to DB for persistence
                    async with async_session() as session:
                        new_task = PendingLink(
                            share_url=share_url,
                            metadata_json=metadata or {},
                            status="auditing"
                        )
                        session.add(new_task)
                        await session.commit()
                        db_id = new_task.id
                    
                    return {
                        "status": "pending",
                        "share_url": share_url,
                        "metadata": metadata or {},
                        "db_id": db_id
                    }
                
                if share_state == 7:
                    logger.warning(f"âš ï¸ åˆ†äº«é“¾æ¥å·²è¿‡æœŸ: {share_url}")
                    return {
                        "status": "error",
                        "error_type": "expired",
                        "message": "é“¾æ¥å·²è¿‡æœŸ"
                    }

                if have_vio_file == 1 or "***" in share_title:
                    logger.warning(f"ğŸš« åˆ†äº«é“¾æ¥åŒ…å«è¿è§„å†…å®¹: {share_url}")
                    return {
                        "status": "error",
                        "error_type": "violated",
                        "message": "é“¾æ¥åŒ…å«è¿è§„å†…å®¹"
                    }
                
                if share_state != 1:
                    logger.warning(f"âš ï¸ åˆ†äº«é“¾æ¥çŠ¶æ€å¼‚å¸¸ (state={share_state}): {share_url}")
                    # Allow attempt if state is unknown but not explicitly pending/expired/prohibited
                
                items = snap_resp["data"]["list"]
                if not items:
                    logger.warning("åˆ†äº«é“¾æ¥å†…æ²¡æœ‰æ–‡ä»¶")
                    return None
                
                # Extract file/folder IDs and names
                # Files use 'fid', folders use 'cid'
                fids = []
                names = []
                for item in items:
                    # Try to get fid (file) or cid (folder)
                    fid = item.get("fid") or item.get("cid")
                    if fid:
                        fids.append(str(fid))
                        names.append(item.get("n", "æœªçŸ¥"))
                    else:
                        logger.warning(f"Item missing both fid and cid: {item}")
                
                if not fids:
                    logger.error("æœªèƒ½æå–åˆ°ä»»ä½•æœ‰æ•ˆçš„æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹ ID")
                    return None
                
                logger.info(f"ğŸ“¦ æ£€æµ‹åˆ° {len(fids)} ä¸ªé¡¹ç›®: {', '.join(names[:3])}{'...' if len(names) > 3 else ''}")
                
                # 3. Ensure save directory
                to_cid = await self._ensure_save_dir()
                
                # 4. Receive files
                receive_payload = {
                    "share_code": payload["share_code"],
                    "receive_code": payload["receive_code"] or "",
                    "file_id": ",".join(fids),
                    "cid": to_cid
                }
                
                try:
                    recv_resp = await self.client.share_receive(receive_payload, async_=True)
                    check_response(recv_resp)
                    logger.info(f"âœ… é“¾æ¥è½¬å­˜æŒ‡ä»¤å·²å‘é€: {share_url} -> CID {to_cid}")
                except Exception as recv_error:
                    # Check if it's a "file already received" error (errno 4200045)
                    error_msg = str(recv_error)
                    if "4200045" in error_msg or "å·²ç»æ¥æ”¶" in error_msg:
                        logger.warning(f"âš ï¸ 115 æç¤ºæ–‡ä»¶è¯¥åˆ†äº«å·²æ¥æ”¶è¿‡: {share_url}")
                        # Verify if files really exist in to_cid
                        found_all = False
                        try:
                            # Quick check for existence
                            resp = await self.client.fs_files({"cid": to_cid, "limit": 100}, async_=True)
                            check_response(resp)
                            current_files = [item.get("n") for item in resp.get("data", [])]
                            # Check if at least one of the names exists
                            # (Partial match is better than failing completely if some were deleted)
                            found_count = sum(1 for name in names if name in current_files)
                            if found_count > 0:
                                logger.info(f"âœ… åœ¨ä¿å­˜ç›®å½•ä¸­æ‰¾åˆ° {found_count} ä¸ªåŒåæ–‡ä»¶ï¼Œç»§ç»­å¤„ç†")
                                # Continue to share creation with existing files
                            else:
                                logger.error("âŒ 115 æç¤ºå·²æ¥æ”¶ï¼Œä½†åœ¨ä¿å­˜ç›®å½•æœªæ‰¾åˆ°æ–‡ä»¶ï¼ˆå¯èƒ½å·²è¢«åˆ é™¤ï¼‰ã€‚æ— æ³•é‡æ–°è½¬å­˜åŒä¸€åˆ†äº«é“¾æ¥ã€‚")
                                return {
                                    "status": "error",
                                    "error_type": "already_exists_missing",
                                    "message": "è¯¥åˆ†äº«é“¾æ¥æ‚¨å·²è½¬å­˜è¿‡ã€‚115 é™åˆ¶åŒä¸€é“¾æ¥æ— æ³•ç”±äºæ–‡ä»¶ä¸¢å¤±è€Œé‡å¤è½¬å­˜ï¼Œè¯·å°è¯•å¯»æ‰¾åŸæ–‡ä»¶æˆ–ä»å›æ”¶ç«™è¿˜åŸã€‚"
                                }
                        except Exception as check_e:
                            logger.warning(f"âš ï¸ æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨æ—¶å‡ºé”™: {check_e}")
                            # Assume failure to be safe
                            return {
                                "status": "error", 
                                "error_type": "unknown",
                                "message": "ä¿å­˜å¤±è´¥ï¼Œä¸”æ— æ³•éªŒè¯æ–‡ä»¶æ˜¯å¦å­˜åœ¨"
                            }
                    else:
                        # Other errors, re-raise
                        raise
                
                return {
                    "status": "success", 
                    "to_cid": to_cid, 
                    "names": names,
                    "share_url": share_url,
                    "metadata": metadata or {}  # Include metadata in return value
                }
            except Exception as e:
                logger.error(f"âŒ ä¿å­˜åˆ†äº«é“¾æ¥å¤±è´¥", exc_info=True)
                return None

    async def get_share_status(self, share_url: str):
        """Check the current status of a share link
        
        Returns:
            dict: {
                "share_state": int,
                "is_auditing": bool,
                "is_expired": bool,
                "is_prohibited": bool,
                "title": str
            }
        """
        try:
            payload = share_extract_payload(share_url)
            snap_resp = await self.client.share_snap(payload, async_=True)
            check_response(snap_resp)
            
            data = snap_resp.get("data", {})
            share_info = data.get("shareinfo" if "shareinfo" in data else "share_info", {})
            share_state = data.get("share_state", share_info.get("share_state", share_info.get("status")))
            share_title = share_info.get("share_title", "")
            have_vio_file = share_info.get("have_vio_file", 0)
            
            res = {
                "share_state": share_state,
                "is_auditing": share_state == 0,
                "is_expired": share_state == 7,
                "is_prohibited": have_vio_file == 1 or "***" in share_title,
                "title": share_title
            }
            logger.debug(f"ğŸ“Š æ£€æŸ¥é“¾æ¥çŠ¶æ€: {share_url} -> {res}")
            return res
        except Exception as e:
            logger.error(f"âŒ æ£€æŸ¥é“¾æ¥çŠ¶æ€å¤±è´¥: {share_url}, é”™è¯¯: {e}")
            return None

    async def create_share_link(self, save_result: dict):
        if not self.client or not save_result:
            return None
            
        to_cid = save_result.get("to_cid")
        names = save_result.get("names", [])
        
        try:
            # 5. Wait for 10 seconds as requested
            logger.info(f"â³ ç­‰å¾… 10 ç§’ä»¥ç¡®ä¿æ–‡ä»¶ä¿å­˜å®Œæˆ...")
            await asyncio.sleep(10)
            
            # 6. Find the new file IDs with polling mechanism
            new_fids = []
            matched_files = []
            
            # Polling retry: try to find files and verify they are stable
            max_poll_attempts = 3
            for poll_attempt in range(1, max_poll_attempts + 1):
                try:
                    resp = await self.client.fs_files({"cid": to_cid, "limit": 100}, async_=True)
                    check_response(resp)
                    file_list = resp.get("data", [])
                    
                    # More precise file matching: match by name and verify with timestamp
                    current_matched = []
                    for item in file_list:
                        if item.get("n") in names:
                            # Item can be file (fid) or folder (cid)
                            item_id = item.get("fid") or item.get("cid")
                            if item_id:
                                current_matched.append({
                                    "fid": str(item_id),
                                    "name": item.get("n"),
                                    "size": item.get("s", 0),
                                    "time": item.get("te", 0)  # Modified time
                                })
                    
                    # Check if files are stable (same count and sizes as previous poll)
                    if current_matched:
                        if matched_files:
                            # Compare with previous poll: check if sizes match (file transfer complete)
                            stable = len(current_matched) == len(matched_files)
                            if stable:
                                for curr, prev in zip(sorted(current_matched, key=lambda x: x["fid"]), 
                                                     sorted(matched_files, key=lambda x: x["fid"])):
                                    if curr["fid"] != prev["fid"] or curr["size"] != prev["size"]:
                                        stable = False
                                        break
                            
                            if stable:
                                logger.info(f"âœ… æ–‡ä»¶çŠ¶æ€å·²ç¨³å®šï¼Œæ£€æµ‹åˆ° {len(current_matched)} ä¸ªæ–‡ä»¶")
                                new_fids = [f["fid"] for f in current_matched]
                                break
                            else:
                                logger.debug(f"ğŸ”„ æ–‡ä»¶çŠ¶æ€æœªç¨³å®š (ç¬¬ {poll_attempt}/{max_poll_attempts} æ¬¡è½®è¯¢)ï¼Œç»§ç»­ç­‰å¾…...")
                        
                        matched_files = current_matched
                        
                        if poll_attempt < max_poll_attempts:
                            await asyncio.sleep(3)  # Wait 3s between polls
                    else:
                        logger.warning(f"âš ï¸ è½®è¯¢æœªæ‰¾åˆ°æ–‡ä»¶ (ç¬¬ {poll_attempt}/{max_poll_attempts} æ¬¡)")
                        if poll_attempt < max_poll_attempts:
                            await asyncio.sleep(3)
                            
                except Exception as e:
                    logger.warning(f"âš ï¸ åˆ—å‡ºç›®å½•æ–‡ä»¶å¤±è´¥ (è½®è¯¢ {poll_attempt}/{max_poll_attempts}): {e}")
                    if poll_attempt < max_poll_attempts:
                        await asyncio.sleep(3)
            
            # If polling didn't find stable files, use the last matched files
            if not new_fids and matched_files:
                logger.info(f"âš ï¸ æ–‡ä»¶æœªå®Œå…¨ç¨³å®šï¼Œä½†ä½¿ç”¨ {len(matched_files)} ä¸ªå·²åŒ¹é…çš„æ–‡ä»¶å°è¯•åˆ›å»ºåˆ†äº«")
                new_fids = [f["fid"] for f in matched_files]
            
            if not new_fids:
                logger.warning(f"âš ï¸ åœ¨ä¿å­˜ç›®å½• {to_cid} ä¸­æœªæ‰¾åˆ°å¯¹åº”çš„æ–‡ä»¶ {names}ï¼Œå¯èƒ½ä¿å­˜å°šæœªå®Œæˆ")
                return None
            
            # 7. Create new share with retry mechanism
            share_code = None
            receive_code = None
            max_share_retries = 3
            
            for retry_attempt in range(1, max_share_retries + 1):
                try:
                    logger.info(f"ğŸ“¤ æ­£åœ¨åˆ›å»ºåˆ†äº«é“¾æ¥ (å°è¯• {retry_attempt}/{max_share_retries}): {', '.join(names[:3])}...")
                    send_resp = await self.client.share_send(",".join(new_fids), async_=True)
                    check_response(send_resp)
                    
                    # Extract share_code
                    data = send_resp["data"]
                    share_code = data.get("share_code")
                    receive_code = data.get("receive_code") or data.get("recv_code")
                    
                    logger.info(f"âœ… åˆ†äº«é“¾æ¥åˆ›å»ºæˆåŠŸ: {share_code}")
                    break  # Success, exit retry loop
                    
                except Exception as share_error:
                    error_str = str(share_error)
                    # Check if it's error 4100005 (file moved or deleted)
                    if "4100005" in error_str or "å·²è¢«ç§»åŠ¨æˆ–åˆ é™¤" in error_str:
                        if retry_attempt < max_share_retries:
                            logger.warning(f"âš ï¸ æ–‡ä»¶å°šæœªå°±ç»ª (é”™è¯¯ 4100005)ï¼Œç­‰å¾… 5 ç§’åé‡è¯•...")
                            await asyncio.sleep(5)
                            continue
                        else:
                            logger.error(f"âŒ é‡è¯• {max_share_retries} æ¬¡åä»å¤±è´¥: {share_error}")
                            raise
                    else:
                        # Other errors, don't retry
                        logger.error(f"âŒ åˆ›å»ºåˆ†äº«é“¾æ¥å¤±è´¥ (éæ—¶åºé—®é¢˜): {share_error}")
                        raise
            
            if not share_code:
                logger.error("âŒ æœªèƒ½è·å–åˆ° share_code")
                return None
            
            # 8. Update share to be permanent (share_duration=-1)
            if share_code:
                logger.info(f"ğŸ”„ æ­£åœ¨å°†åˆ†äº«é“¾æ¥ {share_code} è½¬æ¢ä¸ºé•¿æœŸæœ‰æ•ˆ...")
                update_payload = {
                    "share_code": share_code,
                    "share_duration": -1
                }
                update_resp = await self.client.share_update(update_payload, async_=True)
                check_response(update_resp)
                logger.debug(f"Share update response: {update_resp}")

            new_share = f"https://115.com/s/{share_code}"
            if receive_code:
                new_share += f"?password={receive_code}"
                
            logger.info(f"ğŸ”— é•¿æœŸåˆ†äº«é“¾æ¥å·²ç”Ÿæˆ: {new_share}")
            return new_share
            
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºæ–°åˆ†äº«é“¾æ¥å¤±è´¥: {e}")
            return None

    async def cleanup_save_directory(self):
        """Clean up the save directory"""
        async with self._acquire_task_lock("cleanup"):
            logger.info("ğŸ§¹ å¼€å§‹æ¸…ç†ä¿å­˜ç›®å½•...")
            try:
                save_dir_cid = await self._ensure_save_dir()
                if not save_dir_cid:
                    logger.error("æ— æ³•è·å–ä¿å­˜ç›®å½• CID")
                    return False

                # List files in save directory
                resp = await self.client.fs_files({"cid": save_dir_cid, "limit": 100}, async_=True)
                check_response(resp)
                
                file_list = resp.get("data", [])
                if not file_list:
                    logger.info("ä¿å­˜ç›®å½•ä¸ºç©ºï¼Œæ— éœ€æ¸…ç†")
                    return True
                
                # Delete all files
                fids = [item.get("fid") for item in file_list if item.get("fid")]
                if fids:
                   logger.info(f"æ­£åœ¨åˆ é™¤ {len(fids)} ä¸ªæ–‡ä»¶...")
                   del_resp = await self.client.fs_delete(fids, async_=True)
                   check_response(del_resp)
                   logger.info("æ¸…ç†å®Œæˆ")
                return True
            except Exception as e:
                logger.error(f"æ¸…ç†ä¿å­˜ç›®å½•å¤±è´¥: {e}")
                return False

    async def get_history_link(self, original_url: str) -> str | None:
        """Check if a link has been processed before"""
        try:
            from app.models.schema import LinkHistory
            async with async_session() as session:
                result = await session.execute(
                    select(LinkHistory).where(LinkHistory.original_url == original_url)
                )
                record = result.scalar_one_or_none()
                if record:
                    return record.share_link
            return None
        except Exception as e:
            logger.error(f"æŸ¥è¯¢å†å²è®°å½•å¤±è´¥: {e}")
            return None

    async def save_history_link(self, original_url: str, share_link: str):
        """Save a processed link to history"""
        try:
            from app.models.schema import LinkHistory
            async with async_session() as session:
                # Check existance first to avoid unique constraint error
                existing = await session.execute(
                    select(LinkHistory).where(LinkHistory.original_url == original_url)
                )
                if existing.scalar_one_or_none():
                    return
                
                new_record = LinkHistory(original_url=original_url, share_link=share_link)
                session.add(new_record)
                await session.commit()
                logger.info(f"å·²ä¿å­˜å†å²è®°å½•: {original_url} -> {share_link}")
        except Exception as e:
            logger.error(f"ä¿å­˜å†å²è®°å½•å¤±è´¥: {e}")

    async def delete_all_history_links(self):
        """Clear all history links"""
        try:
            from app.models.schema import LinkHistory
            from sqlalchemy import delete
            async with async_session() as session:
                await session.execute(delete(LinkHistory))
                await session.commit()
                logger.info("å·²æ¸…ç©ºæ‰€æœ‰å†å²è®°å½•")
                return True
        except Exception as e:
            logger.error(f"æ¸…ç©ºå†å²è®°å½•å¤±è´¥: {e}")
            return False

    async def cleanup_recycle_bin(self):
        """Empty the recycle bin"""
        async with self._acquire_task_lock("cleanup"):
            logger.info("ğŸ—‘ï¸ å¼€å§‹æ¸…ç©ºå›æ”¶ç«™...")
            try:
                # Prepare payload with optional password
                payload = {}
                if settings.P115_RECYCLE_PASSWORD:
                    payload["password"] = settings.P115_RECYCLE_PASSWORD
                    logger.debug("ä½¿ç”¨å›æ”¶ç«™å¯†ç ")
                
                # Call recycle bin cleanup API
                resp = await self.client.recyclebin_clean_app(payload, async_=True)
                check_response(resp)
                
                logger.info("âœ… å›æ”¶ç«™å·²æ¸…ç©º")
                return True
            except Exception as e:
                logger.error("âŒ æ¸…ç©ºå›æ”¶ç«™å¤±è´¥: {}", e)
                return False

p115_service = P115Service()
