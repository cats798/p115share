from p115client import P115Client, check_response
from p115client.fs import P115FileSystem
from p115client.util import share_extract_payload
from p115client.tool import share_iterdir_walk
from app.core.config import settings
from loguru import logger
import asyncio
import time
import random
import re
from contextlib import asynccontextmanager
from typing import Literal, Optional, Tuple, Union, List, Dict
from app.core.database import async_session
from app.models.schema import PendingLink, LinkHistory
from sqlalchemy import select, delete
from app.services.tmdb import TMDBClient, MediaOrganizer, SmartMediaAnalyzer, QualityLevel

# é»˜è®¤ API è¯·æ±‚è¶…æ—¶ï¼ˆç§’ï¼‰
API_TIMEOUT = 60
# é»˜è®¤ API é‡è¯•æ¬¡æ•°
API_MAX_RETRIES = 3
# é‡è¯•é—´éš”ï¼ˆç§’ï¼‰
API_RETRY_DELAY = 5

# iOS ç”¨æˆ·ä»£ç†
IOS_UA = (
    "Mozilla/5.0 (iPhone; CPU iPhone OS 17_5_1 like Mac OS X) "
    "AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148 115wangpan_ios/36.2.20"
)


class P115Service:
    def __init__(self):
        self.client = None
        self.fs = None
        self.is_connected = False
        self._task_lock: Optional[asyncio.Lock] = None
        self._current_task: str | None = None
        self._save_dir_cid: int = 0
        self._task_queue = asyncio.Queue()
        self._worker_task = None
        self._worker_lock = asyncio.Lock()
        self._current_task_info = None
        self._restriction_until: float = 0
        
        if settings.P115_COOKIE:
            self.init_client(settings.P115_COOKIE)

    @property
    def queue_size(self) -> int:
        return self._task_queue.qsize()

    @property
    def is_busy(self) -> bool:
        return self._current_task_info is not None or self.is_restricted

    @property
    def is_restricted(self) -> bool:
        return time.time() < self._restriction_until

    def set_restriction(self, hours: float = 1.0):
        self._restriction_until = time.time() + (hours * 3600)
        logger.warning(f"ğŸš« 115 æœåŠ¡å·²è¿›å…¥å…¨å±€é™åˆ¶æ¨¡å¼ï¼Œé¢„è®¡æŒç»­ {hours} å°æ—¶ (ç›´åˆ° {time.strftime('%H:%M:%S', time.localtime(self._restriction_until))})")

    def clear_restriction(self):
        if self._restriction_until > 0:
            self._restriction_until = 0
            logger.info("ğŸ”“ 115 å…¨å±€é™åˆ¶æ¨¡å¼å·²è§£é™¤")

    def _get_ios_ua_kwargs(self):
        return {
            "headers": {
                "user-agent": IOS_UA,
                "accept-encoding": "gzip, deflate"
            },
            "app": "ios"
        }

    async def _task_worker(self):
        logger.info("ğŸš€ P115 ä»»åŠ¡é˜Ÿåˆ— Worker å·²å¯åŠ¨")
        while True:
            task_func, args, kwargs, future, task_type = await self._task_queue.get()
            self._current_task_info = task_type
            try:
                logger.info(f"âš¡ é˜Ÿåˆ—æ­£åœ¨å¤„ç†ä»»åŠ¡: {task_type}")
                result = await task_func(*args, **kwargs)
                if not future.done():
                    future.set_result(result)
            except Exception as e:
                logger.error(f"âŒ é˜Ÿåˆ—æ‰§è¡Œä»»åŠ¡ {task_type} å‡ºé”™: {e}")
                if not future.done():
                    future.set_exception(e)
            finally:
                self._task_queue.task_done()
                self._current_task_info = None

    async def _api_call_with_timeout(
        self,
        coro_func,
        *args,
        timeout: int = API_TIMEOUT,
        max_retries: int = API_MAX_RETRIES,
        retry_delay: int = API_RETRY_DELAY,
        label: str = "API",
        **kwargs,
    ):
        last_error = None
        for attempt in range(1, max_retries + 1):
            try:
                result = await asyncio.wait_for(
                    coro_func(*args, **kwargs),
                    timeout=timeout,
                )
                return result
            except asyncio.TimeoutError:
                last_error = TimeoutError(f"{label} è¯·æ±‚è¶…æ—¶ ({timeout}s), å°è¯• {attempt}/{max_retries}")
                logger.warning(f"â±ï¸ {label} è¯·æ±‚è¶…æ—¶ (å°è¯• {attempt}/{max_retries})")
            except Exception as e:
                raise
            
            if attempt < max_retries:
                logger.info(f"ğŸ”„ {label} å°†åœ¨ {retry_delay}s åé‡è¯•...")
                await asyncio.sleep(retry_delay)
        
        raise last_error

    def init_client(self, cookie: str):
        try:
            import os
            if settings.PROXY_ENABLED and settings.PROXY_HOST and settings.PROXY_PORT:
                proxy_type = settings.PROXY_TYPE.lower()
                auth = f"{settings.PROXY_USER}:{settings.PROXY_PASS}@" if settings.PROXY_USER and settings.PROXY_PASS else ""
                proxy_url = f"{proxy_type}://{auth}{settings.PROXY_HOST}:{settings.PROXY_PORT}"
                
                os.environ['HTTP_PROXY'] = proxy_url
                os.environ['http_proxy'] = proxy_url
                os.environ['HTTPS_PROXY'] = proxy_url
                os.environ['https_proxy'] = proxy_url
                
            self.client = P115Client(cookie, check_for_relogin=True)
            self.fs = P115FileSystem(self.client)
            
            proxy_info = ""
            if settings.PROXY_ENABLED:
                proxy_info = f" (Proxy: {settings.PROXY_TYPE}://{settings.PROXY_HOST}:{settings.PROXY_PORT})"
            logger.info(f"P115Client and FileSystem initialized successfully{proxy_info}")
            asyncio.create_task(self.verify_connection())
        except Exception as e:
            logger.error(f"Failed to initialize P115Client: {e}")
            self.client = None
            self.fs = None
            self.is_connected = False

    @asynccontextmanager
    async def _acquire_task_lock(self, task_type: Literal["save_share", "cleanup"], wait: bool = True):
        yield

    async def _enqueue_op(self, task_type: str, func, *args, **kwargs):
        if self._worker_task is None or self._worker_task.done():
            async with self._worker_lock:
                if self._worker_task is None or self._worker_task.done():
                    self._worker_task = asyncio.create_task(self._task_worker())
                    logger.info("âš¡ å»¶è¿Ÿå¯åŠ¨ P115 ä»»åŠ¡é˜Ÿåˆ— Worker")

        future = asyncio.get_running_loop().create_future()
        await self._task_queue.put((func, args, kwargs, future, task_type))
        return await future

    async def verify_connection(self) -> bool:
        if not self.client:
            self.is_connected = False
            return False
            
        try:
            resp = await self._api_call_with_timeout(
                self.client.user_info, async_=True,
                timeout=30, max_retries=2, label="user_info",
                **self._get_ios_ua_kwargs()
            )
            if resp.get("state"):
                self.is_connected = True
                logger.info("âœ… 115 ç½‘ç›˜ç™»å½•éªŒè¯æˆåŠŸ")
                return True
        except Exception as e:
            logger.error(f"âŒ 115 ç½‘ç›˜ç™»å½•éªŒè¯å¤±è´¥: {e}")
            self.is_connected = False
            return False
            
        self.is_connected = False
        return False

    def clear_save_dir_cache(self):
        self._save_dir_cid = 0
        logger.debug("ğŸ—‘ï¸ å·²æ¸…é™¤ä¿å­˜ç›®å½• CID ç¼“å­˜")

    async def _ensure_save_dir(self, path: Optional[str] = None):
        is_default = path is None
        path = path or settings.P115_SAVE_DIR or "/åˆ†äº«ä¿å­˜"
        
        if is_default and self._save_dir_cid > 0:
            logger.debug(f"ğŸ“‚ ä½¿ç”¨ç¼“å­˜çš„ä¿å­˜ç›®å½• CID: {self._save_dir_cid}")
            return self._save_dir_cid
        
        logger.info(f"ğŸ” å¼€å§‹æ£€æŸ¥/åˆ›å»ºä¿å­˜ç›®å½•: {path}")
        
        if not self.client:
            raise RuntimeError("P115Client æœªåˆå§‹åŒ–ï¼Œæ— æ³•åˆ›å»ºä¿å­˜ç›®å½•")
        
        last_error = None
        for attempt in range(1, 4):
            try:
                logger.info(f"ğŸ“ è°ƒç”¨ fs_makedirs_app åˆ›å»ºç›®å½•... (å°è¯• {attempt}/3)")
                resp = await asyncio.wait_for(
                    self.client.fs_makedirs_app(path, pid=0, async_=True, **self._get_ios_ua_kwargs()),
                    timeout=30
                )
                logger.info(f"ğŸ“‹ fs_makedirs_app å“åº”: {resp}")
                check_response(resp)
                
                cid = 0
                if "cid" in resp:
                    cid = int(resp["cid"])
                    logger.info(f"ğŸ”¢ ä»å“åº”ä¸­æå–åˆ° CID: {cid}")
                elif "data" in resp:
                    data = resp["data"]
                    cid = int(data.get("category_id") or data.get("cid") or data.get("id") or 0)
                    logger.info(f"ğŸ”¢ ä» data å­—æ®µä¸­æå–åˆ° CID: {cid}")
                else:
                    logger.error(f"âŒ å“åº”ä¸­æ²¡æœ‰ 'cid' æˆ– 'data' å­—æ®µ: {resp}")
                    
                if cid == 0:
                    raise RuntimeError(f"æ— æ³•ä»å“åº”è·å–æœ‰æ•ˆçš„ CID: {resp}")
                    
                if is_default:
                    self._save_dir_cid = cid
                logger.info(f"âœ… ä¿å­˜ç›®å½•å·²ç¡®è®¤: {path} (CID: {cid})")
                return cid
                
            except asyncio.TimeoutError:
                last_error = TimeoutError(f"fs_makedirs_app è¯·æ±‚è¶…æ—¶ (30s), å°è¯• {attempt}/3")
                logger.warning(f"â±ï¸ fs_makedirs_app è¯·æ±‚è¶…æ—¶ (å°è¯• {attempt}/3)")
            except Exception as e:
                last_error = e
                logger.warning(f"âš ï¸ åˆ›å»ºç›®å½•å¤±è´¥ (å°è¯• {attempt}/3): {e}")
            
            if attempt < 3:
                await asyncio.sleep(3)
        
        raise RuntimeError(f"æ— æ³•ç¡®ä¿ä¿å­˜ç›®å½• {path} å­˜åœ¨ (å·²é‡è¯•3æ¬¡): {last_error}")

    async def _handle_already_received(self, to_cid: int, names: list[str], share_url: str, metadata: dict, have_vio_file: int, receive_payload: dict):
        logger.warning(f"âš ï¸ 115 æç¤ºæ–‡ä»¶è¯¥åˆ†äº«å·²æ¥æ”¶è¿‡: {share_url}")
        try:
            found_files = await self._find_files_in_dir(to_cid, names)
            found_count = len(found_files)
            if found_count > 0:
                logger.info(f"âœ… åœ¨ä¿å­˜ç›®å½•ä¸­æ‰¾åˆ° {found_count} ä¸ªåŒåæ–‡ä»¶ï¼Œç»§ç»­å¤„ç†")
                return {
                    "status": "success", 
                    "to_cid": to_cid, 
                    "names": names,
                    "share_url": share_url,
                    "recursive_links": [],
                    "metadata": metadata or {},
                    "have_vio": have_vio_file == 1
                }
            else:
                logger.warning("âš ï¸ 115 æç¤ºå·²æ¥æ”¶ï¼Œä½†åœ¨ä¿å­˜ç›®å½•æœªæ‰¾åˆ°æ–‡ä»¶ã€‚å°è¯•åˆ›å»ºæ–°ç›®å½•é‡è¯•è½¬å­˜...")
                new_folder_name = f"Retry_{int(time.time())}"
                resp = await self._api_call_with_timeout(
                    self.client.fs_makedirs_app, new_folder_name, pid=to_cid, async_=True,
                    **self._get_ios_ua_kwargs()
                )
                check_response(resp)
                new_cid = int(resp.get("cid") or resp.get("id") or (resp.get("data") or {}).get("cid") or 0)
                
                if not new_cid:
                    raise RuntimeError("åˆ›å»ºé‡è¯•ç›®å½•å¤±è´¥ï¼Œæœªè·å–åˆ°æœ‰æ•ˆCID")
                    
                logger.info(f"ğŸ“ å·²åˆ›å»ºé‡è¯•ç›®å½•: {new_folder_name} (CID: {new_cid})")
                
                retry_payload = receive_payload.copy()
                retry_payload["cid"] = new_cid
                
                recv_resp = await self._api_call_with_timeout(
                    self.client.share_receive_app, retry_payload, async_=True,
                    timeout=API_TIMEOUT, label="share_receive_retry",
                    **self._get_ios_ua_kwargs()
                )
                check_response(recv_resp)
                logger.info(f"âœ… åœ¨æ–°ç›®å½•è½¬å­˜æˆåŠŸ: {share_url} -> CID {new_cid}")
                
                return {
                    "status": "success", 
                    "to_cid": new_cid, 
                    "names": names,
                    "share_url": share_url,
                    "recursive_links": [],
                    "metadata": metadata or {},
                    "have_vio": have_vio_file == 1
                }
                
        except Exception as check_e:
            logger.error(f"âŒ å¤„ç†å·²æ¥æ”¶é€»è¾‘(éªŒè¯æˆ–é‡è¯•è½¬å­˜)æ—¶å‡ºé”™: {check_e}")
            
            errno_val = getattr(check_e, "errno", None)
            if hasattr(check_e, 'args') and len(check_e.args) >= 2 and isinstance(check_e.args[1], dict):
                if not errno_val:
                    errno_val = check_e.args[1].get("errno")
                    
            if errno_val == 4200045 or "4200045" in str(check_e) or "å·²ç»æ¥æ”¶" in str(check_e) or "å·²æ¥æ”¶" in str(check_e):
                return {
                    "status": "error",
                    "error_type": "already_exists_missing",
                    "message": "è¯¥åˆ†äº«é“¾æ¥æ‚¨å·²è½¬å­˜è¿‡ã€‚115 é™åˆ¶åŒä¸€é“¾æ¥ç”±äºæ–‡ä»¶ä¸¢å¤±è€Œæ— æ³•é‡å¤è½¬å­˜ï¼Œé‡è¯•è½¬å­˜ä¹Ÿå¤±è´¥ï¼Œè¯·å°è¯•å¯»æ‰¾åŸæ–‡ä»¶æˆ–ä»å›æ”¶ç«™è¿˜åŸã€‚"
                }
            return {
                "status": "error", 
                "error_type": "unknown",
                "message": f"ä¿å­˜å¤±è´¥ï¼Œä¸”é‡è¯•è½¬å­˜æŠ¥é”™: {str(check_e)}"
            }

    async def save_share_link(self, share_url: str, metadata: dict = None, target_dir: Optional[str] = None):
        return await self._enqueue_op("save_share", self._save_share_link_internal, share_url, metadata, target_dir)

    async def save_and_share(self, share_url: str, metadata: dict = None, target_dir: Optional[str] = None):
        async def _internal_flow():
            save_res = await self._save_share_link_internal(share_url, metadata, target_dir)
            if save_res and save_res.get("status") == "success":
                save_res = await self._organize_files(save_res, metadata)
                share_res = await self.create_share_link(save_res)
                if isinstance(share_res, str):
                    return {"status": "success", "share_link": share_res}
                elif isinstance(share_res, dict) and share_res.get("status") == "error":
                    return {
                        "status": "error",
                        "error_type": share_res.get("error_type", "share_failed"),
                        "message": share_res.get("message", "ç”Ÿæˆåˆ†äº«é“¾æ¥å¤±è´¥")
                    }
                return {
                    "status": "error",
                    "error_type": "share_failed",
                    "message": "è½¬å­˜æˆåŠŸä½†ç”Ÿæˆåˆ†äº«é“¾æ¥å¤±è´¥"
                }
            return save_res

        return await self._enqueue_op(f"save_and_share({share_url})", _internal_flow)

    async def _save_share_link_internal(self, share_url: str, metadata: dict = None, target_dir: Optional[str] = None):
        if not self.client:
            logger.warning("P115Client not initialized, cannot save link")
            return None
        
        logger.info(f"ğŸ“¥ å¼€å§‹å¤„ç†åˆ†äº«é“¾æ¥: {share_url}")
        try:
            payload = share_extract_payload(share_url)
            
            snap_resp = await self._api_call_with_timeout(
                self.client.share_snap_app, payload, async_=True,
                timeout=API_TIMEOUT, label="share_snap",
                **self._get_ios_ua_kwargs()
            )
            check_response(snap_resp)
            logger.debug(f"ğŸ“‹ share_snap å“åº”æ•°æ®: {snap_resp.get('data')}")

            data = snap_resp.get("data", {})
            if not data:
                logger.error("âŒ share_snap å“åº”ä¸­ç¼ºå°‘ data å­—æ®µ")
                return {
                    "status": "error",
                    "error_type": "api_error",
                    "message": "è·å–åˆ†äº«ä¿¡æ¯å¤±è´¥ï¼šAPI å“åº”æ•°æ®ä¸ºç©º"
                }

            share_info = data.get("shareinfo" if "shareinfo" in data else "share_info", {})
            share_state = data.get("share_state", share_info.get("share_state", share_info.get("status")))
            if share_state is not None:
                try:
                    share_state = int(share_state)
                except (ValueError, TypeError):
                    pass
            share_title = share_info.get("share_title", "")
            have_vio_file = share_info.get("have_vio_file", 0)
            
            logger.info(f"ğŸ“Š åˆ†äº«çŠ¶æ€: {share_state}, æ ‡é¢˜: {share_title}, è¿è§„æ ‡å¿—: {have_vio_file}")

            if have_vio_file == 1:
                logger.warning(f"âš ï¸ åˆ†äº«é“¾æ¥åŒ…å«è¿è§„å†…å®¹æ ‡å¿— (have_vio_file=1): {share_url}")

            is_snapshotting = "æ­£åœ¨ç”Ÿæˆæ–‡ä»¶å¿«ç…§" in str(snap_resp)
            if share_state == 0 or is_snapshotting:
                reason = "snapshotting" if is_snapshotting else "auditing"
                logger.info(f"ğŸ” åˆ†äº«é“¾æ¥å¤„äº{ 'å®¡æ ¸ä¸­' if reason == 'auditing' else 'å¿«ç…§ç”Ÿæˆä¸­' }ï¼Œè¿›å…¥è½®è¯¢ç­‰å¾…é˜Ÿåˆ—: {share_url}")
                async with async_session() as session:
                    new_task = PendingLink(
                        share_url=share_url,
                        metadata_json=metadata or {},
                        status=reason
                    )
                    session.add(new_task)
                    await session.commit()
                    db_id = new_task.id
                
                return {
                    "status": "pending",
                    "reason": reason,
                    "share_url": share_url,
                    "metadata": metadata or {},
                    "db_id": db_id
                }
            
            if share_state == 7:
                logger.warning(f"âš ï¸ åˆ†äº«é“¾æ¥å·²è¿‡æœŸ: {share_url}")
                return {
                    "status": "error",
                    "error_type": "expired",
                    "message": "é“¾æ¥å·²è¿‡æœŸ"
                }
            
            if share_state != 1:
                logger.warning(f"âš ï¸ åˆ†äº«é“¾æ¥çŠ¶æ€å¼‚å¸¸ (state={share_state}): {share_url}")
            
            items = data.get("list", [])
            if not items:
                logger.warning(f"âš ï¸ åˆ†äº«é“¾æ¥å†…æ²¡æœ‰æ–‡ä»¶ã€‚have_vio_file={have_vio_file}, çŠ¶æ€: {snap_resp.get('state')}")
                if have_vio_file == 1:
                    return {
                        "status": "error",
                        "error_type": "violated",
                        "message": "é“¾æ¥åŒ…å«è¿è§„å†…å®¹ï¼Œæ— æ³•è½¬å­˜åˆ†äº«"
                    }
                return {
                    "status": "error",
                    "error_type": "empty_share",
                    "message": "åˆ†äº«é“¾æ¥å†…æ²¡æœ‰å¯ä¾›è½¬å­˜çš„æ–‡ä»¶"
                }
            
            fids = []
            names = []
            for item in items:
                fid = item.get("fid") or item.get("cid")
                if fid:
                    fids.append(str(fid))
                    raw_name = item.get("n") or item.get("fn") or item.get("name") or item.get("file_name") or item.get("title")
                    if not raw_name:
                        logger.warning(f"âš ï¸ æ— æ³•ä»åˆ†äº«é¡¹æå–æ–‡ä»¶åï¼Œå¯ç”¨çš„é”®æœ‰: {list(item.keys())}")
                        raw_name = "æœªçŸ¥"
                    cleaned_name = raw_name.replace("\\'", "'").replace('\\"', '"')
                    names.append(cleaned_name)
                else:
                    logger.warning(f"Item missing both fid and cid: {item}")
            
            if not fids:
                logger.error(f"âŒ æœªèƒ½ä»åˆ—è¡¨é¡¹æå–åˆ°ä»»ä½•æœ‰æ•ˆçš„æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹ IDã€‚é¡¹ç›®æ•°: {len(items)}")
                return {
                    "status": "error",
                    "error_type": "parse_error",
                    "message": "è§£æåˆ†äº«æ–‡ä»¶åˆ—è¡¨å¤±è´¥ï¼Œæ— æ³•æå–æ–‡ä»¶ ID"
                }
            
            logger.info(f"ğŸ“¦ æ£€æµ‹åˆ° {len(fids)} ä¸ªé¡¹ç›®: {', '.join(names[:3])}{'...' if len(names) > 3 else ''}")
            
            to_cid = None
            max_network_wait = 1800
            network_start = time.time()
            network_attempt = 0
            
            while True:
                try:
                    to_cid = await self._ensure_save_dir(target_dir)
                    if network_attempt > 0:
                        logger.info(f"ğŸ‰ ç½‘ç»œå·²æ¢å¤ï¼Œç»§ç»­å¤„ç†ä»»åŠ¡ (ç­‰å¾…äº† {time.time() - network_start:.0f}s)")
                    break
                except Exception as dir_err:
                    network_attempt += 1
                    elapsed = time.time() - network_start
                    remaining = max_network_wait - elapsed
                    
                    if remaining <= 0:
                        logger.error(f"âŒ ç½‘ç›˜ç½‘ç»œæ¢å¤ç­‰å¾…è¶…æ—¶ (30åˆ†é’Ÿ)ï¼Œä¸­æ­¢ä»»åŠ¡: {dir_err}")
                        return {
                            "status": "error",
                            "error_type": "dir_failed",
                            "message": f"ç½‘ç›˜ç½‘ç»œæŒç»­ä¸å¯ç”¨ (å·²ç­‰å¾…30åˆ†é’Ÿ): {dir_err}"
                        }
                    
                    wait_time = min(30, remaining)
                    logger.warning(
                        f"â¸ï¸ ç½‘ç›˜ç½‘ç»œå¼‚å¸¸ï¼Œä»»åŠ¡æš‚åœç­‰å¾…æ¢å¤ "
                        f"(ç¬¬{network_attempt}æ¬¡é‡è¯•, å·²ç­‰å¾… {elapsed:.0f}s, å‰©ä½™ {remaining:.0f}s): {dir_err}"
                    )
                    await asyncio.sleep(wait_time)
            
            try:
                total_size = int(share_info.get("file_size") or 0)
            except (ValueError, TypeError):
                total_size = 0
            await self.check_and_prepare_capacity(file_count=len(fids), total_size=total_size)
            to_cid = await self._ensure_save_dir(target_dir)

            receive_payload = {
                "share_code": payload["share_code"],
                "receive_code": payload["receive_code"] or "",
                "file_id": ",".join(fids),
                "cid": to_cid
            }
            
            try:
                recv_resp = await self._api_call_with_timeout(
                    self.client.share_receive_app, receive_payload, async_=True,
                    timeout=API_TIMEOUT, label="share_receive",
                    **self._get_ios_ua_kwargs()
                )
                check_response(recv_resp)
                logger.info(f"âœ… é“¾æ¥è½¬å­˜æŒ‡ä»¤å·²å‘é€: {share_url} -> CID {to_cid}")
                recursive_links = []
            except Exception as recv_error:
                error_info = getattr(recv_error, "args", [None, {}])[1] if hasattr(recv_error, "args") and len(recv_error.args) >= 2 else {}
                errno_val = error_info.get("errno") if isinstance(error_info, dict) else None
                
                if errno_val == 4200044 or "è¶…è¿‡å½“å‰ç­‰çº§é™åˆ¶" in str(recv_error):
                    logger.warning(f"âš ï¸ è§¦å‘ 115 éä¼šå‘˜ 500 æ–‡ä»¶ä¿å­˜é™åˆ¶ï¼Œå°è¯•é€’å½’åˆ†æ‰¹ä¿å­˜: {share_url}")
                    recursive_links = await self._save_share_recursive(share_url, to_cid)
                    logger.info(f"âœ… é€’å½’åˆ†æ‰¹ä¿å­˜æŒ‡ä»¤å·²å¤„ç†å®Œæ¯•: {share_url}")
                elif errno_val == 4200045 or "4200045" in str(recv_error) or "å·²ç»æ¥æ”¶" in str(recv_error) or "å·²æ¥æ”¶" in str(recv_error):
                    return await self._handle_already_received(to_cid, names, share_url, metadata, have_vio_file, receive_payload)
                else:
                    raise
            
            return {
                "status": "success", 
                "to_cid": to_cid, 
                "names": names,
                "share_url": share_url,
                "recursive_links": recursive_links if 'recursive_links' in locals() else [],
                "metadata": metadata or {},
                "have_vio": have_vio_file == 1
            }
        except Exception as e:
            try:
                errno_val = getattr(e, "errno", None)
                if hasattr(e, 'args') and len(e.args) >= 2 and isinstance(e.args[1], dict):
                    error_msg = str(e.args[1].get('error', e))
                    if not errno_val:
                        errno_val = e.args[1].get('errno')
                else:
                    error_msg = str(e)
            except:
                error_msg = "æœªçŸ¥å¼‚å¸¸"
                errno_val = None
            
            if "æ­£åœ¨ç”Ÿæˆæ–‡ä»¶å¿«ç…§" in error_msg:
                logger.info(f"ğŸ” åˆ†äº«é“¾æ¥æ­£åœ¨ç”Ÿæˆå¿«ç…§ï¼Œè¿›å…¥è½®è¯¢ç­‰å¾…é˜Ÿåˆ—: {share_url}")
                async with async_session() as session:
                    new_task = PendingLink(
                        share_url=share_url,
                        metadata_json=metadata or {},
                        status="snapshotting"
                    )
                    session.add(new_task)
                    await session.commit()
                    db_id = new_task.id
                
                return {
                    "status": "pending",
                    "reason": "snapshotting",
                    "share_url": share_url,
                    "metadata": metadata or {},
                    "db_id": db_id
                }
            
            if "é™åˆ¶æ¥æ”¶" in error_msg:
                logger.warning(f"ğŸš« è§¦å‘ 115 æ¥æ”¶é™åˆ¶: {share_url}")
                self.set_restriction(hours=1.0)
                
                async with async_session() as session:
                    new_task = PendingLink(
                        share_url=share_url,
                        metadata_json=metadata or {},
                        status="restricted"
                    )
                    session.add(new_task)
                    await session.commit()
                    db_id = new_task.id
                
                return {
                    "status": "pending",
                    "reason": "restricted",
                    "share_url": share_url,
                    "metadata": metadata or {},
                    "db_id": db_id
                }

            if errno_val == 4200045 or "4200045" in error_msg or "å·²ç»æ¥æ”¶" in error_msg or "å·²æ¥æ”¶" in error_msg:
                retry_payload = {
                    "share_code": payload["share_code"],
                    "receive_code": payload["receive_code"] or "",
                    "file_id": ",".join(fids) if 'fids' in locals() else "",
                    "cid": to_cid
                }
                return await self._handle_already_received(to_cid, names, share_url, metadata, have_vio_file, retry_payload)

            logger.error("âŒ ä¿å­˜åˆ†äº«é“¾æ¥å‘ç”Ÿç¨‹åºå¼‚å¸¸: {}", error_msg)
            return {
                "status": "error",
                "error_type": "exception",
                "message": f"ç¨‹åºå¼‚å¸¸: {error_msg}"
            }

    async def _save_share_recursive(self, share_url: str, target_pid: int) -> list[str]:
        payload = share_extract_payload(share_url)
        share_code = payload["share_code"]
        receive_code = payload["receive_code"] or ""
        
        cid_map = {0: target_pid}
        share_links = []
        files_saved_total = 0
        
        share_structure = {0: (None, "")}
        
        async def reconstruct_path(current_share_cid, current_cid_map):
            new_root_cid = await self._ensure_save_dir()
            current_cid_map.clear()
            current_cid_map[0] = new_root_cid
            
            path_names = []
            temp_cid = current_share_cid
            while temp_cid != 0:
                parent, name = share_structure[temp_cid]
                path_names.append(name)
                temp_cid = parent
            path_names.reverse()
            
            current_share = 0
            current_real = new_root_cid
            for name in path_names:
                child_share = next(s_cid for s_cid, info in share_structure.items() if info[0] == current_share and info[1] == name)
                resp = await self._api_call_with_timeout(
                    self.client.fs_makedirs_app, name, pid=current_real, async_=True,
                    **self._get_ios_ua_kwargs()
                )
                check_response(resp)
                current_real = int(resp.get("cid") or resp.get("id") or (resp.get("data") or {}).get("cid") or 0)
                current_cid_map[child_share] = current_real
                current_share = child_share
            
            return current_real

        async for pid, dirs, files in share_iterdir_walk(
            self.client, share_code, receive_code, async_=True
        ):
            if pid not in cid_map:
                logger.info(f"ğŸ”„ æ­£åœ¨é€’å½’æ·±åº¦ä¸­é‡å»ºç›®å½•ç»“æ„ (Share CID: {pid})...")
                cid_map[pid] = await reconstruct_path(pid, cid_map)
                
            current_target_pid = cid_map[pid]
            
            for d in dirs:
                share_cid = d["id"]
                name = d["name"]
                share_structure[share_cid] = (pid, name)
                try:
                    resp = await self._api_call_with_timeout(
                        self.client.fs_makedirs_app, name, pid=current_target_pid, async_=True,
                        label=f"fs_makedirs({name})",
                        **self._get_ios_ua_kwargs()
                    )
                    check_response(resp)
                    new_cid = int(resp.get("cid") or resp.get("id") or (resp.get("data") or {}).get("cid") or 0)
                    if new_cid:
                        cid_map[share_cid] = new_cid
                except Exception as e:
                    if "å·²ç»å­˜åœ¨" in str(e) or "40004" in str(e):
                        found = await self._find_files_in_dir(current_target_pid, [name])
                        if found:
                            cid_map[share_cid] = int(found[0]["fid"])
                    else:
                        logger.error(f"âŒ é€’å½’ä¿å­˜è¿‡ç¨‹ä¸­åˆ›å»ºå­ç›®å½• {name} å¤±è´¥: {e}")
            
            fids = [str(f["id"]) for f in files]
            if not fids:
                continue
                
            for i in range(0, len(fids), 500):
                need_cleanup = files_saved_total >= 10000
                if not need_cleanup and settings.P115_CLEANUP_CAPACITY_ENABLED:
                    used, total = await self.get_storage_stats()
                    if total > 0 and (used / total) > 0.9:
                        need_cleanup = True
                        logger.warning(f"âš ï¸ å®¹é‡é€¼è¿‘ä¸Šé™ ({used/total:.1%})ï¼Œè§¦å‘ä¸­è½¬æ¸…ç†")

                if need_cleanup:
                    logger.info("ğŸ“¦ è§¦å‘ä¸­è½¬æµç¨‹ï¼šæ­£åœ¨ç”Ÿæˆå½“å‰å·²ä¿å­˜å†…å®¹çš„åˆ†äº«é“¾æ¥...")
                    save_dir_cid = await self._ensure_save_dir()
                    ls_resp = await self._api_call_with_timeout(
                        self.client.fs_files_app2, save_dir_cid, async_=True,
                        **self._get_ios_ua_kwargs()
                    )
                    ls_items = ls_resp.get("data", [])
                    ls_names = [it["n"] for it in ls_items]
                    
                    if ls_names:
                        intermediate_link = await self.create_share_link({"to_cid": save_dir_cid, "names": ls_names})
                        if intermediate_link:
                            logger.info(f"ğŸ“¤ ä¸­è½¬é“¾æ¥å·²ç”Ÿæˆ: {intermediate_link}")
                            share_links.append(intermediate_link)

                    await self._do_cleanup_logic()
                    logger.info("ğŸ§¹ ä¸­è½¬æ¸…ç†å®Œæˆï¼Œç­‰å¾… 5 ç§’æ¢å¤...")
                    await asyncio.sleep(5)
                    
                    files_saved_total = 0
                    current_target_pid = await reconstruct_path(pid, cid_map)
                
                batch = fids[i:i+500]
                try:
                    receive_payload = {
                        "share_code": share_code,
                        "receive_code": receive_code,
                        "file_id": ",".join(batch),
                        "cid": current_target_pid
                    }
                    recv_resp = await self._api_call_with_timeout(
                        self.client.share_receive_app, receive_payload, async_=True,
                        timeout=API_TIMEOUT, label=f"share_receive_batch({i//500})",
                        **self._get_ios_ua_kwargs()
                    )
                    check_response(recv_resp)
                    files_saved_total += len(batch)
                    logger.info(f"âœ… é€’å½’åˆ†æ‰¹è½¬å­˜æˆåŠŸ: {len(batch)} ä¸ªæ–‡ä»¶ -> CID {current_target_pid} (æœ¬è½®ç´¯è®¡: {files_saved_total})")
                    
                    await asyncio.sleep(random.randint(2, 3))
                except Exception as e:
                    errno_val = getattr(e, "errno", None)
                    if hasattr(e, 'args') and len(e.args) >= 2 and isinstance(e.args[1], dict):
                        if not errno_val:
                            errno_val = e.args[1].get("errno")
                            
                    if errno_val == 4200045 or "4200045" in str(e) or "å·²ç»æ¥æ”¶" in str(e) or "å·²æ¥æ”¶" in str(e):
                        continue
                    logger.error(f"âŒ é€’å½’è½¬å­˜æ–‡ä»¶åŒ…å¤±è´¥: {e}")
        
        return share_links

    async def get_share_status(self, share_url: str):
        try:
            payload = share_extract_payload(share_url)
            snap_resp = await self._api_call_with_timeout(
                self.client.share_snap_app, payload, async_=True,
                timeout=API_TIMEOUT, label="share_snap(status)",
                **self._get_ios_ua_kwargs()
            )
            check_response(snap_resp)
            
            data = snap_resp.get("data", {})
            share_info = data.get("shareinfo" if "shareinfo" in data else "share_info", {})
            share_state = data.get("share_state", share_info.get("share_state", share_info.get("status")))
            if share_state is not None:
                try:
                    share_state = int(share_state)
                except (ValueError, TypeError):
                    pass
            share_title = share_info.get("share_title", "")
            have_vio_file = share_info.get("have_vio_file", 0)
            
            is_snapshotting = "æ­£åœ¨ç”Ÿæˆæ–‡ä»¶å¿«ç…§" in str(snap_resp)
            res = {
                "share_state": share_state,
                "is_auditing": share_state == 0,
                "is_snapshotting": is_snapshotting,
                "is_pending": share_state == 0 or is_snapshotting,
                "is_expired": share_state == 7,
                "is_prohibited": have_vio_file == 1,
                "title": share_title
            }
            if is_snapshotting:
                logger.info(f"ğŸ“Š æ£€æŸ¥é“¾æ¥å‘ç°æ­£åœ¨ç”Ÿæˆå¿«ç…§: {share_url}")
            logger.debug(f"ğŸ“Š æ£€æŸ¥é“¾æ¥çŠ¶æ€: {share_url} -> {res}")
            return res
        except Exception as e:
            error_msg = str(e)
            if any(code in error_msg for code in ["4100009", "4100010"]) or \
               any(msg in error_msg for msg in ["é“¾æ¥å·²å¤±æ•ˆ", "åˆ†äº«å·²å–æ¶ˆ"]):
                logger.warning(f"â° æ£€æŸ¥é“¾æ¥çŠ¶æ€å‘ç°é“¾æ¥å·²å¤±æ•ˆæˆ–è¢«å–æ¶ˆ: {share_url}")
                return {
                    "share_state": 7,
                    "is_auditing": False,
                    "is_expired": True,
                    "is_prohibited": False,
                    "title": ""
                }
            if "æ­£åœ¨ç”Ÿæˆæ–‡ä»¶å¿«ç…§" in error_msg:
                logger.info(f"ğŸ“Š æ£€æŸ¥é“¾æ¥çŠ¶æ€å‘ç°æ­£åœ¨ç”Ÿæˆå¿«ç…§: {share_url}")
                return {
                    "share_state": 0,
                    "is_auditing": False,
                    "is_snapshotting": True,
                    "is_pending": True,
                    "is_expired": False,
                    "is_prohibited": False,
                    "title": ""
                }
            logger.error(f"âŒ æ£€æŸ¥é“¾æ¥çŠ¶æ€å¤±è´¥: {share_url}, é”™è¯¯: {e}")
            return None

    async def _find_files_in_dir(self, cid: int, target_names: list) -> list:
        matched = []
        
        for name in target_names:
            try:
                search_resp = await self._api_call_with_timeout(
                    self.client.fs_search_app2,
                    {"search_value": name, "cid": cid, "limit": 20},
                    async_=True,
                    timeout=30, max_retries=2, label=f"fs_search({name})",
                    **self._get_ios_ua_kwargs()
                )
                check_response(search_resp)
                search_data = search_resp.get("data", [])
                
                if isinstance(search_data, dict):
                    search_items = search_data.get("list", [])
                else:
                    search_items = search_data
                
                logger.debug(f"ğŸ” fs_search '{name}' åœ¨ CID:{cid} è¿”å› {len(search_items)} æ¡ç»“æœ")
                
                for item in search_items:
                    item_name = item.get("n") or item.get("fn") or item.get("name") or item.get("file_name") or item.get("title") or item.get("category_name")
                    if item_name == name:
                        item_id = item.get("fid") or item.get("cid") or item.get("file_id") or item.get("category_id")
                        if item_id:
                            matched.append({
                                "fid": str(item_id),
                                "name": item_name,
                                "size": item.get("s", item.get("file_size", 0)),
                                "time": item.get("te", 0),
                            })
                            logger.info(f"ğŸ“„ fs_search æ‰¾åˆ°: {item_name} (ID: {item_id})")
                            break
            except Exception as e:
                logger.warning(f"âš ï¸ fs_search æœç´¢ '{name}' å¤±è´¥: {e}")
        
        if len(matched) == len(target_names):
            return matched
        
        found_names = {m["name"] for m in matched}
        remaining_names = [n for n in target_names if n not in found_names]
        logger.info(f"ğŸ” fs_search æ‰¾åˆ° {len(matched)}/{len(target_names)} ä¸ªæ–‡ä»¶ï¼Œå°è¯• fs_files æŸ¥æ‰¾å‰©ä½™: {remaining_names}")
        
        try:
            resp = await self._api_call_with_timeout(
                self.client.fs_files_app2,
                {"cid": cid, "limit": 500, "show_dir": 1},
                async_=True,
                timeout=30, max_retries=2, label="fs_files",
                **self._get_ios_ua_kwargs()
            )
            check_response(resp)
            file_list = resp.get("data", [])
            
            if isinstance(file_list, dict):
                file_list = file_list.get("list", [])
            
            resp_path = resp.get("path", [])
            resp_cid = None
            if resp_path:
                last_path = resp_path[-1] if isinstance(resp_path, list) else resp_path
                resp_cid = last_path.get("cid") if isinstance(last_path, dict) else None
            
            actual_count = resp.get("count", "?")
            logger.debug(f"ğŸ“‚ fs_files CID:{cid} è¿”å› {len(file_list)} é¡¹ (æ€»æ•°: {actual_count}, è·¯å¾„CID: {resp_cid})")
            
            if resp_cid is not None and str(resp_cid) != str(cid):
                logger.warning(f"âš ï¸ fs_files è¿”å›çš„ç›®å½• CID({resp_cid}) ä¸è¯·æ±‚çš„ CID({cid}) ä¸åŒ¹é…ï¼å¯èƒ½ç›®å½•ä¸å­˜åœ¨")
            
            if file_list:
                dir_file_names = [(item.get("n") or item.get("fn") or item.get("name") or item.get("file_name") or item.get("title") or item.get("category_name") or f"? (keys: {list(item.keys())})") for item in file_list[:10]]
                logger.debug(f"ğŸ“‹ ç›®å½•å†…æ–‡ä»¶(å‰10): {dir_file_names}")
            
            for item in file_list:
                item_name = item.get("n") or item.get("fn") or item.get("name") or item.get("file_name") or item.get("title") or item.get("category_name")
                if item_name in remaining_names:
                    item_id = item.get("fid") or item.get("cid") or item.get("file_id") or item.get("category_id") or item.get("id")
                    if item_id:
                        matched.append({
                            "fid": str(item_id),
                            "name": item_name,
                            "size": item.get("s", 0),
                            "time": item.get("te", 0),
                        })
                        logger.info(f"ğŸ“„ fs_files æ‰¾åˆ°: {item_name} (ID: {item_id})")
                        
        except Exception as e:
            logger.warning(f"âš ï¸ fs_files åˆ—ç›®å½•å¤±è´¥: {e}")
        
        return matched

    async def create_share_link(self, save_result: dict):
        if not self.client or not save_result:
            return None
            
        to_cid = save_result.get("to_cid")
        names = save_result.get("names", [])
        
        try:
            logger.info(f"â³ ç­‰å¾… 2 ç§’ä»¥ç¡®ä¿æ–‡ä»¶ä¿å­˜å¼€å§‹...")
            await asyncio.sleep(2)
            
            new_fids = []
            matched_files = []
            
            max_poll_attempts = 10
            for poll_attempt in range(1, max_poll_attempts + 1):
                try:
                    logger.info(f"ğŸ” æ­£åœ¨æŸ¥æ‰¾æ–‡ä»¶ (ç¬¬ {poll_attempt}/{max_poll_attempts} æ¬¡), ç›®æ ‡ç›®å½• CID: {to_cid}")
                    current_matched = await self._find_files_in_dir(to_cid, names)
                    
                    if current_matched:
                        if len(current_matched) == len(names):
                            logger.info(f"âœ… æ–‡ä»¶å·²å…¨éƒ¨åˆ°è¾¾ï¼Œå…± {len(current_matched)} ä¸ªï¼Œç«‹å³ç»§ç»­")
                            new_fids = [f["fid"] for f in current_matched]
                            break
                        
                        if matched_files:
                            stable = len(current_matched) == len(matched_files)
                            if stable:
                                for curr, prev in zip(sorted(current_matched, key=lambda x: x["fid"]), 
                                                     sorted(matched_files, key=lambda x: x["fid"])):
                                    if curr["fid"] != prev["fid"] or curr["size"] != prev["size"]:
                                        stable = False
                                        break
                            
                            if stable:
                                logger.info(f"âœ… æ–‡ä»¶çŠ¶æ€å·²ç¨³å®šï¼Œæ£€æµ‹åˆ° {len(current_matched)} ä¸ªæ–‡ä»¶")
                                new_fids = [f["fid"] for f in current_matched]
                                break
                            else:
                                logger.debug(f"ğŸ”„ æ–‡ä»¶çŠ¶æ€å˜åŒ–ä¸­ (ç¬¬ {poll_attempt}/{max_poll_attempts} æ¬¡è½®è¯¢)")
                        
                        matched_files = current_matched
                        
                        if poll_attempt < max_poll_attempts:
                            await asyncio.sleep(2)
                    else:
                        logger.warning(f"âš ï¸ è½®è¯¢æœªæ‰¾åˆ°æ–‡ä»¶ (ç¬¬ {poll_attempt}/{max_poll_attempts} æ¬¡)")
                        if poll_attempt < max_poll_attempts:
                            await asyncio.sleep(2)
                            
                except Exception as e:
                    logger.warning(f"âš ï¸ æŸ¥æ‰¾æ–‡ä»¶å¤±è´¥ (è½®è¯¢ {poll_attempt}/{max_poll_attempts}): {e}")
                    if poll_attempt < max_poll_attempts:
                        await asyncio.sleep(5)
            
            if not new_fids and matched_files:
                logger.info(f"âš ï¸ æ–‡ä»¶æœªå®Œå…¨ç¨³å®šï¼Œä½†ä½¿ç”¨ {len(matched_files)} ä¸ªå·²åŒ¹é…çš„æ–‡ä»¶å°è¯•åˆ›å»ºåˆ†äº«")
                new_fids = [f["fid"] for f in matched_files]
            
            if not new_fids:
                logger.warning(f"âš ï¸ åœ¨ä¿å­˜ç›®å½• {to_cid} ä¸­æœªæ‰¾åˆ°å¯¹åº”çš„æ–‡ä»¶ {names}ï¼Œå¯èƒ½ 115 å¤„ç†å»¶è¿Ÿæˆ–ä¿å­˜å¤±è´¥")
                return None
            
            share_links = []
            fids_str_list = [str(fid) for fid in new_fids]
            max_share_retries = 3
            
            for batch_idx, i in enumerate(range(0, len(fids_str_list), 10000), 1):
                batch_fids = fids_str_list[i:i+10000]
                batch_share_code = None
                batch_receive_code = None
                
                for retry_attempt in range(1, max_share_retries + 1):
                    try:
                        logger.info(f"ğŸ“¤ æ­£åœ¨åˆ›å»ºåˆ†äº«é“¾æ¥ (åˆ†å· {batch_idx}, å°è¯• {retry_attempt}/{max_share_retries})...")
                        send_resp = await self._api_call_with_timeout(
                            self.client.share_send_app, ",".join(batch_fids), async_=True,
                            timeout=API_TIMEOUT, max_retries=1, label=f"share_send_batch_{batch_idx}",
                            **self._get_ios_ua_kwargs()
                        )
                        check_response(send_resp)
                        
                        data = send_resp["data"]
                        batch_share_code = data.get("share_code")
                        batch_receive_code = data.get("receive_code") or data.get("recv_code")
                        
                        logger.info(f"âœ… åˆ†äº«åˆ†å· {batch_idx} åˆ›å»ºæˆåŠŸ: {batch_share_code}")
                        break
                        
                    except Exception as share_error:
                        error_str = str(share_error)
                        if ("4100005" in error_str or "å·²è¢«ç§»åŠ¨æˆ–åˆ é™¤" in error_str) and retry_attempt < max_share_retries:
                            logger.warning(f"âš ï¸ æ–‡ä»¶å°šæœªå°±ç»ªï¼Œç­‰å¾… 5 ç§’åé‡è¯•...")
                            await asyncio.sleep(5)
                        else:
                            logger.error(f"âŒ åˆ›å»ºåˆ†äº«åˆ†å· {batch_idx} å¤±è´¥: {share_error}")
                            if batch_idx == 1: raise
                            break
                
                if batch_share_code:
                    try:
                        logger.info(f"ğŸ”„ æ­£åœ¨å°†åˆ†äº«é“¾æ¥ {batch_share_code} è½¬æ¢ä¸ºé•¿æœŸæœ‰æ•ˆ...")
                        await self._api_call_with_timeout(
                            self.client.share_update_app, {"share_code": batch_share_code, "share_duration": -1},
                            async_=True, timeout=API_TIMEOUT, max_retries=2, label=f"share_update_{batch_idx}",
                            **self._get_ios_ua_kwargs()
                        )
                    except Exception as e:
                        logger.warning(f"âš ï¸ è½¬æ¢é•¿æœŸåˆ†äº«å¤±è´¥ (åˆ†å· {batch_idx}): {e}")
                    
                    full_link = f"https://115.com/s/{batch_share_code}"
                    if batch_receive_code:
                        full_link += f"?password={batch_receive_code}"
                    share_links.append(full_link)
            
            if not share_links:
                logger.error("âŒ æœªèƒ½ç”Ÿæˆä»»ä½•åˆ†äº«é“¾æ¥")
                return None
            
            if len(share_links) > 1:
                formatted_links = []
                for idx, link in enumerate(share_links, 1):
                    formatted_links.append(f"é“¾æ¥ {idx}: {link}")
                result_share = "\n".join(formatted_links)
                logger.info(f"ğŸ”— å·²ç”Ÿæˆ {len(share_links)} ä¸ªåˆ†å·åˆ†äº«é“¾æ¥")
            else:
                result_share = share_links[0]
                logger.info(f"ğŸ”— é•¿æœŸåˆ†äº«é“¾æ¥å·²ç”Ÿæˆ: {result_share}")
                
            return result_share
            
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºæ–°åˆ†äº«é“¾æ¥å¤±è´¥: {e}")
            error_info = getattr(e, "args", [None, {}])[1] if hasattr(e, "args") and len(e.args) >= 2 else {}
            errno_val = error_info.get("errno") if isinstance(error_info, dict) else None
            
            if errno_val == 4100016 and save_result.get("have_vio"):
                return {
                    "status": "error",
                    "error_type": "violated",
                    "message": "é“¾æ¥åŒ…å«è¿è§„å†…å®¹ï¼Œæ— æ³•è½¬å­˜åˆ†äº«"
                }
            
            error_msg = str(e)
            if "é™åˆ¶åˆ†äº«" in error_msg:
                logger.warning(f"ğŸš« è§¦å‘ 115 åˆ†äº«é™åˆ¶")
                self.set_restriction(hours=1.0)
                return {
                    "status": "pending",
                    "reason": "restricted",
                    "share_url": save_result.get("share_url"),
                    "metadata": save_result.get("metadata", {})
                }

            return None

    async def cleanup_save_directory(self, wait: bool = True):
        try:
            async with self._acquire_task_lock("cleanup", wait=wait):
                return await self._cleanup_save_directory_internal()
        except BlockingIOError:
            return False

    async def _cleanup_save_directory_internal(self) -> bool:
        try:
            logger.info(f"ğŸ§¹ å¼€å§‹æ¸…ç†ä¿å­˜ç›®å½•: {settings.P115_SAVE_DIR}")
            cid = await self._ensure_save_dir()
            if not cid:
                return False
            
            resp = await self._api_call_with_timeout(
                self.client.fs_delete, cid, async_=True,
                timeout=API_TIMEOUT, label="fs_delete",
                **self._get_ios_ua_kwargs()
            )
            check_response(resp)
            
            self.clear_save_dir_cache()
            logger.info("âœ… ä¿å­˜ç›®å½•æ¸…ç†å®Œæˆ")
            return True
        except Exception as e:
            logger.error(f"âŒ å†…éƒ¨æ¸…ç†ä¿å­˜ç›®å½•å¤±è´¥: {e}")
            return False

    async def get_storage_stats(self) -> Tuple[int, int]:
        if not self.client:
            return 0, 0
        try:
            resp = await self._api_call_with_timeout(
                self.client.user_space_info, async_=True,
                timeout=API_TIMEOUT, label="user_space_info",
                **self._get_ios_ua_kwargs()
            )
            check_response(resp)
            data = resp.get("data", {})
            
            def extract_size(val) -> int:
                if isinstance(val, dict):
                    return int(val.get("size") or val.get("size_total") or val.get("size_use") or 0)
                try:
                    return int(val) if val is not None else 0
                except (ValueError, TypeError):
                    return 0

            used = extract_size(data.get("all_used") or data.get("all_use") or data.get("used") or 0)
            total = extract_size(data.get("all_total") or data.get("total") or 0)
            return used, total
        except Exception as e:
            logger.error("âŒ è·å–ç½‘ç›˜å®¹é‡å¤±è´¥: {}", str(e))
            return 0, 0

    async def check_and_prepare_capacity(self, file_count: int = 0, total_size: int = 0):
        if not settings.P115_CLEANUP_CAPACITY_ENABLED:
            return

        used_bytes, total_bytes = await self.get_storage_stats()
        if total_bytes == 0:
            return
            
        remaining_bytes = total_bytes - used_bytes

        if file_count > 500 and total_size > remaining_bytes:
            logger.info(f"ğŸš€ é¢„æµ‹æ€§æ¸…ç†ï¼šæ£€æµ‹åˆ°å¤§æ‰¹é‡æ–‡ä»¶ ({file_count} ä¸ª, {total_size/(1024**3):.2f}GB)ï¼Œå‰©ä½™ç©ºé—´ä¸è¶³ï¼Œæ‰§è¡Œæ¸…ç†...")
            await self._do_cleanup_logic()
            await asyncio.sleep(3)
            return

        if total_size > 0 and total_size > remaining_bytes:
             logger.warning(f"âš ï¸ å‰©ä½™ç©ºé—´ä¸è¶³ (éœ€ {total_size/(1024**3):.2f}GB, å‰© {remaining_bytes/(1024**3):.2f}GB)ï¼Œæ‰§è¡Œæ¸…ç†...")
             await self._do_cleanup_logic()
             await asyncio.sleep(3)

    async def check_capacity_and_cleanup(self, mode: str = "manual"):
        wait_for_lock = True
        if mode == "scheduled":
            wait_for_lock = False
            try:
                async with self._acquire_task_lock("capacity_check_probe", wait=False):
                    pass
            except BlockingIOError:
                logger.info("â­ï¸ å®šæ—¶å®¹é‡æ£€æŸ¥ï¼šæ£€æµ‹åˆ°è½¬å­˜ä»»åŠ¡è¿è¡Œä¸­ï¼ŒæŒ‰è®¡åˆ’è·³è¿‡é”å®šç›‘æµ‹")
                return False
        
        logger.debug(f"ğŸ” [å®¹é‡æ£€æŸ¥] æ¨¡å¼: {mode}, æ­£åœ¨è·å–å­˜å‚¨çŠ¶æ€...")
            
        use_fallback = (mode == "batch" and not settings.P115_CLEANUP_CAPACITY_ENABLED)
        limit = settings.P115_CLEANUP_CAPACITY_LIMIT
        unit = settings.P115_CLEANUP_CAPACITY_UNIT
        
        used_bytes, total_bytes = await self.get_storage_stats()
        if total_bytes <= 0:
            return False

        should_cleanup = False
        
        if use_fallback:
            if (total_bytes - used_bytes) < (total_bytes * 0.1):
                logger.warning(f"ğŸš¨ [æ‰¹é‡ä»»åŠ¡] å‰©ä½™ç©ºé—´ä¸è¶³ 10% ({(total_bytes-used_bytes)/(1024**4):.2f}TB)ï¼Œè§¦å‘ç¡¬æ€§æ¸…ç†")
                should_cleanup = True
        elif settings.P115_CLEANUP_CAPACITY_ENABLED and limit > 0:
            limit_bytes = limit * (1024**4) if unit == "TB" else limit * (1024**3)
            if used_bytes > limit_bytes:
                logger.info(f"ğŸ“Š [{mode}] ç½‘ç›˜å·²ç”¨ç©ºé—´ ({used_bytes/(1024**4):.2f}TB) è¶…è¿‡é˜ˆå€¼ ({limit} {unit})")
                should_cleanup = True
        
        if should_cleanup or mode == "manual":
            try:
                async with self._acquire_task_lock("cleanup", wait=wait_for_lock):
                    logger.info(f"ğŸ§¹ æ‰§è¡Œå®¹é‡ç®¡ç†æ¸…ç† (æ¨¡å¼: {mode})...")
                    await self._do_cleanup_logic()
                    return True
            except BlockingIOError:
                if mode == "scheduled":
                    logger.info("â­ï¸ å®šæ—¶å®¹é‡æ£€æŸ¥ï¼šè½¬å­˜é”è·å–å†²çªï¼ŒæŒ‰è®¡åˆ’è·³è¿‡ä»»åŠ¡")
                return False
        else:
            logger.debug(f"âœ… [å®¹é‡æ£€æŸ¥] æ¨¡å¼: {mode}, å½“å‰ç©ºé—´å……è¶³ ({used_bytes/(1024**4):.2f}TB)ï¼Œæ— éœ€æ¸…ç†")
        return False

    async def _do_cleanup_logic(self):
        await self._cleanup_save_directory_internal()
        await self._cleanup_recycle_bin_internal()

    async def get_history_link(self, original_url: str) -> Optional[Union[str, list[str]]]:
        try:
            import json
            from app.models.schema import LinkHistory
            async with async_session() as session:
                result = await session.execute(
                    select(LinkHistory).where(LinkHistory.original_url == original_url)
                )
                record = result.scalar_one_or_none()
                if record:
                    link_val = record.share_link
                    if link_val.startswith("[") and link_val.endswith("]"):
                        try:
                            return json.loads(link_val)
                        except:
                            return link_val
                    return link_val
            return None
        except Exception as e:
            logger.error(f"æŸ¥è¯¢å†å²è®°å½•å¤±è´¥: {e}")
            return None

    async def save_history_link(self, original_url: str, share_link: Union[str, list[str]]):
        try:
            import json
            from app.models.schema import LinkHistory
            
            if isinstance(share_link, list):
                if not share_link:
                    return
                link_to_store = json.dumps(share_link) if len(share_link) > 1 else share_link[0]
            else:
                link_to_store = share_link

            async with async_session() as session:
                existing = await session.execute(
                    select(LinkHistory).where(LinkHistory.original_url == original_url)
                )
                record = existing.scalar_one_or_none()
                if record:
                    record.share_link = link_to_store
                else:
                    new_record = LinkHistory(original_url=original_url, share_link=link_to_store)
                    session.add(new_record)
                await session.commit()
                logger.info(f"å·²ä¿å­˜å†å²è®°å½•: {original_url} -> {link_to_store[:50]}...")
        except Exception as e:
            logger.error(f"ä¿å­˜å†å²è®°å½•å¤±è´¥: {e}")

    async def delete_all_history_links(self):
        try:
            from app.models.schema import LinkHistory
            from sqlalchemy import delete
            async with async_session() as session:
                await session.execute(delete(LinkHistory))
                await session.commit()
                logger.info("å·²æ¸…ç©ºæ‰€æœ‰å†å²è®°å½•")
                return True
        except Exception as e:
            logger.error(f"æ¸…ç©ºå†å²è®°å½•å¤±è´¥: {e}")
            return False

    async def get_all_history_links(self, limit: int = 50) -> List[Dict]:
        try:
            from app.models.schema import LinkHistory
            from sqlalchemy import select, desc
            async with async_session() as session:
                result = await session.execute(
                    select(LinkHistory).order_by(desc(LinkHistory.created_at)).limit(limit)
                )
                records = result.scalars().all()
                return [
                    {
                        "id": r.id,
                        "original_url": r.original_url,
                        "share_link": r.share_link,
                        "created_at": r.created_at.isoformat() if r.created_at else None
                    }
                    for r in records
                ]
        except Exception as e:
            logger.error(f"è·å–å†å²è®°å½•å¤±è´¥: {e}")
            return []

    async def cleanup_recycle_bin(self, wait: bool = True):
        try:
            async with self._acquire_task_lock("cleanup", wait=wait):
                return await self._cleanup_recycle_bin_internal()
        except BlockingIOError:
            return False

    async def _cleanup_recycle_bin_internal(self) -> bool:
        try:
            logger.info("ğŸ—‘ï¸ å¼€å§‹æ¸…ç©ºå›æ”¶ç«™...")
            payload = {}
            if settings.P115_RECYCLE_PASSWORD:
                payload["password"] = settings.P115_RECYCLE_PASSWORD
                logger.debug("ä½¿ç”¨å›æ”¶ç«™å¯†ç ")
            
            resp = await self._api_call_with_timeout(
                self.client.recyclebin_clean_app, payload, async_=True,
                timeout=API_TIMEOUT, label="recyclebin_clean",
                **self._get_ios_ua_kwargs()
            )
            check_response(resp)
            logger.info("âœ… å›æ”¶ç«™å·²æ¸…ç©º")
            return True
        except Exception as e:
            logger.error("âŒ å†…éƒ¨æ¸…ç©ºå›æ”¶ç«™å¤±è´¥: {}", e)
            return False

    # ========== æ•´ç†æ–¹æ³•ï¼ˆä¿®å¤ç‰ˆï¼Œå¢å¼ºå¼‚å¸¸å¤„ç†ï¼‰ ==========
    async def _organize_files(self, save_result: dict, metadata: dict) -> dict:
        """æ•´ç†æ–‡ä»¶ï¼šè¯†åˆ«åª’ä½“ã€ç§»åŠ¨ç›®å½•ã€é‡å‘½å
           ä½¿ç”¨è½¬å­˜åçš„æ–‡ä»¶/æ–‡ä»¶å¤¹åä½œä¸ºæ ‡é¢˜æ¥æº
        """
        if not settings.TMDB_API_KEY:
            logger.debug("TMDB API Key æœªé…ç½®ï¼Œè·³è¿‡æ•´ç†")
            return save_result

        # ç›´æ¥ä» save_result è·å–è½¬å­˜åçš„æ–‡ä»¶/æ–‡ä»¶å¤¹å
        if not save_result.get('names'):
            logger.debug("save_result ä¸­æ— æ–‡ä»¶åä¿¡æ¯ï¼Œè·³è¿‡æ•´ç†")
            return save_result

        title_candidate = save_result['names'][0]
        logger.info(f"ä½¿ç”¨è½¬å­˜åçš„æ–‡ä»¶/æ–‡ä»¶å¤¹åä½œä¸ºæ ‡é¢˜: {title_candidate}")

        # ä½¿ç”¨æ™ºèƒ½åˆ†æå™¨è§£ææ–‡ä»¶åï¼Œè·å–å¹²å‡€æ ‡é¢˜å’Œå…¶ä»–ä¿¡æ¯
        analyzer = SmartMediaAnalyzer()
        parsed = analyzer.analyze(title_candidate)
        clean_title = parsed.title
        logger.info(f"è§£æåçš„å¹²å‡€æ ‡é¢˜: {clean_title}")

        organizer = MediaOrganizer(settings.TMDB_CONFIG)
        tmdb = TMDBClient()
        try:
            # 1. å°è¯•ä»æ–‡ä»¶åä¸­æå– TMDB ID
            tmdb_id = organizer.extract_tmdb_id(title_candidate)
            year = parsed.year or organizer.extract_year(title_candidate)  # ä¼˜å…ˆä½¿ç”¨è§£æåˆ°çš„å¹´ä»½
            
            media_info = None
            match_method = None

            # 2. ä¼˜å…ˆä½¿ç”¨ ID æŸ¥è¯¢
            if tmdb_id:
                for mtype in ['movie', 'tv']:
                    try:
                        media_info = await tmdb.get_details(mtype, tmdb_id)
                    except Exception as e:
                        logger.warning(f"è·å–åª’ä½“è¯¦æƒ…å¤±è´¥ (ID {tmdb_id}, type {mtype}): {e}")
                        continue
                    if media_info:
                        media_info['media_type'] = mtype
                        
                        # è·å–åª’ä½“çš„å¹´ä»½
                        media_year = None
                        if mtype == 'movie':
                            release = media_info.get('release_date')
                            if release:
                                media_year = int(release[:4])
                        else:
                            first_air = media_info.get('first_air_date')
                            if first_air:
                                media_year = int(first_air[:4])
                        
                        # éªŒè¯å¹´ä»½ï¼ˆå¦‚æœæä¾›äº†å¹´ä»½ï¼‰
                        if year and media_year and year != media_year:
                            logger.warning(f"ID {tmdb_id} å¹´ä»½ä¸åŒ¹é…: æœŸæœ› {year}, å®é™… {media_year}ï¼Œæ”¾å¼ƒæ­¤ç»“æœ")
                            media_info = None
                            continue
                        
                        logger.info(f"é€šè¿‡ ID {tmdb_id} æ‰¾åˆ°åª’ä½“: {media_info.get('title') or media_info.get('name')} (å¹´ä»½: {media_year})")
                        match_method = "id"
                        break
                
                if not media_info:
                    logger.info(f"é€šè¿‡ ID {tmdb_id} æœªæ‰¾åˆ°æœ‰æ•ˆåª’ä½“ï¼Œå›é€€åˆ°æ ‡é¢˜æœç´¢")

            # 3. å¦‚æœæœªé€šè¿‡ ID æ‰¾åˆ°ï¼Œä½¿ç”¨æ ‡é¢˜æœç´¢
            if not media_info:
                search_title = clean_title if clean_title else title_candidate
                logger.info(f"ä½¿ç”¨æ ‡é¢˜æœç´¢: {search_title}")
                try:
                    media_info = await tmdb.search_multi(search_title, year)
                except Exception as e:
                    logger.error(f"TMDB æœç´¢å¤±è´¥: {e}")
                    media_info = None
                
                if media_info:
                    # éªŒè¯æœç´¢ç»“æœæ˜¯å¦ä¸æå–çš„å¹´ä»½åŒ¹é…
                    result_id = media_info.get('id')
                    result_year = None
                    mtype = media_info.get('media_type')
                    
                    if mtype == 'movie':
                        release = media_info.get('release_date')
                        if release:
                            result_year = int(release[:4])
                    else:
                        first_air = media_info.get('first_air_date')
                        if first_air:
                            result_year = int(first_air[:4])
                    
                    # å¹´ä»½éªŒè¯ï¼ˆå¦‚æœæä¾›äº†å¹´ä»½ï¼‰
                    if year and result_year and year != result_year:
                        logger.warning(f"æ ‡é¢˜æœç´¢åˆ°çš„åª’ä½“å¹´ä»½ {result_year} ä¸æå–çš„å¹´ä»½ {year} ä¸ä¸€è‡´ï¼Œæ”¾å¼ƒæ­¤ç»“æœ")
                        media_info = None
                    else:
                        logger.info(f"é€šè¿‡æ ‡é¢˜æœç´¢æ‰¾åˆ°åª’ä½“: {clean_title} (ID: {result_id}, å¹´ä»½: {result_year})")
                        match_method = "title"

            if not media_info:
                logger.info(f"TMDB æœªè¯†åˆ«åˆ°åª’ä½“: {title_candidate[:50]}")
                return save_result

            # 4. è·å–è¯¦ç»†ä¿¡æ¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
            media_type = media_info.get('media_type')
            if media_type in ['movie', 'tv'] and 'genres' not in media_info:
                try:
                    details = await tmdb.get_details(media_type, media_info['id'])
                    if details:
                        media_info.update(details)
                except Exception as e:
                    logger.warning(f"è·å–åª’ä½“è¯¦æƒ…å¤±è´¥: {e}")

            # 5. åŒ¹é…è§„åˆ™
            rule = organizer.match_rule(media_info)
            if not rule:
                logger.info(f"æœªåŒ¹é…åˆ°è§„åˆ™: {clean_title}")
                return save_result

            # 6. ç¡®å®šç›®æ ‡ç›®å½•
            target_path = organizer.get_target_path(rule)
            target_cid = await self._ensure_save_dir(target_path)

            # 7. ç”Ÿæˆæ–°åç§°ï¼ˆä½¿ç”¨è§£æåˆ°çš„æŠ€æœ¯å‚æ•°ï¼‰
            # ä» parsed ä¸­è·å–æŠ€æœ¯å‚æ•°
            source = parsed.source
            resolution = parsed.quality.value if parsed.quality != QualityLevel.UNKNOWN else ''
            video_codec = parsed.codec
            audio_codec = parsed.audio
            season_episode = f"S{parsed.season:02d}E{parsed.episode:02d}" if parsed.season and parsed.episode else ''

            # ä½¿ç”¨ TMDB çš„æ ‡é¢˜å’Œå¹´ä»½
            tmdb_title = media_info.get('title') or media_info.get('name') or ''
            tmdb_year = (media_info.get('release_date') or media_info.get('first_air_date') or '')[:4] if media_info else ''

            # æ„å»ºæ–°æ–‡ä»¶å
            parts = [tmdb_title]
            if tmdb_year:
                parts.append(tmdb_year)
            if season_episode:
                parts.append(season_episode)
            if source:
                parts.append(source)
            if resolution:
                parts.append(resolution)
            if video_codec:
                parts.append(video_codec)
            if audio_codec:
                parts.append(audio_codec)

            new_name = '.'.join(parts)
            # ä¿ç•™åŸå§‹æ‰©å±•å
            ext_match = re.search(r'\.([a-zA-Z0-9]+)$', title_candidate)
            if ext_match:
                new_name = f"{new_name}.{ext_match.group(1)}"

            logger.info(f"ç”Ÿæˆæ–°æ–‡ä»¶å: {new_name}")

            # 8. ç§»åŠ¨/é‡å‘½åï¼ˆä¿®å¤è§£åŒ…é”™è¯¯ï¼Œå¢å¼ºå¼‚å¸¸å¤„ç†ï¼‰
            to_cid = save_result.get('to_cid')
            names = save_result.get('names', [])
            if len(names) == 1:
                old_fid = await self._find_single_fid(to_cid, names[0])
                if old_fid:
                    try:
                        logger.debug(f"æ­£åœ¨ç§»åŠ¨æ–‡ä»¶: old_fid={old_fid}, new_name={new_name}, target_cid={target_cid}")
                        # è°ƒç”¨ fs_renameï¼Œå¤„ç†è¿”å›å€¼
                        resp = await self._api_call_with_timeout(
                            self.client.fs_rename,
                            old_fid, new_name, pid=target_cid,
                            async_=True, **self._get_ios_ua_kwargs()
                        )
                        
                        # åˆ¤æ–­å“åº”æ˜¯å¦æˆåŠŸ
                        success = False
                        if resp is None:
                            success = False
                            logger.warning("fs_rename è¿”å› None")
                        elif isinstance(resp, dict):
                            success = resp.get('state', False) or resp.get('errCode') == 0
                            if not success:
                                logger.warning(f"fs_rename è¿”å›å¤±è´¥å­—å…¸: {resp}")
                        elif isinstance(resp, bool):
                            success = resp
                        elif isinstance(resp, int):
                            success = resp == 0
                        elif isinstance(resp, str):
                            success = resp.lower() in ('true', 'ok', 'success')
                        elif isinstance(resp, (list, tuple)):
                            # é¿å…è§£åŒ…é”™è¯¯ï¼Œç›´æ¥å–ç¬¬ä¸€ä¸ªå…ƒç´ ä½œä¸ºçŠ¶æ€
                            if len(resp) > 0:
                                first = resp[0]
                                if isinstance(first, bool):
                                    success = first
                                elif isinstance(first, int):
                                    success = first == 0
                                elif isinstance(first, str):
                                    success = first.lower() in ('true', 'ok', 'success')
                                else:
                                    logger.warning(f"æ— æ³•è¯†åˆ«çš„å…ƒç»„å…ƒç´ ç±»å‹: {type(first)}ï¼Œå‡è®¾æˆåŠŸ")
                                    success = True
                            else:
                                success = False
                        else:
                            # å¦‚æœè¿”å›å…¶ä»–ç±»å‹ï¼Œå¯èƒ½æ˜¯æˆåŠŸï¼ˆæ ¹æ®p115clientæƒ¯ä¾‹ï¼‰
                            logger.warning(f"æœªçŸ¥å“åº”ç±»å‹: {type(resp)}ï¼Œå‡è®¾æˆåŠŸ")
                            success = True
                        
                        if success:
                            logger.info(f"âœ… å·²ç§»åŠ¨å¹¶é‡å‘½å {names[0]} -> {target_path}/{new_name}")
                            # æ›´æ–° save_resultï¼Œä»¥ä¾¿åç»­åˆ†äº«é“¾æ¥ä½¿ç”¨æ–°ä½ç½®å’Œæ–°åç§°
                            save_result['to_cid'] = target_cid
                            save_result['names'] = [new_name]
                        else:
                            logger.error(f"âŒ ç§»åŠ¨/é‡å‘½åå¤±è´¥ï¼Œå“åº”: {resp}")
                    except Exception as e:
                        logger.error(f"âŒ ç§»åŠ¨/é‡å‘½åè¿‡ç¨‹å‘ç”Ÿå¼‚å¸¸: {e}", exc_info=True)
                else:
                    logger.warning(f"âš ï¸ æœªæ‰¾åˆ°æ–‡ä»¶ {names[0]} è¿›è¡Œæ•´ç†")
            else:
                logger.warning("å¤šæ–‡ä»¶æ•´ç†æš‚æœªå®ç°ï¼Œè·³è¿‡")

            return save_result
        except Exception as e:
            logger.error(f"æ•´ç†è¿‡ç¨‹å‡ºé”™: {e}")
            return save_result
        finally:
            await tmdb.close()

    async def _find_single_fid(self, cid: int, name: str) -> Optional[str]:
        """åœ¨æŒ‡å®šç›®å½•ä¸­æŸ¥æ‰¾å•ä¸ªæ–‡ä»¶/æ–‡ä»¶å¤¹çš„ ID"""
        files = await self._find_files_in_dir(cid, [name])
        if files:
            return files[0]['fid']
        return None

    
p115_service = P115Service()