from p115client import P115Client, check_response
from p115client.fs import P115FileSystem
from p115client.util import share_extract_payload
from app.core.config import settings
from loguru import logger
import asyncio
import time
from contextlib import asynccontextmanager
from typing import Literal, Optional
from app.core.database import async_session
from app.models.schema import PendingLink, LinkHistory
from sqlalchemy import select, delete

# é»˜è®¤ API è¯·æ±‚è¶…æ—¶ï¼ˆç§’ï¼‰
API_TIMEOUT = 60
# é»˜è®¤ API é‡è¯•æ¬¡æ•°
API_MAX_RETRIES = 3
# é‡è¯•é—´éš”ï¼ˆç§’ï¼‰
API_RETRY_DELAY = 5


class P115Service:
    def __init__(self):
        self.client = None
        self.fs = None
        self.is_connected = False
        self._task_lock: Optional[asyncio.Lock] = None  # Lazy initialize
        self._current_task: str | None = None  # Track current task type
        self._save_dir_cid: int = 0  # Cached save directory CID
        if settings.P115_COOKIE:
            self.init_client(settings.P115_COOKIE)

    async def _api_call_with_timeout(
        self,
        coro_func,
        *args,
        timeout: int = API_TIMEOUT,
        max_retries: int = API_MAX_RETRIES,
        retry_delay: int = API_RETRY_DELAY,
        label: str = "API",
        **kwargs,
    ):
        """å¸¦è¶…æ—¶å’Œé‡è¯•çš„ API è°ƒç”¨åŒ…è£…å™¨ã€‚
        
        Args:
            coro_func: å¼‚æ­¥æ–¹æ³•ï¼ˆå¦‚ self.client.share_snapï¼‰
            *args: ä¼ ç»™ coro_func çš„ä½ç½®å‚æ•°
            timeout: å•æ¬¡è¯·æ±‚è¶…æ—¶ç§’æ•°
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            retry_delay: é‡è¯•é—´éš”ç§’æ•°
            label: æ—¥å¿—æ ‡è¯†
            **kwargs: ä¼ ç»™ coro_func çš„å…³é”®å­—å‚æ•°
        """
        last_error = None
        for attempt in range(1, max_retries + 1):
            try:
                result = await asyncio.wait_for(
                    coro_func(*args, **kwargs),
                    timeout=timeout,
                )
                return result
            except asyncio.TimeoutError:
                last_error = TimeoutError(f"{label} è¯·æ±‚è¶…æ—¶ ({timeout}s), å°è¯• {attempt}/{max_retries}")
                logger.warning(f"â±ï¸ {label} è¯·æ±‚è¶…æ—¶ (å°è¯• {attempt}/{max_retries})")
            except Exception as e:
                # éè¶…æ—¶å¼‚å¸¸ç›´æ¥æŠ›å‡ºï¼Œä¸é‡è¯•
                raise
            
            if attempt < max_retries:
                logger.info(f"ğŸ”„ {label} å°†åœ¨ {retry_delay}s åé‡è¯•...")
                await asyncio.sleep(retry_delay)
        
        raise last_error

    def init_client(self, cookie: str):
        try:
            # Apply proxy settings to environment if configured
            import os
            if settings.PROXY_ENABLED and settings.PROXY_HOST and settings.PROXY_PORT:
                proxy_type = settings.PROXY_TYPE.lower()
                auth = f"{settings.PROXY_USER}:{settings.PROXY_PASS}@" if settings.PROXY_USER and settings.PROXY_PASS else ""
                proxy_url = f"{proxy_type}://{auth}{settings.PROXY_HOST}:{settings.PROXY_PORT}"
                
                os.environ['HTTP_PROXY'] = proxy_url
                os.environ['http_proxy'] = proxy_url
                os.environ['HTTPS_PROXY'] = proxy_url
                os.environ['https_proxy'] = proxy_url
                
            self.client = P115Client(cookie, check_for_relogin=True)
            self.fs = P115FileSystem(self.client)
            
            proxy_info = ""
            if settings.PROXY_ENABLED:
                proxy_info = f" (Proxy: {settings.PROXY_TYPE}://{settings.PROXY_HOST}:{settings.PROXY_PORT})"
            logger.info(f"P115Client and FileSystem initialized successfully{proxy_info}")
            # Verify connection asynchronously
            asyncio.create_task(self.verify_connection())
        except Exception as e:
            logger.error(f"Failed to initialize P115Client: {e}")
            self.client = None
            self.fs = None
            self.is_connected = False

    @asynccontextmanager
    async def _acquire_task_lock(self, task_type: Literal["save_share", "cleanup"]):
        """Acquire task lock with timeout.
        
        Uses asyncio.wait_for on the actual lock acquisition instead of
        polling, which is both more efficient and avoids race conditions.
        """
        if self._task_lock is None:
            self._task_lock = asyncio.Lock()
            
        max_wait = 2100  # 35 minutes max wait (to accommodate network retry)
        
        if self._task_lock.locked():
            logger.info(f"â³ {task_type} ä»»åŠ¡ç­‰å¾…ä¸­ï¼Œå½“å‰ä»»åŠ¡: {self._current_task}")
        
        try:
            await asyncio.wait_for(self._task_lock.acquire(), timeout=max_wait)
        except asyncio.TimeoutError:
            raise TimeoutError(f"ç­‰å¾…ä»»åŠ¡é”è¶…æ—¶ ({max_wait}s): {task_type}ï¼Œå½“å‰å ç”¨: {self._current_task}")
        
        self._current_task = task_type
        logger.info(f"ğŸ”’ {task_type} ä»»åŠ¡å·²è·å–é”")
        try:
            yield
        finally:
            self._current_task = None
            self._task_lock.release()
            logger.info(f"ğŸ”“ {task_type} ä»»åŠ¡å·²é‡Šæ”¾é”")

    async def verify_connection(self) -> bool:
        """Verify the 115 cookie connection"""
        if not self.client:
            self.is_connected = False
            return False
            
        try:
            # Simple API call to verify cookie
            resp = await self._api_call_with_timeout(
                self.client.user_info, async_=True,
                timeout=30, max_retries=2, label="user_info"
            )
            if resp.get("state"):
                self.is_connected = True
                logger.info("âœ… 115 ç½‘ç›˜ç™»å½•éªŒè¯æˆåŠŸ")
                return True
        except Exception as e:
            logger.error(f"âŒ 115 ç½‘ç›˜ç™»å½•éªŒè¯å¤±è´¥: {e}")
            self.is_connected = False
            return False
            
        self.is_connected = False
        return False

    def clear_save_dir_cache(self):
        """Clear the cached save directory CID (e.g. after cleanup)"""
        self._save_dir_cid = 0
        logger.debug("ğŸ—‘ï¸ å·²æ¸…é™¤ä¿å­˜ç›®å½• CID ç¼“å­˜")

    async def _ensure_save_dir(self, path: Optional[str] = None):
        """Ensure the save directory exists and return its CID.
        
        Uses a cached CID to avoid repeated API calls for the default path.
        If a custom path is provided, it will always verify/create it.
        """
        is_default = path is None
        path = path or settings.P115_SAVE_DIR or "/åˆ†äº«ä¿å­˜"
        
        # Return cached CID if available and using default path
        if is_default and self._save_dir_cid > 0:
            logger.debug(f"ğŸ“‚ ä½¿ç”¨ç¼“å­˜çš„ä¿å­˜ç›®å½• CID: {self._save_dir_cid}")
            return self._save_dir_cid
        
        logger.info(f"ğŸ” å¼€å§‹æ£€æŸ¥/åˆ›å»ºä¿å­˜ç›®å½•: {path}")
        
        if not self.client:
            raise RuntimeError("P115Client æœªåˆå§‹åŒ–ï¼Œæ— æ³•åˆ›å»ºä¿å­˜ç›®å½•")
        
        # Retry up to 3 times with timeout
        last_error = None
        for attempt in range(1, 4):
            try:
                logger.info(f"ğŸ“ è°ƒç”¨ fs_makedirs_app åˆ›å»ºç›®å½•... (å°è¯• {attempt}/3)")
                # Add 30s timeout to prevent indefinite hanging
                resp = await asyncio.wait_for(
                    self.client.fs_makedirs_app(path, pid=0, async_=True),
                    timeout=30
                )
                logger.info(f"ğŸ“‹ fs_makedirs_app å“åº”: {resp}")
                check_response(resp)
                
                # The response structure has 'cid' at the top level (not in 'data')
                # Response format: {'state': True, 'error': '', 'errCode': 0, 'cid': '3358575817564146054'}
                cid = 0
                if "cid" in resp:
                    cid = int(resp["cid"])
                    logger.info(f"ğŸ”¢ ä»å“åº”ä¸­æå–åˆ° CID: {cid}")
                elif "data" in resp:
                    data = resp["data"]
                    cid = int(data.get("category_id") or data.get("cid") or data.get("id") or 0)
                    logger.info(f"ğŸ”¢ ä» data å­—æ®µä¸­æå–åˆ° CID: {cid}")
                else:
                    logger.error(f"âŒ å“åº”ä¸­æ²¡æœ‰ 'cid' æˆ– 'data' å­—æ®µ: {resp}")
                    
                if cid == 0:
                    raise RuntimeError(f"æ— æ³•ä»å“åº”è·å–æœ‰æ•ˆçš„ CID: {resp}")
                    
                # Cache the CID only if it's the default path
                if is_default:
                    self._save_dir_cid = cid
                logger.info(f"âœ… ä¿å­˜ç›®å½•å·²ç¡®è®¤: {path} (CID: {cid})")
                return cid
                
            except asyncio.TimeoutError:
                last_error = TimeoutError(f"fs_makedirs_app è¯·æ±‚è¶…æ—¶ (30s), å°è¯• {attempt}/3")
                logger.warning(f"â±ï¸ fs_makedirs_app è¯·æ±‚è¶…æ—¶ (å°è¯• {attempt}/3)")
            except Exception as e:
                last_error = e
                logger.warning(f"âš ï¸ åˆ›å»ºç›®å½•å¤±è´¥ (å°è¯• {attempt}/3): {e}")
            
            if attempt < 3:
                await asyncio.sleep(3)
        
        # All retries exhausted â€” raise to prevent saving to root
        raise RuntimeError(f"æ— æ³•ç¡®ä¿ä¿å­˜ç›®å½• {path} å­˜åœ¨ (å·²é‡è¯•3æ¬¡): {last_error}")

    async def save_share_link(self, share_url: str, metadata: dict = None, target_dir: Optional[str] = None):
        """Save a 115 share link to the configured directory
        
        Args:
            share_url: The 115 share URL to save
            metadata: Optional metadata dict containing description, full_text, photo_id, etc.
            target_dir: Optional target directory path
        """
        async with self._acquire_task_lock("save_share"):
            if not self.client:
                logger.warning("P115Client not initialized, cannot save link")
                return None
            
            logger.info(f"ğŸ“¥ å¼€å§‹å¤„ç†åˆ†äº«é“¾æ¥: {share_url}")
            try:
                # 1. Extract share/receive codes
                payload = share_extract_payload(share_url)
                
                # 2. Get share snapshot to get file IDs and names (å¸¦è¶…æ—¶é‡è¯•)
                snap_resp = await self._api_call_with_timeout(
                    self.client.share_snap, payload, async_=True,
                    timeout=API_TIMEOUT, label="share_snap"
                )
                check_response(snap_resp)

                # Check for audit and violation status
                data = snap_resp.get("data", {})
                share_info = data.get("shareinfo" if "shareinfo" in data else "share_info", {})
                share_state = data.get("share_state", share_info.get("share_state", share_info.get("status"))) # Multiple fallbacks
                share_title = share_info.get("share_title", "")
                have_vio_file = share_info.get("have_vio_file", 0)

                # ä¼˜å…ˆåˆ¤æ–­è¿è§„å†…å®¹ï¼Œæ— è®ºå®¡æ ¸çŠ¶æ€å¦‚ä½•
                if have_vio_file == 1:
                    logger.warning(f"ğŸš« åˆ†äº«é“¾æ¥åŒ…å«è¿è§„å†…å®¹: {share_url}")
                    return {
                        "status": "error",
                        "error_type": "violated",
                        "message": "é“¾æ¥åŒ…å«è¿è§„å†…å®¹"
                    }

                if share_state == 0:
                    logger.info(f"ğŸ” åˆ†äº«é“¾æ¥å¤„äºå®¡æ ¸ä¸­ï¼Œè¿›å…¥è½®è¯¢ç­‰å¾…é˜Ÿåˆ—: {share_url}")
                    # Save to DB for persistence
                    async with async_session() as session:
                        new_task = PendingLink(
                            share_url=share_url,
                            metadata_json=metadata or {},
                            status="auditing"
                        )
                        session.add(new_task)
                        await session.commit()
                        db_id = new_task.id
                    
                    return {
                        "status": "pending",
                        "share_url": share_url,
                        "metadata": metadata or {},
                        "db_id": db_id
                    }
                
                if share_state == 7:
                    logger.warning(f"âš ï¸ åˆ†äº«é“¾æ¥å·²è¿‡æœŸ: {share_url}")
                    return {
                        "status": "error",
                        "error_type": "expired",
                        "message": "é“¾æ¥å·²è¿‡æœŸ"
                    }
                
                if share_state != 1:
                    logger.warning(f"âš ï¸ åˆ†äº«é“¾æ¥çŠ¶æ€å¼‚å¸¸ (state={share_state}): {share_url}")
                    # Allow attempt if state is unknown but not explicitly pending/expired/prohibited
                
                items = snap_resp["data"]["list"]
                if not items:
                    logger.warning("åˆ†äº«é“¾æ¥å†…æ²¡æœ‰æ–‡ä»¶")
                    return None
                
                # Extract file/folder IDs and names
                # Files use 'fid', folders use 'cid'
                fids = []
                names = []
                for item in items:
                    # Try to get fid (file) or cid (folder)
                    fid = item.get("fid") or item.get("cid")
                    if fid:
                        fids.append(str(fid))
                        names.append(item.get("n", "æœªçŸ¥"))
                    else:
                        logger.warning(f"Item missing both fid and cid: {item}")
                
                if not fids:
                    logger.error("æœªèƒ½æå–åˆ°ä»»ä½•æœ‰æ•ˆçš„æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹ ID")
                    return None
                
                logger.info(f"ğŸ“¦ æ£€æµ‹åˆ° {len(fids)} ä¸ªé¡¹ç›®: {', '.join(names[:3])}{'...' if len(names) > 3 else ''}")
                
                # 3. Ensure save directory (with network recovery retry)
                #    If _ensure_save_dir fails (e.g. network issue), pause and retry
                #    for up to 30 minutes instead of discarding the task.
                to_cid = None
                max_network_wait = 1800  # 30 minutes
                network_start = time.time()
                network_attempt = 0
                
                while True:
                    try:
                        to_cid = await self._ensure_save_dir(target_dir)
                        if network_attempt > 0:
                            logger.info(f"ğŸ‰ ç½‘ç»œå·²æ¢å¤ï¼Œç»§ç»­å¤„ç†ä»»åŠ¡ (ç­‰å¾…äº† {time.time() - network_start:.0f}s)")
                        break
                    except Exception as dir_err:
                        network_attempt += 1
                        elapsed = time.time() - network_start
                        remaining = max_network_wait - elapsed
                        
                        if remaining <= 0:
                            logger.error(f"âŒ ç½‘ç›˜ç½‘ç»œæ¢å¤ç­‰å¾…è¶…æ—¶ (30åˆ†é’Ÿ)ï¼Œä¸­æ­¢ä»»åŠ¡: {dir_err}")
                            return {
                                "status": "error",
                                "error_type": "dir_failed",
                                "message": f"ç½‘ç›˜ç½‘ç»œæŒç»­ä¸å¯ç”¨ (å·²ç­‰å¾…30åˆ†é’Ÿ): {dir_err}"
                            }
                        
                        wait_time = min(30, remaining)
                        logger.warning(
                            f"â¸ï¸ ç½‘ç›˜ç½‘ç»œå¼‚å¸¸ï¼Œä»»åŠ¡æš‚åœç­‰å¾…æ¢å¤ "
                            f"(ç¬¬{network_attempt}æ¬¡é‡è¯•, å·²ç­‰å¾… {elapsed:.0f}s, å‰©ä½™ {remaining:.0f}s): {dir_err}"
                        )
                        await asyncio.sleep(wait_time)
                
                # 4. Receive files
                receive_payload = {
                    "share_code": payload["share_code"],
                    "receive_code": payload["receive_code"] or "",
                    "file_id": ",".join(fids),
                    "cid": to_cid
                }
                
                try:
                    recv_resp = await self._api_call_with_timeout(
                        self.client.share_receive, receive_payload, async_=True,
                        timeout=API_TIMEOUT, label="share_receive"
                    )
                    check_response(recv_resp)
                    logger.info(f"âœ… é“¾æ¥è½¬å­˜æŒ‡ä»¤å·²å‘é€: {share_url} -> CID {to_cid}")
                except Exception as recv_error:
                    # Check if it's a "file already received" error (errno 4200045)
                    error_msg = str(recv_error)
                    if "4200045" in error_msg or "å·²ç»æ¥æ”¶" in error_msg:
                        logger.warning(f"âš ï¸ 115 æç¤ºæ–‡ä»¶è¯¥åˆ†äº«å·²æ¥æ”¶è¿‡: {share_url}")
                        # Verify if files really exist in to_cid
                        found_all = False
                        try:
                            # ç”¨ _find_files_in_dir æŸ¥æ‰¾ï¼ˆæ”¯æŒ search + list åŒé‡æŸ¥æ‰¾ï¼‰
                            found_files = await self._find_files_in_dir(to_cid, names)
                            found_count = len(found_files)
                            if found_count > 0:
                                logger.info(f"âœ… åœ¨ä¿å­˜ç›®å½•ä¸­æ‰¾åˆ° {found_count} ä¸ªåŒåæ–‡ä»¶ï¼Œç»§ç»­å¤„ç†")
                                # Continue to share creation with existing files
                            else:
                                logger.error("âŒ 115 æç¤ºå·²æ¥æ”¶ï¼Œä½†åœ¨ä¿å­˜ç›®å½•æœªæ‰¾åˆ°æ–‡ä»¶ï¼ˆå¯èƒ½å·²è¢«åˆ é™¤ï¼‰ã€‚æ— æ³•é‡æ–°è½¬å­˜åŒä¸€åˆ†äº«é“¾æ¥ã€‚")
                                return {
                                    "status": "error",
                                    "error_type": "already_exists_missing",
                                    "message": "è¯¥åˆ†äº«é“¾æ¥æ‚¨å·²è½¬å­˜è¿‡ã€‚115 é™åˆ¶åŒä¸€é“¾æ¥æ— æ³•ç”±äºæ–‡ä»¶ä¸¢å¤±è€Œé‡å¤è½¬å­˜ï¼Œè¯·å°è¯•å¯»æ‰¾åŸæ–‡ä»¶æˆ–ä»å›æ”¶ç«™è¿˜åŸã€‚"
                                }
                        except Exception as check_e:
                            logger.warning(f"âš ï¸ æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨æ—¶å‡ºé”™: {check_e}")
                            # Assume failure to be safe
                            return {
                                "status": "error", 
                                "error_type": "unknown",
                                "message": "ä¿å­˜å¤±è´¥ï¼Œä¸”æ— æ³•éªŒè¯æ–‡ä»¶æ˜¯å¦å­˜åœ¨"
                            }
                    else:
                        # Other errors, re-raise
                        raise
                
                return {
                    "status": "success", 
                    "to_cid": to_cid, 
                    "names": names,
                    "share_url": share_url,
                    "metadata": metadata or {}  # Include metadata in return value
                }
            except Exception as e:
                logger.error(f"âŒ ä¿å­˜åˆ†äº«é“¾æ¥å¤±è´¥", exc_info=True)
                return None

    async def get_share_status(self, share_url: str):
        """Check the current status of a share link
        
        Returns:
            dict: {
                "share_state": int,
                "is_auditing": bool,
                "is_expired": bool,
                "is_prohibited": bool,
                "title": str
            }
        """
        try:
            payload = share_extract_payload(share_url)
            snap_resp = await self._api_call_with_timeout(
                self.client.share_snap, payload, async_=True,
                timeout=API_TIMEOUT, label="share_snap(status)"
            )
            check_response(snap_resp)
            
            data = snap_resp.get("data", {})
            share_info = data.get("shareinfo" if "shareinfo" in data else "share_info", {})
            share_state = data.get("share_state", share_info.get("share_state", share_info.get("status")))
            share_title = share_info.get("share_title", "")
            have_vio_file = share_info.get("have_vio_file", 0)
            
            res = {
                "share_state": share_state,
                "is_auditing": share_state == 0,
                "is_expired": share_state == 7,
                "is_prohibited": have_vio_file == 1,
                "title": share_title
            }
            logger.debug(f"ğŸ“Š æ£€æŸ¥é“¾æ¥çŠ¶æ€: {share_url} -> {res}")
            return res
        except Exception as e:
            logger.error(f"âŒ æ£€æŸ¥é“¾æ¥çŠ¶æ€å¤±è´¥: {share_url}, é”™è¯¯: {e}")
            return None

    async def _find_files_in_dir(self, cid: int, target_names: list) -> list:
        """åœ¨æŒ‡å®šç›®å½•ä¸­æŸ¥æ‰¾æ–‡ä»¶ï¼Œä½¿ç”¨å¤šç§æ–¹å¼ç¡®ä¿æ‰¾åˆ°
        
        ä¼˜å…ˆä½¿ç”¨ fs_searchï¼ˆæŒ‰æ–‡ä»¶åæœç´¢ï¼‰ï¼Œå¤±è´¥åå›é€€åˆ° fs_filesï¼ˆåˆ—ç›®å½•ï¼‰ã€‚
        
        Args:
            cid: ç›®å½• ID
            target_names: è¦æŸ¥æ‰¾çš„æ–‡ä»¶ååˆ—è¡¨
            
        Returns:
            åŒ¹é…çš„æ–‡ä»¶åˆ—è¡¨ [{fid, name, size, time}, ...]
        """
        matched = []
        
        # æ–¹å¼ 1: ä½¿ç”¨ fs_search æŒ‰æ–‡ä»¶åæœç´¢ï¼ˆæ›´å¯é ï¼Œä¸ä¾èµ–ç›®å½•ç¼“å­˜ï¼‰
        for name in target_names:
            try:
                search_resp = await self._api_call_with_timeout(
                    self.client.fs_search,
                    {"search_value": name, "cid": cid, "limit": 20},
                    async_=True,
                    timeout=30, max_retries=2, label=f"fs_search({name})"
                )
                check_response(search_resp)
                search_data = search_resp.get("data", [])
                
                # fs_search çš„ç»“æœå¯èƒ½åœ¨ data æ•°ç»„æˆ– data.list ä¸­
                if isinstance(search_data, dict):
                    search_items = search_data.get("list", [])
                else:
                    search_items = search_data
                
                logger.debug(f"ğŸ” fs_search '{name}' åœ¨ CID:{cid} è¿”å› {len(search_items)} æ¡ç»“æœ")
                
                for item in search_items:
                    item_name = item.get("n") or item.get("file_name")
                    if item_name == name:
                        item_id = item.get("fid") or item.get("cid") or item.get("file_id") or item.get("category_id")
                        if item_id:
                            matched.append({
                                "fid": str(item_id),
                                "name": item_name,
                                "size": item.get("s", item.get("file_size", 0)),
                                "time": item.get("te", 0),
                            })
                            logger.info(f"ğŸ“„ fs_search æ‰¾åˆ°: {item_name} (ID: {item_id})")
                            break
            except Exception as e:
                logger.warning(f"âš ï¸ fs_search æœç´¢ '{name}' å¤±è´¥: {e}")
        
        if len(matched) == len(target_names):
            return matched
        
        # æ–¹å¼ 2: å›é€€åˆ° fs_files åˆ—ç›®å½•
        found_names = {m["name"] for m in matched}
        remaining_names = [n for n in target_names if n not in found_names]
        logger.info(f"ğŸ” fs_search æ‰¾åˆ° {len(matched)}/{len(target_names)} ä¸ªæ–‡ä»¶ï¼Œå°è¯• fs_files æŸ¥æ‰¾å‰©ä½™: {remaining_names}")
        
        try:
            resp = await self._api_call_with_timeout(
                self.client.fs_files,
                {"cid": cid, "limit": 500, "show_dir": 1},
                async_=True,
                timeout=30, max_retries=2, label="fs_files"
            )
            check_response(resp)
            file_list = resp.get("data", [])
            
            # æ£€æŸ¥ data çš„ç±»å‹ï¼Œå…¼å®¹ä¸åŒå“åº”æ ¼å¼
            if isinstance(file_list, dict):
                file_list = file_list.get("list", [])
            
            # è·å–å“åº”ä¸­çš„å®é™… CIDï¼ŒéªŒè¯æ˜¯å¦æ­£ç¡®åˆ—å‡ºäº†ç›®æ ‡ç›®å½•
            resp_path = resp.get("path", [])
            resp_cid = None
            if resp_path:
                last_path = resp_path[-1] if isinstance(resp_path, list) else resp_path
                resp_cid = last_path.get("cid") if isinstance(last_path, dict) else None
            
            actual_count = resp.get("count", "?")
            logger.debug(f"ğŸ“‚ fs_files CID:{cid} è¿”å› {len(file_list)} é¡¹ (æ€»æ•°: {actual_count}, è·¯å¾„CID: {resp_cid})")
            
            # éªŒè¯è¿”å›çš„æ˜¯å¦æ˜¯æ­£ç¡®çš„ç›®å½•ï¼ˆé˜²æ­¢ CID ä¸å­˜åœ¨æ—¶å›é€€åˆ°æ ¹ç›®å½•ï¼‰
            if resp_cid is not None and str(resp_cid) != str(cid):
                logger.warning(f"âš ï¸ fs_files è¿”å›çš„ç›®å½• CID({resp_cid}) ä¸è¯·æ±‚çš„ CID({cid}) ä¸åŒ¹é…ï¼å¯èƒ½ç›®å½•ä¸å­˜åœ¨")
            
            # æ—¥å¿—æ‰“å°ç›®å½•ä¸­çš„å‰10ä¸ªæ–‡ä»¶åï¼Œä¾¿äºæ’æŸ¥
            if file_list:
                dir_file_names = [item.get("n", "?") for item in file_list[:10]]
                logger.debug(f"ğŸ“‹ ç›®å½•å†…æ–‡ä»¶(å‰10): {dir_file_names}")
            
            for item in file_list:
                item_name = item.get("n")
                if item_name in remaining_names:
                    item_id = item.get("fid") or item.get("cid")
                    if item_id:
                        matched.append({
                            "fid": str(item_id),
                            "name": item_name,
                            "size": item.get("s", 0),
                            "time": item.get("te", 0),
                        })
                        logger.info(f"ğŸ“„ fs_files æ‰¾åˆ°: {item_name} (ID: {item_id})")
                        
        except Exception as e:
            logger.warning(f"âš ï¸ fs_files åˆ—ç›®å½•å¤±è´¥: {e}")
        
        return matched

    async def create_share_link(self, save_result: dict):
        if not self.client or not save_result:
            return None
            
        to_cid = save_result.get("to_cid")
        names = save_result.get("names", [])
        
        try:
            # 5. Wait for 10 seconds as requested
            logger.info(f"â³ ç­‰å¾… 10 ç§’ä»¥ç¡®ä¿æ–‡ä»¶ä¿å­˜å®Œæˆ...")
            await asyncio.sleep(10)
            
            # 6. Find files with polling (using search + list as fallback)
            new_fids = []
            matched_files = []
            
            max_poll_attempts = 5
            for poll_attempt in range(1, max_poll_attempts + 1):
                try:
                    logger.info(f"ğŸ” å¼€å§‹æŸ¥æ‰¾æ–‡ä»¶ (ç¬¬ {poll_attempt}/{max_poll_attempts} æ¬¡), ç›®æ ‡ç›®å½• CID: {to_cid}, æŸ¥æ‰¾: {names}")
                    current_matched = await self._find_files_in_dir(to_cid, names)
                    
                    if current_matched:
                        if matched_files:
                            # Compare with previous poll
                            stable = len(current_matched) == len(matched_files)
                            if stable:
                                for curr, prev in zip(sorted(current_matched, key=lambda x: x["fid"]), 
                                                     sorted(matched_files, key=lambda x: x["fid"])):
                                    if curr["fid"] != prev["fid"] or curr["size"] != prev["size"]:
                                        stable = False
                                        break
                            
                            if stable:
                                logger.info(f"âœ… æ–‡ä»¶çŠ¶æ€å·²ç¨³å®šï¼Œæ£€æµ‹åˆ° {len(current_matched)} ä¸ªæ–‡ä»¶")
                                new_fids = [f["fid"] for f in current_matched]
                                break
                            else:
                                logger.debug(f"ğŸ”„ æ–‡ä»¶çŠ¶æ€å˜åŒ–ä¸­ (ç¬¬ {poll_attempt}/{max_poll_attempts} æ¬¡è½®è¯¢)")
                        
                        matched_files = current_matched
                        
                        if poll_attempt < max_poll_attempts:
                            await asyncio.sleep(5)
                    else:
                        logger.warning(f"âš ï¸ è½®è¯¢æœªæ‰¾åˆ°æ–‡ä»¶ (ç¬¬ {poll_attempt}/{max_poll_attempts} æ¬¡)")
                        if poll_attempt < max_poll_attempts:
                            await asyncio.sleep(5)
                            
                except Exception as e:
                    logger.warning(f"âš ï¸ æŸ¥æ‰¾æ–‡ä»¶å¤±è´¥ (è½®è¯¢ {poll_attempt}/{max_poll_attempts}): {e}")
                    if poll_attempt < max_poll_attempts:
                        await asyncio.sleep(5)
            
            # If polling didn't find stable files, use the last matched files
            if not new_fids and matched_files:
                logger.info(f"âš ï¸ æ–‡ä»¶æœªå®Œå…¨ç¨³å®šï¼Œä½†ä½¿ç”¨ {len(matched_files)} ä¸ªå·²åŒ¹é…çš„æ–‡ä»¶å°è¯•åˆ›å»ºåˆ†äº«")
                new_fids = [f["fid"] for f in matched_files]
            
            if not new_fids:
                logger.warning(f"âš ï¸ åœ¨ä¿å­˜ç›®å½• {to_cid} ä¸­æœªæ‰¾åˆ°å¯¹åº”çš„æ–‡ä»¶ {names}ï¼Œå¯èƒ½ä¿å­˜å°šæœªå®Œæˆ")
                return None
            
            # 7. Create new share with retry mechanism
            share_code = None
            receive_code = None
            max_share_retries = 3
            
            for retry_attempt in range(1, max_share_retries + 1):
                try:
                    logger.info(f"ğŸ“¤ æ­£åœ¨åˆ›å»ºåˆ†äº«é“¾æ¥ (å°è¯• {retry_attempt}/{max_share_retries}): {', '.join(names[:3])}...")
                    send_resp = await self._api_call_with_timeout(
                        self.client.share_send, ",".join(new_fids), async_=True,
                        timeout=API_TIMEOUT, max_retries=1, label="share_send"
                    )
                    check_response(send_resp)
                    
                    # Extract share_code
                    data = send_resp["data"]
                    share_code = data.get("share_code")
                    receive_code = data.get("receive_code") or data.get("recv_code")
                    
                    logger.info(f"âœ… åˆ†äº«é“¾æ¥åˆ›å»ºæˆåŠŸ: {share_code}")
                    break  # Success, exit retry loop
                    
                except Exception as share_error:
                    error_str = str(share_error)
                    # Check if it's error 4100005 (file moved or deleted)
                    if "4100005" in error_str or "å·²è¢«ç§»åŠ¨æˆ–åˆ é™¤" in error_str:
                        if retry_attempt < max_share_retries:
                            logger.warning(f"âš ï¸ æ–‡ä»¶å°šæœªå°±ç»ª (é”™è¯¯ 4100005)ï¼Œç­‰å¾… 5 ç§’åé‡è¯•...")
                            await asyncio.sleep(5)
                            continue
                        else:
                            logger.error(f"âŒ é‡è¯• {max_share_retries} æ¬¡åä»å¤±è´¥: {share_error}")
                            raise
                    else:
                        # Other errors, don't retry
                        logger.error(f"âŒ åˆ›å»ºåˆ†äº«é“¾æ¥å¤±è´¥ (éæ—¶åºé—®é¢˜): {share_error}")
                        raise
            
            if not share_code:
                logger.error("âŒ æœªèƒ½è·å–åˆ° share_code")
                return None
            
            # 8. Update share to be permanent (share_duration=-1)
            if share_code:
                logger.info(f"ğŸ”„ æ­£åœ¨å°†åˆ†äº«é“¾æ¥ {share_code} è½¬æ¢ä¸ºé•¿æœŸæœ‰æ•ˆ...")
                update_payload = {
                    "share_code": share_code,
                    "share_duration": -1
                }
                update_resp = await self._api_call_with_timeout(
                    self.client.share_update, update_payload, async_=True,
                    timeout=API_TIMEOUT, max_retries=2, label="share_update"
                )
                check_response(update_resp)
                logger.debug(f"Share update response: {update_resp}")

            new_share = f"https://115.com/s/{share_code}"
            if receive_code:
                new_share += f"?password={receive_code}"
                
            logger.info(f"ğŸ”— é•¿æœŸåˆ†äº«é“¾æ¥å·²ç”Ÿæˆ: {new_share}")
            return new_share
            
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºæ–°åˆ†äº«é“¾æ¥å¤±è´¥: {e}")
            return None

    async def cleanup_save_directory(self):
        """Clean up the save directory by deleting the entire folder.
        It will be automatically recreated by _ensure_save_dir on next save."""
        async with self._acquire_task_lock("cleanup"):
            logger.info("ğŸ§¹ å¼€å§‹æ¸…ç†ä¿å­˜ç›®å½•...")
            try:
                save_dir_cid = await self._ensure_save_dir()
                if not save_dir_cid:
                    logger.error("æ— æ³•è·å–ä¿å­˜ç›®å½• CID")
                    return False
                # Clear cache since we're deleting the directory
                self.clear_save_dir_cache()

                # ç›´æ¥åˆ é™¤æ•´ä¸ªä¿å­˜ç›®å½•æ–‡ä»¶å¤¹
                save_path = settings.P115_SAVE_DIR or "/åˆ†äº«ä¿å­˜"
                logger.info(f"ğŸ—‘ï¸ æ­£åœ¨åˆ é™¤ä¿å­˜ç›®å½•: {save_path} (CID: {save_dir_cid})")
                del_resp = await self._api_call_with_timeout(
                    self.client.fs_delete, str(save_dir_cid), async_=True,
                    timeout=API_TIMEOUT, label="fs_delete"
                )
                check_response(del_resp)
                logger.info(f"âœ… ä¿å­˜ç›®å½•å·²åˆ é™¤ï¼Œä¸‹æ¬¡ä¿å­˜æ—¶å°†è‡ªåŠ¨é‡å»º")
                return True
            except Exception as e:
                logger.error(f"æ¸…ç†ä¿å­˜ç›®å½•å¤±è´¥: {e}")
                return False

    async def get_history_link(self, original_url: str) -> str | None:
        """Check if a link has been processed before"""
        try:
            from app.models.schema import LinkHistory
            async with async_session() as session:
                result = await session.execute(
                    select(LinkHistory).where(LinkHistory.original_url == original_url)
                )
                record = result.scalar_one_or_none()
                if record:
                    return record.share_link
            return None
        except Exception as e:
            logger.error(f"æŸ¥è¯¢å†å²è®°å½•å¤±è´¥: {e}")
            return None

    async def save_history_link(self, original_url: str, share_link: str):
        """Save a processed link to history"""
        try:
            from app.models.schema import LinkHistory
            async with async_session() as session:
                # Check existance first to avoid unique constraint error
                existing = await session.execute(
                    select(LinkHistory).where(LinkHistory.original_url == original_url)
                )
                if existing.scalar_one_or_none():
                    return
                
                new_record = LinkHistory(original_url=original_url, share_link=share_link)
                session.add(new_record)
                await session.commit()
                logger.info(f"å·²ä¿å­˜å†å²è®°å½•: {original_url} -> {share_link}")
        except Exception as e:
            logger.error(f"ä¿å­˜å†å²è®°å½•å¤±è´¥: {e}")

    async def delete_all_history_links(self):
        """Clear all history links"""
        try:
            from app.models.schema import LinkHistory
            from sqlalchemy import delete
            async with async_session() as session:
                await session.execute(delete(LinkHistory))
                await session.commit()
                logger.info("å·²æ¸…ç©ºæ‰€æœ‰å†å²è®°å½•")
                return True
        except Exception as e:
            logger.error(f"æ¸…ç©ºå†å²è®°å½•å¤±è´¥: {e}")
            return False

    async def cleanup_recycle_bin(self):
        """Empty the recycle bin"""
        async with self._acquire_task_lock("cleanup"):
            logger.info("ğŸ—‘ï¸ å¼€å§‹æ¸…ç©ºå›æ”¶ç«™...")
            try:
                # Prepare payload with optional password
                payload = {}
                if settings.P115_RECYCLE_PASSWORD:
                    payload["password"] = settings.P115_RECYCLE_PASSWORD
                    logger.debug("ä½¿ç”¨å›æ”¶ç«™å¯†ç ")
                
                # Call recycle bin cleanup API
                resp = await self._api_call_with_timeout(
                    self.client.recyclebin_clean_app, payload, async_=True,
                    timeout=API_TIMEOUT, label="recyclebin_clean"
                )
                check_response(resp)
                
                logger.info("âœ… å›æ”¶ç«™å·²æ¸…ç©º")
                return True
            except Exception as e:
                logger.error("âŒ æ¸…ç©ºå›æ”¶ç«™å¤±è´¥: {}", e)
                return False

p115_service = P115Service()
